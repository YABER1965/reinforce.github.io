## QEUR21_2DMZA2:　Actor-Criticにフィードバック法を使う

## ～  もう一歩、なんだけど・・・  ～

D先生 : “前回では「６つのランドマーク距離」を使いました。Actor-Criticじゃなくて、メトリックスのもんだいじゃないですか？”

**（学習曲線）**

![imageRL4-11-1](https://reinforce.github.io/images/imageRL4-11-1.jpg) 

**（パフォーマンス推移）**

![imageRL4-11-2](https://reinforce.github.io/images/imageRL4-11-2.jpg) 

QEU:FOUNDER ： “メトリックスはボードのセル数（size x size）にするということですね。あとは、minRT距離とスタート、ゴール距離を使います。今回の場合、トータルで103件の情報になります。それでは、プログラムをドン！！”

```python
# ----------------
# メイズ(MAZE)ゲームの強化学習システム
# stepC2 : RTを動的メトリックスとして活用する
# stepC2 : reinforce_maze_pyTorch(sminRT).py
# stepC2 : sminRT距離を導入する(フィードバックはオプションです)
# ---------------- 
# 数値計算のﾗｲﾌﾞﾗﾘ読み込み
import numpy as np
import pandas as pd
import math, copy, random, time
# -----
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.distributions import Categorical
# -----
import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = (16, 10)

# =================================================
# Game board difinition 
# =================================================
foldername  = "./MAZE_test/"
init_maze = [['#', 'S', '#', '#', '#', '#', '#', '#', '#', '#'], ['#', 0, '#', -1, 0, -1, 0, '#', '#', '#'], ['#', -1, '#', -1, -1, 0, -1, -1, 0, '#'], ['#', 0, -1, 0, 0, -1, 0, 0, '#', '#'], ['#', '#', -1, 0, 0, -1, '#', -1, 0, '#'], ['#', -1, 0, 0, -1, -1, 0, '#', -1, '#'], ['#', 0, 0, 0, -1, '#', -1, '#', -1, '#'], ['#', 0, -1, 0, -1, 0, -1, 0, -1, '#'], ['#', 0, -1, 0, 0, -1, 0, 0, -1, '#'], ['#', '#', '#', '#', '#', '#', '#', 50, '#', '#']]
mx_lm2 = [[0, 1],[9, 7]]

# --------------------------------------------------
# MAZE盤面をCSVファイルに保存する
def save_csvMAZE(maze): 
    # --------------------------------------------------
    # CSV出力用のデータフレームを作る(2)
    arr_columns = list(range(10))
    # -----
    df_csvout = pd.DataFrame(maze, columns=arr_columns)
    #print(df_csvout)
    
    # --------------------------------------------------
    # CSV ファイル (file_csvout) として出力
    code_csvout = "maze_board.csv"       # file name  
    file_csvout = foldername + code_csvout   # standard(training) file name   
    print("メトリックス保管用CSVファイル ：{0}".format(file_csvout))
    # -----
    df_csvout.to_csv(file_csvout, index=True)

# MAZE盤面をCSVファイルに保存する
#save_csvMAZE(init_maze)


# =================================================
# difinition of STATE CALCULATION function
# =================================================
# タイムライン用minRTメトリックスを計算する
def timeline_minRT(a_row, a_col, iCnt_turn):
    len_route = len(mx_route)
    if iCnt_turn <= len_route - 1:
        pos_rt  = mx_route[iCnt_turn]
    else:
        pos_rt  = mx_route[len_route - 1]
    # -----
    tlEuc   = (a_row - pos_rt[0])**2 + (a_col - pos_rt[1])**2
    dist_tlminRT = np.sqrt(tlEuc)
        
    return round(dist_tlminRT,5)

# -------
# 空間用minRTメトリックスを計算する
def space_minRT(a_row, a_col):
    len_route = len(mx_route)
    arr_sminRT = []
    for i in range(len_route):
        pos_rt  = mx_route[i]
        sqEuc   = (a_row - pos_rt[0])**2 + (a_col - pos_rt[1])**2
        distEuc = np.sqrt(sqEuc)
        arr_sminRT.append(round(distEuc,5))   
    dist_sminRT = np.min(arr_sminRT)
        
    return dist_sminRT

# -------
# 状態の表記を変更(STATE-6_landmarkへ)、空間用minRTメトリックスつき
def calc_address(a_row, a_col):

    temp_state = np.zeros([size, size])
    temp_state[a_row, a_col] = 1.0
    onehot_state = temp_state.flatten()
    
    add_state  = np.zeros(2)
    for i in range(2):
        pos_lm  = mx_lm2[i]
        sqEuc   = (a_row - pos_lm[0])**2 + (a_col - pos_lm[1])**2
        distEuc = np.sqrt(sqEuc)
        add_state[i] = round(distEuc,5)
             
    # 空間用minRTメトリックスを計算する
    dist_sminRT  = space_minRT(a_row, a_col)

    state = np.hstack([onehot_state, [dist_sminRT], add_state])

    return state


# ----------
# Maze Class
class Maze(object):
    def __init__(self, size=10, blocks_rate=0.1):
        self.size = size if size > 3 else 10
        self.blocks = int((size ** 2) * blocks_rate) 
        self.s_list = []
        self.maze_list = []
        self.e_list = []

    def create_mid_lines(self, k):
        if k == 0: self.maze_list.append(self.s_list)
        elif k == self.size - 1: self.maze_list.append(self.e_list)
        else:
            tmp_list = []
            for l in range(0,self.size):
                if l == 0: tmp_list.extend("#")
                elif l == self.size-1: tmp_list.extend("#")
                else:
                    a = random.randint(-1, 0)
                    tmp_list.extend([a])
            self.maze_list.append(tmp_list)

    def insert_blocks(self, k, s_r, e_r):
        b_y = random.randint(1, self.size-2)
        b_x = random.randint(1, self.size-2)
        if [b_y, b_x] == [1, s_r] or [b_y, b_x] == [self.size - 2, e_r]: k = k-1
        else: self.maze_list[b_y][b_x] = "#"
            
    def generate_maze(self): 
        s_r = random.randint(1, int((self.size / 2)) - 1)
        for i in range(0, self.size):
            if i == s_r: self.s_list.extend("S")
            else: self.s_list.extend("#")
        start_point = [0, s_r]

        e_r = random.randint(int((self.size / 2)) + 1, self.size - 2)
        for j in range(0, self.size):
            if j == e_r: self.e_list.extend([50])
            else: self.e_list.extend("#")
        goal_point = [self.size - 1, e_r]

        for k in range(0, self.size):
            self.create_mid_lines(k)
        
        for k in range(self.blocks):
            self.insert_blocks(k, s_r, e_r)

        return self.maze_list, start_point, goal_point

    # --------------------------------------------------
    # 盤面のCSVファイルを読み込み表示する
    def read_boardfile(self):
        # CSVファイルの読み込み
        code_csvout = "maze_board.csv"       # file name
        file_readcsv = foldername + code_csvout   # standard(training) file name  
        df = pd.read_csv(file_readcsv)
        #max_play = len(df)
        # print("データ量 max_play",max_play)
        # print(df)
        # --------------------------------------------------
        # 選択項目の読み込み
        maze = df.loc[:, "0":"9"].values.tolist()
        start_point = [0,0]
        goal_point  = [0,0]
        for i in range(size):
            for j in range(size):
                if maze[i][j] == 'S':
                    start_point = [i, j]
                if maze[i][j] == '50':
                    goal_point  = [i, j]

        self.maze_list   = maze
        self.start_point = start_point
        self.goal_point  = goal_point
        #print("----- mx_maze(read_boardfile) -----")
        #print(mx_maze)

        return self.maze_list, start_point, goal_point

    # --------------------------------------------------
    # コマの最短ルートのCSVファイルを読み込み表示する
    def read_chessfile(self):
        # CSVファイルの読み込み
        code_csvout = "chess_route.csv"       # file name
        file_readcsv = foldername + code_csvout   # standard(training) file name  
        df = pd.read_csv(file_readcsv)
        #max_play = len(df)
        # print("データ量 max_play",max_play)
        # print(df)
        # --------------------------------------------------
        # 選択項目の読み込み
        mx_route = df.loc[:, "row":"col"].values.tolist()
        #print("----- route -----")
        #print(route)

        return mx_route

# ----------
# Maze functions
class Field(object):
    def __init__(self, maze, start_point, goal_point):
        self.maze = maze
        self.start_point = start_point
        self.goal_point  = goal_point
        self.movable_vec = [[-1,0],[1,0],[0,-1],[0,1]]

    def display(self, point=None):
        field_data = copy.deepcopy(self.maze)
        if not point is None:
                y, x = point
                field_data[y][x] = "@@"
        else:
                point = ""
        print("================")
        for line in field_data:
                print ("\t" + "%3s " * len(line) % tuple(line))
        print("================")

    def all_display(self, mx_route):
        field_data = copy.deepcopy(self.maze)
        for i in range(len(mx_route)):
            point = mx_route[i]
            y, x = point[0], point[1]
            field_data[y][x] = "@@"
        print("================")
        for line in field_data:
                print ("\t" + "%3s " * len(line) % tuple(line))
        print("================")

    def get_actions(self, state):
        movables = []
        if state == self.start_point:
            y = state[0] + 1
            x = state[1]
            a = [[y, x]]
            return a
        else:
            for v in self.movable_vec:
                y = state[0] + v[0]
                x = state[1] + v[1]
                if not(0 < x < len(self.maze) and
                       0 <= y <= len(self.maze) - 1 and
                       maze[y][x] != "#" and
                       maze[y][x] != "S"):
                    continue
                movables.append([y,x])
            if len(movables) != 0:
                return movables
            else:
                return None

    # --------------------------------------------------
    # 次のコマの「決定」位置と報酬、ゲームフラグを決める
    # 新概念：マスク「#」位置に着くと、スタート地点に戻る
    def get_val(self, action_chess, state_chess, movables):
        yPos = action_chess[0]
        xPos = action_chess[1]
        # ----------------
        if action_chess == self.start_point:
            nxt_state = action_chess
            return nxt_state, 0, False, 0
        elif self.maze[yPos][xPos] == "#":
            #print("movables: ", movables)
            if len(movables) > 0:
                nxt_state = random.choice(movables)
            else:
                nxt_state = self.start_point
            v = -0.00        # -10
            return nxt_state, v, False, 1
        else:
            nxt_state = action_chess
            v = float(self.maze[yPos][xPos])
            if action_chess == self.goal_point: 
                return nxt_state, v, True, 0
            else: 
                return nxt_state, v, False, 0

    # --------------------------------------------------
    # 次のコマの位置を決める
    def nxtPosition(self, state_chess, action):
        # ----------------
        # 次の行動(ACTION)に進む
        # action number / row / col / 方向 / onehot
        # 0 / -1 / 0  / 上(up) / 1000
        # 1 / +1 / 0  / 下(down) / 0100
        # 2 / 0  / -1 / 左(left) / 0010
        # 3 / 0  / +1 / 右(right) / 0001
        # ----------------
        # ACTION_CHESS
        if action == 0:
            diff_row, diff_col = -1, 0
        if action == 1:
            diff_row, diff_col = 1, 0 
        if action == 2:
            diff_row, diff_col = 0, -1 
        if action == 3:
            diff_row, diff_col = 0, 1 
        # ----------------
        nxtPos    = [0,0]
        nxtPos[0] = state_chess[0] + diff_row   # y方向
        nxtPos[1] = state_chess[1] + diff_col   # x方向

        return nxtPos


# ----------
# Generate a maze
size = 10
barriar_rate = 0.1

maze_1 = Maze(size, barriar_rate)
#maze, start_point, goal_point = maze_1.generate_maze()
maze, start_point, goal_point = maze_1.read_boardfile()
maze_field = Field(maze, start_point, goal_point)
# 普通の表示
maze_field.display()

# 最短ルートを読み込み
mx_route = maze_1.read_chessfile()
# ルートの表示(個別表示)
#for i in range(len(mx_route)):
#    maze_field.display(mx_route[i])
# ルートの表示(全表示)
maze_field.all_display(mx_route)


#=================================================
# Calculation class(1) : ACModel
#=================================================
# ACモデルを定義する
class ACModel(nn.Module):
    def __init__(self, input_shape, num_actions):
        super(ACModel, self).__init__()
        self.fc1 = nn.Linear(input_shape,hidden_size)
        #self.fc2 = nn.Linear(hidden_size,hidden_size)
        self.fc_pi = nn.Linear(hidden_size,num_actions)
        self.fc_v = nn.Linear(hidden_size,1)

    def v(self, x):
        x = F.relu(self.fc1(x))
        #x = F.relu(self.fc2(x))
        v = self.fc_v(x)
        return v

    def pi(self, x, softmax_dim = 0):
        x = F.relu(self.fc1(x))
        #x = F.relu(self.fc2(x))
        x = self.fc_pi(x)
        prob = F.softmax(x, dim=softmax_dim)
        return prob

# --------
# Actor-Criticsのクラス
class ActorCritic():
    def __init__(self):
        super(ActorCritic, self).__init__()
        self.data = []
        self.optimizer  = optim.Adam(modelAC.parameters(), lr=learning_rate)
        self.scheduler  = optim.lr_scheduler.StepLR(self.optimizer, step_size=100, gamma=0.8)
    
    def put_data(self, transition):
        self.data.append(transition)
        
    def make_batch(self):
        s_lst, a_lst, r_lst, s_prime_lst, done_lst = [], [], [], [], []
        for transition in self.data:
            s,a,r,s_prime,done = transition
            s_lst.append(s)
            a_lst.append([a])
            r_lst.append([r])
            s_prime_lst.append(s_prime)
            done_mask = 0.0 if done else 1.0
            done_lst.append([done_mask])
        # -----
        s_lst       = np.array(s_lst)
        #a_lst       = np.array(a_lst)
        r_lst       = np.array(r_lst)
        s_prime_lst = np.array(s_prime_lst)
        s_batch, a_batch, r_batch, s_prime_batch, done_batch = torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \
                                                               torch.tensor(r_lst, dtype=torch.float), torch.tensor(s_prime_lst, dtype=torch.float), \
                                                               torch.tensor(done_lst, dtype=torch.float)
        self.data = []
        return s_batch, a_batch, r_batch, s_prime_batch, done_batch
  
    def train_net(self):
        s, a, r, s_prime, done = self.make_batch()
        td_target = r + gamma * modelAC.v(s_prime) * done
        delta = td_target - modelAC.v(s)
        # -----
        pi = modelAC.pi(s, softmax_dim=1)
        pi_a = pi.gather(1,a)
        loss = -torch.log(pi_a) * delta.detach() + F.smooth_l1_loss(modelAC.v(s), td_target.detach())
        self.optimizer.zero_grad()
        loss.mean().backward()
        self.optimizer.step()
        # -----
        actor_loss  = -(torch.log(pi_a) * delta.detach()).mean()
        critic_loss = F.smooth_l1_loss(modelAC.v(s), td_target.detach()).mean()

        return round(actor_loss.item(),4), round(critic_loss.item(),4)


#=================================================
# Calculation class(2) : Agent
#=================================================
class Agent():
    def __init__(self):

        # ---------------------------
        # 盤面データを読む
        self.maze = maze
        self.start_point = start_point
        self.goal_point = goal_point
        # ---------------------------
        # SUM_RESET
        sum_iplay     = 0
        sum_maxscore  = 0
        sum_maxturn   = 0
        sum_lsactor   = 0
        sum_lscritic  = 0
        sum_maxTD     = 0  # Total距離の最大値
        sum_sminRTD   = 0  # 空間minRT距離の最大値
        sum_tlminRTD  = 0  # タイムラインminRT距離の最大値
        # 初期化
        self.arr_iplay      = []
        self.arr_maxscore   = []
        self.arr_maxturn    = []
        self.arr_actorloss  = []
        self.arr_criticloss = []
        # 距離の分析用(プレイベース)
        self.arr_maxTD      = []  # Total距離の最大値
        self.arr_sminRTD    = []  # 空間minRT距離の最大値
        self.arr_tlminRTD   = []  # タイムラインminRT距離の最大値
        # ----------------
        for iCnt_play in range(num_episodes):

            # エピソード内の処理
            maxturn, maxscore, flg_done = self.get_episode(iCnt_play)
            sum_maxscore    = sum_maxscore + maxscore
            sum_maxturn     = sum_maxturn + maxturn
                
            # AAC学習
            actor_loss, critic_loss = model.train_net()
            sum_lsactor     = sum_lsactor + actor_loss
            sum_lscritic    = sum_lscritic + critic_loss

            # 距離
            sum_maxTD     = sum_maxTD + np.max(self.arr_Tdist)  # Total距離の最大値
            sum_sminRTD   = sum_sminRTD + np.max(self.arr_sminRT)  # 空間minRT距離の最大値
            sum_tlminRTD  = sum_tlminRTD + np.max(self.arr_tlminRT)  # タイムラインminRT距離の最大値
            # ------
            if iCnt_play%print_interval == 0 and iCnt_play != 0:
                sum_iplay      = sum_iplay + print_interval
                avg_score      = round(sum_maxscore/print_interval,4)
                avg_turn       = round(sum_maxturn/print_interval,4)
                avg_lsactor    = round(sum_lsactor/print_interval,4)
                avg_lscritic   = round(sum_lscritic/print_interval,4)
                avg_maxTD      = round(sum_maxTD/print_interval,4) - 7.0 # Total距離の最大値
                avg_sminRTD    = round(sum_sminRTD/print_interval,4)  # 空間minRT距離の最大値
                avg_tlminRTD   = round(np.sqrt(sum_tlminRTD/print_interval),4)  # タイムラインminRT距離の最大値
                # ------
                self.arr_iplay.append(sum_iplay)
                self.arr_maxscore.append(avg_score)
                self.arr_maxturn.append(avg_turn)
                self.arr_actorloss.append(avg_lsactor)
                self.arr_criticloss.append(avg_lscritic)
                print("# of episode :{}, avg_turn : {:.1f}, avg_score : {:.1f}".format(iCnt_play, avg_turn, avg_score))
                # ----------
                # 距離の保管(プレイベース)
                self.arr_maxTD.append(avg_maxTD)           # Total距離の最大値
                self.arr_sminRTD.append(avg_sminRTD)       # 空間minRT距離の最大値
                self.arr_tlminRTD.append(avg_tlminRTD)      # タイムラインminRT距離の最大値
                # ------
                # SUM_RESET
                sum_maxscore  = 0
                sum_maxturn   = 0
                sum_lsactor   = 0
                sum_lscritic  = 0
                sum_maxTD     = 0  # Total距離の最大値
                sum_sminRTD   = 0  # 空間minRT距離の最大値
                sum_tlminRTD  = 0  # タイムラインminRT距離の最大値

        # ---------------------------
        # 学習履歴の出力
        fig = plt.figure(figsize=(12, 6))
        ax1 = fig.add_subplot(1, 2, 1)
        ax1.plot(self.arr_iplay, self.arr_actorloss)
        ax1.set_xlabel('#episode')
        ax1.set_ylabel('actor_loss')
        # -----
        ax2 = fig.add_subplot(1, 2, 2)
        ax2.plot(self.arr_iplay, self.arr_criticloss)
        ax2.set_xlabel('#episode')
        ax2.set_ylabel('critic_loss')
        # -----
        fig.tight_layout()
        #fig.savefig("./REINFORCE_img.png")
        plt.show()
        
        # ---------------------------
        # 学習結果のグラフ化
        self.show_graph()

        # --------
        # 学習結果を保管する
        #model_path = './model_ActorCritic2.pth'
        #torch.save(modelAC.to('cpu').state_dict(), model_path)


    # ----------------
    # 学習結果のグラフ化
    def show_graph(self):

        # ------
        # 学習履歴の出力
        fig2 = plt.figure(figsize=(12, 9))
        ax1 = fig2.add_subplot(2, 2, 1)
        ax1.plot(self.arr_iplay, self.arr_maxscore)
        ax1.set_xlabel('#episode')
        ax1.set_ylabel('score')
        # -----
        ax2 = fig2.add_subplot(2, 2, 2)
        ax2.set_title('learning transition : maxTD, sminRTD distance')
        ax2.set_xlabel('#episode')
        ax2.set_ylabel('distance')
        ax2.grid(True)
        ax2.scatter(self.arr_iplay, self.arr_maxTD, label="maxTD", color="blue")
        ax2.scatter(self.arr_iplay, self.arr_sminRTD, label="sminRTD", color="red")
        ax2.legend(loc='best')
        # -----
        ax3 = fig2.add_subplot(2, 2, 3)
        ax3.plot(self.arr_iplay, self.arr_maxturn)
        ax3.set_xlabel('#episode')
        ax3.set_ylabel('turn')
        # -----
        ax4 = fig2.add_subplot(2, 2, 4)
        ax4.set_title('learning transition : tlminRTD distance')
        ax4.set_xlabel('#episode')
        ax4.set_ylabel('distance')
        ax4.grid(True)
        ax4.scatter(self.arr_iplay, self.arr_tlminRTD, label="tlminRTD", color="blue")
        ax4.legend(loc='best')
        # -----
        fig2.tight_layout()
        #fig2.savefig("./REINFORCE_img.png")
        plt.show()


    # ----------------
    # エピソード内の処理
    def get_episode(self, iCnt_play):

        # ----------
        # リストの初期化
        self.arr_Tdist     = []    # Total距離
        self.arr_sminRT    = []    # 空間minRT距離
        self.arr_tlminRT   = []    # タイムラインminRT距離
        # ----------
        # 状態の表記を変更(ONEHOTプラス距離)
        state_chess = self.start_point
        state = calc_address(state_chess[0], state_chess[1])
        # ----------
        # 移動稼働なオプション
        movables = maze_field.get_actions(state_chess)
        # ----------
        sum_bump = 0
        score = 0
        done  = False
        for iCnt_turn in range(num_maxturn):

            # ----------------
            # 次の行動(ACTION)に進む
            # action number / row / col / 方向 / onehot
            # 0 / -1 / 0  / 上(up) / 1000
            # 1 / +1 / 0  / 下(down) / 0100
            # 2 / 0  / -1 / 左(left) / 0010
            # 3 / 0  / +1 / 右(right) / 0001
            # ----------------
            # 状態(STATE)から行動(ACTION)を抽出する
            prob = modelAC.pi(torch.from_numpy(state).float())
            m = Categorical(prob)
            action = m.sample().item()
            action_chess = maze_field.nxtPosition(state_chess, action)
            # ----------------
            # 強制的に最適ルートを入力する
            if iCnt_play == 0 or iCnt_play == 2 or iCnt_play == 5 or iCnt_play == 8 or iCnt_play == 12 or iCnt_play == 16 or iCnt_play == 21:
                if iCnt_turn <= len(mx_route) - 1:
                    action_chess = mx_route[iCnt_turn]
            # ----------------
            #if iCnt_turn%10 == 0:
            #print("行動比較 - episode:{0}, iCnt_turn:{1}, action_chess:{2}, state_chess:{3}, ac-tion:{4}".format(iCnt_play, iCnt_turn, action_chess, state_chess, action))
            # ----------------
            # 次の報酬を得る（#に当たったら、STUCKPOINTを生成する）
            next_state_chess, reward, done, num_bump = maze_field.get_val(action_chess, state_chess, movables)
            # 状態の表記を変更(ONEHOTプラス距離)
            next_state = calc_address(next_state_chess[0], next_state_chess[1])
            if next_state_chess[0] == 9 and next_state_chess[1] == 7:
                done = True
                reward = 8000 - 5*iCnt_turn - 100*sum_bump
            #print("iCnt_turn{0}, state for imput DL:{1}".format(iCnt_turn, state))
            # ----------
            # 移動稼働なオプション
            next_movables = maze_field.get_actions(next_state_chess)
            # ----------------
            # 距離の計算
            T_Dist  = state[101] + state[102]
            dist_sminRT  = state[100]
            dist_tlminRT = timeline_minRT(state_chess[0], state_chess[1], iCnt_turn)
            # ----------
            self.arr_Tdist.append(T_Dist)       # Total距離
            self.arr_sminRT.append(dist_sminRT)    # 空間minRT距離
            self.arr_tlminRT.append(dist_tlminRT)    # タイムラインminRT距離
            # ----------------
            #model.put_data((state,action,reward,next_state,done))
            model.put_data((state, action, reward - 12 * dist_sminRT, next_state, done))
            state_chess = next_state_chess
            state = next_state
            movables = next_movables
            score += reward
            sum_bump += num_bump
            # ----------------
            if done == True:
                break

        return iCnt_turn, score, done


#=================================================
# main function            
#=================================================
if __name__ == "__main__":

    # --------
    input_shape   = 103
    num_actions   = 4
    hidden_size   = 128

    # --------
    # Hyperparameters
    learning_rate = 0.001
    gamma         = 0.98

    # --------
    # parameters
    device  = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    #device
    modelAC = ACModel(input_shape, num_actions).to(device)
    model   = ActorCritic() 
    
    # --------
    # 学習結果を読み出す
    #model_path = './model_ActorCritic.pth'
    #modelAC.load_state_dict(torch.load(model_path))

    # --------
    print_interval  = 5
    num_episodes    = 7000
    num_maxturn     = 5000

    # ----------
    # 強化学習を実行する
    Agent()


```

QEU:FOUNDER ： “学習加速のフィードバックについてですが、フィードバックは報酬に対して使います。”

**(学習曲線)**

![imageRL4-11-3](https://reinforce.github.io/images/imageRL4-11-3.jpg) 

**(パフォーマンス推移)**

![imageRL4-11-4](https://reinforce.github.io/images/imageRL4-11-4.jpg) 

D先生 : “インプット情報の量を大きくしても、ぜん～ぜん、だめじゃん・・・。ああ・・・、愕然・・・。”

QEU:FOUNDER ： “ここで、報酬へのフィードバック機構を動かしましょう。”

![imageRL4-11-5](https://reinforce.github.io/images/imageRL4-11-5.jpg) 

D先生 ： “フィードバック係数はいくらにしますか？”

QEU:FOUNDER ： “さしあたりは、係数値を1.5にしましょう。で、学習の結果をドン・・・。”

**（学習曲線）**

![imageRL4-11-6](https://reinforce.github.io/images/imageRL4-11-6.jpg) 

**（パフォーマンス推移）**

![imageRL4-11-7](https://reinforce.github.io/images/imageRL4-11-7.jpg) 

D先生 ： “あんまり効果なし・・・。FB係数を変えてみましょう。”

**（学習曲線）**

![imageRL4-11-8](https://reinforce.github.io/images/imageRL4-11-8.jpg) 

**（パフォーマンス推移）**

![imageRL4-11-9](https://reinforce.github.io/images/imageRL4-11-9.jpg) 

D先生 ： “キター！！もう一声！！”

QEU:FOUNDER ： “いろいろパラメタを変えたけど、これ以上うまくはいかなかったよ・・・。”

D先生 ： “単に、学習時間が少なすぎるだけでしょうからね・・・。”

QEU:FOUNDER ： “たぶんそうじゃないかとも思います・・・。”

D先生 ： “AAC（Advantage Actor Critics）ではどうなんでしょうかねぇ・・・。”

QEU:FOUNDER ： “AACの場合、Actor-Criticsのように学習結果をREINFORCEに導入できるのでしょうか・・・。それが出来なかったら、DQN with Experience replay（Q学習）でもいいんじゃないかと・・・。”

D先生 ： “Q学習では、movables（移動可能なaction）を直接使って評価ができるから、REINFORCEよりもDL関数が簡単になるんですよね。”

QEU:FOUNDER ： “AACへの展開については、まあペンディングと・・・。”

<iframe width="560" height="315" src="https://www.youtube.com/embed/F4tvM4Vb3A0" ti-tle="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; en-crypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

D先生 ： “Jeremy Howard先生の講座を聴講することを優先するかもしれません（笑）。なにか、新しい話題が出てくると面白いし・・・。”

## ～　まとめ　～

### ・・・　前回のつづきです　・・・

QEU:FOUNDER ： “過去に発行された「紙の本」を引っ張り出して（保守を）再構成するしかないよね。ははは・・・、ルネサンスに10年はかかるわ・・・。MMTって、小生に言わせれば「土の時代」の産物だし・・・。”

![imageRL4-11-10](https://reinforce.github.io/images/imageRL4-11-10.jpg) 

D先生 : “土の時代・・・？”

![imageRL4-11-11](https://reinforce.github.io/images/imageRL4-11-11.jpg) 

QEU:FOUNDER ： “例によって、コレ（↑）の話です。”

![imageRL4-11-12](https://reinforce.github.io/images/imageRL4-11-12.jpg) 

QEU:FOUNDER ： “一見、同じことを話しているように見えても、その目的が違うと全く違うものになるんですよ。それを簡単にいうと、「土の時代」と「風の時代」の差になります。”

<iframe width="560" height="315" src="https://www.youtube.com/embed/V9KefpazXoA" ti-tle="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; en-crypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

D先生 : “発電でいうと、「核発電（ウラン、土）」と「風力発電（自然エネルギー）」のちがいでしょうか・・・。”

![imageRL4-11-13](https://reinforce.github.io/images/imageRL4-11-13.jpg) 

QEU:FOUNDER ： “前回の風の時代って、今と似た時代であることがわかるよね。なんといっても、今、「風って旬なんです」よ。あの宗主国様でもね・・・。”

![imageRL4-11-14](https://reinforce.github.io/images/imageRL4-11-14.jpg) 

D先生 : “「土（ウラン）」にこだわる意味がわからないんだよなぁ・・・。”


