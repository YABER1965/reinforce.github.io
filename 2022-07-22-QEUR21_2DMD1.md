## QEUR21_2DMZ1:　自分なりのベースラインをつくる

## ～　もう、RT法にはこだわらない　～

QEU:FOUNDER ; “前回も同様にメイズ（MAZE：迷路）の強化学習を行ったが、今回もこのブログ（↓）をお手本にしましょう。”

![imageRL4-2-1](https://reinforce.github.io/images/imageRL4-2-1.jpg)

D先生 : “たしか、このブログのサンプルプログラムは収束しにくかったような・・・。ひょっとして、ひそかに修正されていたとか？”

QEU:FOUNDER : “あの事例は同じモノですよ。ですから、しょうがないので自分で収束しやすいモノに修正しましょう。Cliff_Walkingのプロジェクトでも同じ目にあったように、状態(STATE)が2次元ベクトルであれば収束しにくいですよ。ですから、そこを含めてコードを全面的にかえましょう。ついでにKerasじゃなくて、pyTorchに入れ替えて・・・。”

![imageRL4-2-2](https://reinforce.github.io/images/imageRL4-2-2.jpg)

D先生： “2次元の表現では、環境(Environment)の特性と抽象化する関数がマッチすると強いがそうでないと収束しないですからね。”

QEU:FOUNDER : “それでは、修正版のプログラムをドン・・・。”


```python
# ----------------
# メイズ(MAZE)ゲームの強化学習システム
# step1 : RTを動的メトリックスとして活用する
# step1_dqn_maze_pyTorch(onehot).py
# step1 : DQN-Experience replay
# ---------------- 
# 数値計算のﾗｲﾌﾞﾗﾘ読み込み
import numpy as np
import pandas as pd
import math, copy, random, time
from collections import deque, Counter
from scipy.special import softmax
from IPython.display import clear_output
# ----------------
import torch
import torch.nn.functional
import torch.utils.data
# ---------------- 
import matplotlib.pyplot as plt
#%matplotlib inline

# =================================================
# difinition of function
# =================================================
# 状態の表記を変更(STATE-oneHOTへ)
def calc_address(a_row, a_col, size):

    state = np.zeros([size, size])
    a_row = int(a_row + 0.01)
    a_col = int(a_col + 0.01)
    state[a_row, a_col] = 1.0

    return state.flatten().tolist()

# -------
# Maze Class
class Maze(object):
    def __init__(self, size=10, blocks_rate=0.1):
        self.size = size if size > 3 else 10
        self.blocks = int((size ** 2) * blocks_rate) 
        self.s_list = []
        self.maze_list = []
        self.e_list = []

    def create_mid_lines(self, k):
        if k == 0: self.maze_list.append(self.s_list)
        elif k == self.size - 1: self.maze_list.append(self.e_list)
        else:
            tmp_list = []
            for l in range(0,self.size):
                if l == 0: tmp_list.extend("#")
                elif l == self.size-1: tmp_list.extend("#")
                else:
                    a = random.randint(-1, 0)
                    tmp_list.extend([a])
            self.maze_list.append(tmp_list)

    def insert_blocks(self, k, s_r, e_r):
        b_y = random.randint(1, self.size-2)
        b_x = random.randint(1, self.size-2)
        if [b_y, b_x] == [1, s_r] or [b_y, b_x] == [self.size - 2, e_r]: k = k-1
        else: self.maze_list[b_y][b_x] = "#"
            
    def generate_maze(self): 
        s_r = random.randint(1, int((self.size / 2)) - 1)
        for i in range(0, self.size):
            if i == s_r: self.s_list.extend("S")
            else: self.s_list.extend("#")
        start_point = [0, s_r]

        e_r = random.randint(int((self.size / 2)) + 1, self.size - 2)
        for j in range(0, self.size):
            if j == e_r: self.e_list.extend([50])
            else: self.e_list.extend("#")
        goal_point = [self.size - 1, e_r]

        for k in range(0, self.size):
            self.create_mid_lines(k)
        
        for k in range(self.blocks):
            self.insert_blocks(k, s_r, e_r)

        return self.maze_list, start_point, goal_point

# -------
# Maze functions
class Field(object):
    def __init__(self, maze, start_point, goal_point):
        self.maze = maze
        self.start_point = start_point
        self.goal_point = goal_point
        self.movable_vec = [[1,0],[-1,0],[0,1],[0,-1]]

    def display(self, point=None):
        field_data = copy.deepcopy(self.maze)
        if not point is None:
                y, x = point
                field_data[y][x] = "@@"
        else:
                point = ""
        for line in field_data:
                print ("\t" + "%3s " * len(line) % tuple(line))

    def get_actions(self, state):
        movables = []
        if state == self.start_point:
            y = state[0] + 1
            x = state[1]
            a = [[y, x]]
            return a
        else:
            for v in self.movable_vec:
                y = state[0] + v[0]
                x = state[1] + v[1]
                if not(0 < x < len(self.maze) and
                       0 <= y <= len(self.maze) - 1 and
                       maze[y][x] != "#" and
                       maze[y][x] != "S"):
                    continue
                movables.append([y,x])
            if len(movables) != 0:
                return movables
            else:
                return None

    def get_val(self, state):
        y, x = state
        if state == self.start_point: return 0, False
        else:
            v = float(self.maze[y][x])
            if state == self.goal_point: 
                return v, True
            else: 
                return v, False

# -------
# Generate a maze
size = 10
barriar_rate = 0.1

maze_1 = Maze(size, barriar_rate)
maze, start_point, goal_point = maze_1.generate_maze()
maze_field = Field(maze, start_point, goal_point)

maze_field.display()


#=================================================
# Deep Learning Model class            
#=================================================
# PyTorchのDLの定義
class Net(torch.nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = torch.nn.Linear(dim_input, 128)
        self.fc1.weight.data.normal_(0, 0.1)
        self.fc2 = torch.nn.Linear(128, 128)
        self.fc2.weight.data.normal_(0, 0.1)
        self.fc3 = torch.nn.Linear(128, 128)
        self.fc3.weight.data.normal_(0, 0.1)
        self.fc4 = torch.nn.Linear(128, dim_output)
        self.fc4.weight.data.normal_(0, 0.1)
        
    def forward(self, x):
        x = torch.nn.functional.relu(self.fc1(x))
        x = torch.nn.functional.relu(self.fc2(x))
        x = torch.nn.functional.relu(self.fc3(x))
        x = self.fc4(x)
        return x

# =======================================
# =======================================
# Solving the maze in Deep Q-learning
class DQN_Solver:
    def __init__(self, state_size, action_size):
        self.state_size = state_size # list size of state
        self.action_size = action_size # list size of action
        self.memory = deque(maxlen=10000) # memory space
        self.gamma = 0.96 # discount rate
        self.epsilon = 1.0 # randomness of choosing random action or the best one
        self.e_decay = 0.9995 # epsilon decay rate
        self.e_min = 0.01 # minimum rate of epsilon
        self.learning_rate = 0.001 # learning rate of neural network
        # --------------------------------------------------
        # crate instance for input
        self.eval_net, self.target_net = Net(), Net()                           # Netを利用して２つのニューラルネットをつくる
        self.learn_step_counter = 0    
        # --------------------------------------------------
        # Save and load the model via state_dict
        #self.eval_net.load_state_dict(torch.load(file_input_model))
        # --------------------------------------------------
        # set validaiton mode
        self.eval_net.eval()
        # --------------------------------------------------
        # set training parameters
        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=self.learning_rate)
        self.criterion = torch.nn.MSELoss()

    # ----------------
    # PyTorchモデルの保存  
    #def save_models(self):
        #torch.save(self.eval_net.state_dict(), file_output_model)

    # remember state, action, its reward, next state and next possible action. done means boolean for goal
    def remember_memory(self, state, action, reward, next_state, next_movables, done):
        self.memory.append((state, action, reward, next_state, next_movables, done))

    # choosing action depending on epsilon
    def choose_action(self, state, movables):
        acType = "random"
        Qvalue = -0.001
        #print("movables: ", movables)
        if self.epsilon >= random.random():
            # randomly choosing action
            action = random.choice(movables)
            return acType, action, Qvalue
        else:
            # choosing the best action from model.predict()
            acType, action, Qvalue = self.choose_best_action(state, movables)
            return acType, action, Qvalue
        
    # choose the best action to maximize reward expectation
    def choose_best_action(self, state, movables):

        action = [0,0]
        acType = "machine"
        Qvalue = -0.001
        iCnt = 0
        for a in movables:

            # ONEHOT状態(INPUT_STATE)を生成する
            input_state = calc_address(state[0], state[1], size)
            np_action = np.hstack([input_state, a])
            #print("np_action: ",np_action)
            if iCnt == 0:
                mx_input = [np_action]  # 状態(S)行動(A)マトリックス
            else:
                mx_input = np.append(mx_input, [np_action], axis=0)     # 状態(S)マトリックス      
            iCnt = iCnt + 1
        # print("----- mx_input -----")
        # print(mx_input)
        # --------------------------------------------------
        # generate new 'x'
        x_input_tensor = torch.tensor(mx_input).float()
        # predict 'y'
        with torch.no_grad():
            y_pred_tensor = self.eval_net(x_input_tensor)
        # convert tensor to numpy
        y_pred      = y_pred_tensor.data.numpy().flatten()
        Qvalue      = np.max(y_pred)
        temp_order  = np.argmax(y_pred)
        action      = movables[temp_order]

        return acType, action, Qvalue


    # this experience replay is going to train the model from memorized states, actions and rewards
    def replay_experience(self, batch_size):

        # ターゲットNetのパラメタを更新する
        if self.learn_step_counter < 5 or self.learn_step_counter % TARGET_REPLACE_ITER == 0:                  # 最初5回は毎回に更新し、あとは定期的に更新する
            self.target_net.load_state_dict(self.eval_net.state_dict())         # 評価NetのパラメタをターゲットNetに引き渡す
        self.learn_step_counter += 1   
        # --------------------------------------------------
        batch_size = min(batch_size, len(self.memory))
        minibatch = random.sample(self.memory, batch_size)
        X, Y = [], []
        # --------------------------------------------------
        for ibat in range(batch_size):
            state, action, reward, next_state, next_movables, done = minibatch[ibat]

            # ONEHOT状態(INPUT_STATE)を生成する
            input_state = calc_address(state[0], state[1], size)
            state_action_curr = np.hstack([input_state, action])
            #print("state_action_curr",state_action_curr)
            # ----------------
            if done:
                target_f = reward
            else:
                next_rewards = []
                iCnt = 0
                for a in next_movables:

                    # ONEHOT状態(INPUT_STATE)を生成する
                    input_next_state = calc_address(next_state[0], next_state[1], size)
                    np_next_s_a = np.hstack([input_next_state, a])
                    
                    if iCnt == 0:
                        mx_input = [np_next_s_a]  # 状態(S)行動(A)マトリックス
                    else:
                        mx_input = np.concatenate([mx_input, [np_next_s_a]], axis=0)     # 状態(S)マトリックス
                    
                    iCnt = iCnt + 1
                # ----------------
                # generate new 'x'
                x_input_tensor = torch.tensor(mx_input).float()
                # predict 'y'
                with torch.no_grad():
                    y_pred_tensor = self.target_net(x_input_tensor)
                # convert tensor to numpy
                y_pred = y_pred_tensor.data.numpy().flatten()
                np_n_r_max = np.amax(y_pred)
                target_f = reward + self.gamma * np_n_r_max
            # ----------------
            if ibat == 0:
                X = np.array([state_action_curr])  # 状態(S)行動(A)マトリックス
            else:
                X = np.concatenate([X, np.array([state_action_curr])], axis=0)  # 状態(S)行動(A)マトリックス
            Y = np.append(Y,target_f)

        # --------------------------------------------------
        # TRAINING
        # convert numpy array to tensor
        state_action_eval = torch.from_numpy(X).float()
        q_target = torch.from_numpy(Y.reshape(-1, 1)).float()
        #print("state_action_eval:",state_action_eval)
        #print("q_target:",q_target)

        # --- building model ---
        q_eval = self.eval_net(state_action_eval)
        # calculate loss
        loss = self.criterion(q_eval, q_target)
        # update weights
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        # Show progress
        #print('learn done -- [epsilon: {0}, loss: {1}]'.format(self.epsilon, loss))
        if self.epsilon > self.e_min:
            self.epsilon *= self.e_decay

        return round(self.epsilon,5) , round(loss.data.item(),5)


# ---------------------------
# パラメタの設定
SLEEP_TIME  = 0.01
TARGET_REPLACE_ITER = 100             # ターゲットNet更新頻度
state_size  = size*size
action_size = 2
dim_input   = size*size + action_size
dim_output  = 1
dql_solver  = DQN_Solver(state_size, action_size)
BATCH_SIZE  = 128                      # サンプルサイズ

# number of episodes to run training
episodes = 3000

# number of times to sample the combination of state, action and reward
times    = 5000

# ---------------------------
# 出力用:Pytorchモデルのファイル名
#comment_output_model = "initial"
#code_output_model = "model_cliffwalk_DQNER_{0}.pt".format(comment_output_model)  # モデルの指定
#file_output_model = foldername + code_output_model  # ファイルパス名の生成

#=================================================
# Calculation class(3) : Agent
#=================================================
class Agent():
    # -----
    def __init__(self):

        # ---------------------------
        # 記録用パラメタ類(プレイベース)
        self.arr_iplay     = []  # count game play    プレイ番号
        self.arr_maxturn   = []  # turn game play    ターン数
        self.arr_maxscore  = []  # rl_score game play    報酬の総和
        self.arr_victory   = []  # victory    勝利したか
        self.arr_loss      = []  # DQN-Experience Replay学習
        self.arr_epsilon   = []  # ε-Greedy        
        # ---------------------------
        # Q値の分析用(プレイベース)
        self.arr_numQV     = []  # QV値のN数
        self.arr_maxQV     = []  # QV値の最大値
        self.arr_q25QV     = []  # QV値の4分の1値
        self.arr_q75QV     = []  # QV値の4分の3値
        # ---------------------------
        # エピソードの実行
        for iCnt_play in range(episodes):

            maxturn, maxscore, flag_goal = self.get_episode(iCnt_play)
            # ----------
            # DQN-Experience Replay学習  
            val_epsilon, val_loss = dql_solver.replay_experience(BATCH_SIZE)
            # 学習結果の出力
            if iCnt_play % 5 == 0:
                print("iCnt_play:{0}, maxturn:{1}, maxscore:{2}, flag_goal:{3}, epsilon:{4}, loss:{5}".format(iCnt_play, maxturn, maxscore, flag_goal, val_epsilon, val_loss))

            # ----------
            # 記録用パラメタ類(プレイベース)の追加
            self.arr_iplay.append(iCnt_play)        # count game play    プレイ番号
            self.arr_maxturn.append(maxturn)        # max_turn   ゲームのターン数
            self.arr_maxscore.append(maxscore)      # rl_score game play    最終プレイスコア
            self.arr_victory.append(flag_goal)      # victory    勝利したか
            self.arr_loss.append(val_loss)          # DQN-Experience Replay学習
            self.arr_epsilon.append(val_epsilon)    # イプシロン       
            # ----------
            # Q値の保管(プレイベース)
            self.arr_numQV.append(maxturn)      # QV値のN数
            self.arr_maxQV.append(np.max(self.arr_predQV))                 # QV値の最大値
            self.arr_q25QV.append(np.percentile(self.arr_predQV, q=25))    # QV値の4分の1値
            self.arr_q75QV.append(np.percentile(self.arr_predQV, q=75))    # QV値の4分の3値
            # ----------
            # しばらくすれば表示が消えます
            if iCnt_play%50 == 0:
                time.sleep(SLEEP_TIME)
                clear_output(wait=True)

        # --------------------------------------------------
        # 学習履歴を出力する
        fig = plt.figure(figsize=(14, 6))
        ax1 = fig.add_subplot(1, 2, 1)
        ax1.set_title('learning transition : epsilon')
        ax1.plot(self.arr_iplay, self.arr_epsilon, label="epsilon", color="blue")
        ax1.set_xlabel('#episode')
        ax1.set_ylabel('epsilon')
        ax1.legend(loc='best')
        # -----
        ax2 = fig.add_subplot(1, 2, 2)
        ax2.set_title('learning transition : Learning loss')
        ax2.set_xlabel('#episode')
        ax2.set_ylabel('Loss amount')
        ax2.plot(self.arr_iplay, self.arr_loss, label="loss", color="blue")
        ax2.legend(loc='best')
        plt.show()

        # --------------------------------------------------
        # グラフを表示する
        self.show_graph()


    # --------------------------------------------------
    # エピソードを運用する
    def get_episode(self, iCnt_play):

        # ---------------------------
        # 記録用パラメタ類(ターンベース)
        self.arr_iturn     = []    # ターン・カウンタリスト
        self.arr_orders    = []    # 指示リスト(アドレス)
        self.arr_acType    = []    # 指示のタイプ
        self.arr_scores    = []    # ゲームスコアリスト
        self.arr_dones     = []    # ゲームオーバーフラグ
        self.arr_predQV    = []    # Q値のリスト

        state = start_point
        score = 0
        for time in range(times):
            movables = maze_field.get_actions(state)
            acType, action, Qvalue = dql_solver.choose_action(state, movables)
            reward, done = maze_field.get_val(action)
            score = score + reward
            next_state = action
            next_movables = maze_field.get_actions(next_state)
            dql_solver.remember_memory(state, action, reward, next_state, next_movables, done)
            if done or time == (times - 1):
                #if iCnt_play % 5 == 0:
                #    print("episode: {}/{}, score: {}, iCnt_play: {:.2} \t @ {}"
                #            .format(iCnt_play, episodes, score, dql_solver.epsilon, time))
                break
            state = next_state

            # ---------------------------
            # 記録用リストを追記する
            self.arr_iturn.append(time)    # ターン・カウンタリスト
            self.arr_orders.append(action)      # 指示リスト(アドレス)
            self.arr_acType.append(acType)      # 指示のタイプ
            self.arr_scores.append(reward)      # ゲームスコアリスト   
            self.arr_dones.append(done)         # ゲームオーバーフラグ
            self.arr_predQV.append(Qvalue)      # Q値のリスト

        # run experience replay after sampling the state, action and reward for defined times
        #val_epsilon, val_loss = dql_solver.replay_experience(BATCH_SIZE)
        # 結果の出力
        maxturn   = time
        maxscore  = score
        flag_goal = done

        return maxturn, maxscore, flag_goal


    # ----------------
    # 学習結果のグラフ化
    def show_graph(self):

        fig2 = plt.figure(figsize=(14, 12))
        # -----
        ax1 = fig2.add_subplot(2, 2, 1)
        ax1.set_title('DQN parameter transition: QValue')
        ax1.set_xlabel('#episode')
        ax1.set_ylabel('QValue')
        ax1.plot(self.arr_iplay, self.arr_maxQV, label="max_QV", color="blue")
        ax1.plot(self.arr_iplay, self.arr_q25QV, label="q25_QV", color="red")
        ax1.plot(self.arr_iplay, self.arr_q75QV, label="q75_QV", color="green")
        ax1.grid(True)
        ax1.legend(loc='best')
        # -----
        # 移動平均を付ける
        y_rolling = pd.Series(self.arr_maxturn).rolling(window=12, center=True).mean()
        ax3 = fig2.add_subplot(2, 2, 2)
        ax3.set_title('learning transition : maxturn')
        ax3.set_xlabel('#episode')
        ax3.set_ylabel('maxturn')
        ax3.grid(True)
        ax3.plot(self.arr_iplay, self.arr_maxturn, label="original", color="blue")
        ax3.plot(self.arr_iplay, y_rolling, label="moving", color="red")
        ax3.legend(loc='best')
        # -----
        # 移動平均を付ける
        y_rolling = pd.Series(self.arr_maxscore).rolling(window=12, center=True).mean()
        ax4 = fig2.add_subplot(2, 2, 3)
        ax4.set_title('learning transition : maxscore')
        ax4.set_xlabel('#episode')
        ax4.set_ylabel('maxscore')
        ax4.grid(True)
        ax4.plot(self.arr_iplay, self.arr_maxscore, label="original", color="blue")
        ax4.plot(self.arr_iplay, y_rolling, label="moving", color="red")
        ax4.legend(loc='best')
        # -----
        fig2.tight_layout()
        #fig.savefig("./AAC_img.png")
        plt.show()


# ---------------------------
# ゲームを実行する
Agent()

```

QEU:FOUNDER : “今回は、「素うどん」だよ・・・。メトリックスがまったくないんで・・・”

**（学習曲線）**

![imageRL4-2-3](https://reinforce.github.io/images/imageRL4-2-3.jpg)

**（パフォーマンス推移）**

![imageRL4-2-4](https://reinforce.github.io/images/imageRL4-2-4.jpg)

D先生： “かなり、学習が「うまく行っている」と言えるでしょう・・・。次は、この学習プログラムになんらかのメトリックスを追加するんでしょ？またもやRT距離を使うんですか？”

QEU:FOUNDER : “たぶんRTは使いません。Cliff_WalkingのときでもRT法は使いにくいと思ったし・・・。”

![imageRL4-2-5](https://reinforce.github.io/images/imageRL4-2-5.jpg)

D先生： “しかし、収束を加速するには、「ベストパターンからの距離」を評価し、定量化する必要があります。”

QEU:FOUNDER : “RT法は画像パターンには使いやすいが、今回のように比較する配列の大きさが変動すると使いにくいです。”

D先生： “次は普通にスタート距離とゴール距離を追加して学習させてみましょう。”

## ～　まとめ　～

QEU:FOUNDER ： “困ったですね・・・。”

D先生 : “は？なにが？”

<iframe width="560" height="315" src="https://www.youtube.com/embed/YxVIDplUqrM" ti-tle="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; en-crypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

QEU:FOUNDER ： “これ（↓）は、信じられる？”

![imageRL4-2-6](https://reinforce.github.io/images/imageRL4-2-6.jpg)

D先生 : “これは、求人広告のようです。アルバイトかな？1時間当たりの賃金は・・・。ゲッ！！”

![imageRL4-2-7](https://reinforce.github.io/images/imageRL4-2-7.jpg)

QEU:FOUNDER ： “あの国はある意味、「究極の自由社会」だからね。悪い方向に循環すると歯止めがかからないんですよ。その意味で生活安全保障のしくみって、必要なんですよ。お偉い方は自己責任で不要だとおもっているんでしょうが・・・。“

<iframe width="560" height="315" src="https://www.youtube.com/embed/QOo8nYQrQpo" ti-tle="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; en-crypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

D先生 : “困りますね・・・。ゲッ！！”


