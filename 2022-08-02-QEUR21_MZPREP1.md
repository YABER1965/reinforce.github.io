## QEUR21_MZPREP1: まずはA3Cから始めましょう（カリキュラム学習）

## ～　まずはちょっとだけ前回のつづきをば・・・　～

QEU:FOUNDER ; “従来タイプの生産計画って、パッケージ（ロット）が単位になっているんです。「かんばん」が代表例ね・・・。・・・でも、IoTであらゆる管理が知能化されていれば端数の処理って大して問題じゃないでしょ？さらにいえば、QEUシステムではすでに外観検査自動機を設計しました。良品であることが保証されている場合には端数でも十分に流通できます。まあ・・・、そういうアイデアが「風の時代」から始まるんですね（笑）。”

![imageRL5-2-1](https://reinforce.github.io/images/imageRL5-2-1.jpg) 

D先生 : “生産力を強化するためのポリシーが政治上はっきりと発信されていれば、お金は刷れますよ。「土の時代の産物にこだわった」り、「生産性のない無駄な兵器を買う」から円安になるわけで・・・（笑）。”

QEU:FOUNDER ; “もっと大きな話をすると、搬送台車(AGV)が知能化すれば経済学も変わるかもね。少なくとも「仕掛品」はなくなるだろうし・・・？”

![imageRL5-2-2](https://reinforce.github.io/images/imageRL5-2-2.jpg) 

D先生 ; “えっ？なんで仕掛品が減るの？AGVの導入でリーン（Lean）になる理由はないと思いますが・・・。”

![imageRL5-2-3](https://reinforce.github.io/images/imageRL5-2-3.jpg) 

QEU:FOUNDER ; “工程(Workshop)間を動き回る搬送台車(AGV)が知能化すれば、AGVが人間のように工程に対して注文し、加工後の品を購入することができます。そうすると、従来の仕掛品はすべて製品になるわけですから、仕掛品はなくなります。”

D先生 ; “まあ、言葉の定義が変わったことは変わりますよ、でもあまりメリットがあるように思いません。”

![imageRL5-2-4](https://reinforce.github.io/images/imageRL5-2-4.jpg) 

QEU:FOUNDER ; “さて、図（↑）のように甲乙の2種類の製品がありました。現在、製品を作る前で仕掛品がたくさんあります。部品は甲乙汎用なのですが、仕掛品が製品ラインに紐付けられているので動かせません。”

![imageRL5-2-5](https://reinforce.github.io/images/imageRL5-2-5.jpg) 

QEU:FOUNDER ; “知能をもったAGVがあれば、その製品と部品を縛り付けている「ひも」が切れてしまいます。じゃあ、需要に応じて製品甲の仕掛品の一部を製品乙に移せばいいじゃないですか。そうすれば、製品が生産され、物理的な意味の仕掛品の量が大きく減ることになります。”

D先生 ; “それって、手間がかかって逆に非効率になるんじゃないですか？”

QEU:FOUNDER ; “**「エネルギー源有限の世界」では、この方法は明らかに非効率です。でも、「エネルギー源無限の世界」では問題ないんですよ。**”

C部長 : “この手の話、どっかで聞いたことがあります・・・。昔の、東大阪や蒲田などの工場下町って、こんな感じじゃなかったですか？”

QEU:FOUNDER ; “そうそう・・・。それを目指したい。”

## ～　カリキュラム学習は、まだ道半ば　～

QEU:FOUNDER ; “じゃあ、3次元メイズの準備編、「本題」に入ろうか・・・。さしあたりは、この2次元メイズを解きましょう。”

![imageRL5-2-6](https://reinforce.github.io/images/imageRL5-2-6.jpg) 

D先生 : “XY座標を使った強化学習で、ここまで大きいメイズ（27x27）ができるのかなぁ・・・。とんでもない時間がかかるかも・・・。”

（カリキュラムの例）

__10 x 10メイズ　→　13 x 13メイズ　→　16 x 16メイズ　→　…__

QEU:FOUNDER ; “それでは計算負荷を軽くするためにカリキュラム学習をしてみましょう・・・。でもね、率直言って難しいと思うよ。**10x10のメイズで学習収束がギリギリ**だったんだから。”

**（学習曲線）**

![imageRL5-2-7](https://reinforce.github.io/images/imageRL5-2-7.jpg) 

**（パフォーマンス推移）**

![imageRL5-2-8](https://reinforce.github.io/images/imageRL5-2-8.jpg) 

QEU:FOUNDER ; “ははは・・・。その結果、かなり強引な学習になった・・・。プログラムをドン！！”


```python
# ----------------
# メイズ(MAZE)ゲームの強化学習システム
# step1 : 3Dメイズの準備段階です(ORIGINAL)
# step1 : reinforce_3DMazePrep_pyTorch(A3C_sminRT_size27).py
# step1 : A3CとsminRT距離を導入する(フィードバックはオプションです)
# ---------------- 
# 数値計算のﾗｲﾌﾞﾗﾘ読み込み
import numpy as np
import pandas as pd
import math, copy, random, time
# -----
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable
# -----
import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = (16, 10)

# =================================================
# Game board difinition 
# =================================================
foldername  = "./MAZE_test/"
mx_lm = [[0, 2],[26, 25]]

# =================================================
# difinition of STATE CALCULATION function
# =================================================
# タイムライン用minRTメトリックスを計算する
def timeline_minRT(a_row, a_col, iCnt_turn):
    len_route = len(mx_route)
    if iCnt_turn <= len_route - 1:
        pos_rt  = mx_route[iCnt_turn]
    else:
        pos_rt  = mx_route[len_route - 1]
    # -----
    tlEuc   = (a_row - pos_rt[0])**2 + (a_col - pos_rt[1])**2
    dist_tlminRT = np.sqrt(tlEuc)
        
    return round(dist_tlminRT,5)

# -------
# 空間用minRTメトリックスを計算する
def space_minRT(a_row, a_col):
    len_route = len(mx_route)
    arr_sminRT = []
    for i in range(len_route):
        pos_rt  = mx_route[i]
        sqEuc   = (a_row - pos_rt[0])**2 + (a_col - pos_rt[1])**2
        distEuc = np.sqrt(sqEuc)
        arr_sminRT.append(round(distEuc,5))   
    dist_sminRT = np.min(arr_sminRT)
        
    return dist_sminRT

# -------
# 状態の表記を変更(XY座標へ)、空間用minRTメトリックスつき
def calc_address(a_row, a_col, num_bump, mx_lm):

    add_state = np.zeros(2)
    for i in range(2):
        pos_lm  = mx_lm[i]
        sqEuc   = (a_row - pos_lm[0])**2 + (a_col - pos_lm[1])**2
        distEuc = np.sqrt(sqEuc)
        add_state[i] = round(distEuc,5)
             
    # 空間用minRTメトリックスを計算する
    dist_sminRT  = space_minRT(a_row, a_col)

    state = np.hstack([[a_row, a_col, num_bump], add_state, [dist_sminRT]])

    return state

# ----------
# Maze Class
class Maze(object):
    def __init__(self, size=10, blocks_rate=0.1):
        self.size = size if size > 3 else 10
        self.blocks = int((size ** 2) * blocks_rate) 
        self.s_list = []
        self.maze_list = []
        self.e_list = []

    def create_mid_lines(self, k):
        if k == 0: self.maze_list.append(self.s_list)
        elif k == self.size - 1: self.maze_list.append(self.e_list)
        else:
            tmp_list = []
            for l in range(0,self.size):
                if l == 0: tmp_list.extend("#")
                elif l == self.size-1: tmp_list.extend("#")
                else:
                    a = random.randint(-1, 0)
                    tmp_list.extend([a])
            self.maze_list.append(tmp_list)

    def insert_blocks(self, k, s_r, e_r):
        b_y = random.randint(1, self.size-2)
        b_x = random.randint(1, self.size-2)
        if [b_y, b_x] == [1, s_r] or [b_y, b_x] == [self.size - 2, e_r]: k = k-1
        else: self.maze_list[b_y][b_x] = "#"
            
    def generate_maze(self): 
        s_r = random.randint(1, int((self.size / 2)) - 1)
        for i in range(0, self.size):
            if i == s_r: self.s_list.extend("S")
            else: self.s_list.extend("#")
        start_point = [0, s_r]

        e_r = random.randint(int((self.size / 2)) + 1, self.size - 2)
        for j in range(0, self.size):
            if j == e_r: self.e_list.extend([50])
            else: self.e_list.extend("#")
        goal_point = [self.size - 1, e_r]

        for k in range(0, self.size):
            self.create_mid_lines(k)
        
        for k in range(self.blocks):
            self.insert_blocks(k, s_r, e_r)

        return self.maze_list, start_point, goal_point

    # --------------------------------------------------
    # 盤面のCSVファイルを読み込み表示する
    def read_boardfile(self):
        # CSVファイルの読み込み
        code_csvout = "maze_board27.csv"       # file name
        file_readcsv = foldername + code_csvout   # standard(training) file name  
        df = pd.read_csv(file_readcsv)
        #max_play = len(df)
        # print("データ量 max_play",max_play)
        # print(df)
        # --------------------------------------------------
        # 選択項目の読み込み
        maze = df.loc[:, "0":"26"].values.tolist()
        start_point = [0,0]
        goal_point  = [0,0]
        for i in range(size):
            for j in range(size):
                if maze[i][j] == 'S':
                    start_point = [i, j]
                if maze[i][j] == '5000':
                    goal_point  = [i, j]

        self.maze_list   = maze
        self.start_point = start_point
        self.goal_point  = goal_point
        print("goal_point", goal_point)
        print("----- mx_maze(read_boardfile) -----")
        print(maze)

        return self.maze_list, start_point, goal_point

    # --------------------------------------------------
    # コマの最短ルートのCSVファイルを読み込み表示する
    def read_chessfile(self):
        # CSVファイルの読み込み
        code_csvout = "chess_route27.csv"       # file name
        file_readcsv = foldername + code_csvout   # standard(training) file name  
        df = pd.read_csv(file_readcsv)
        #max_play = len(df)
        # print("データ量 max_play",max_play)
        # print(df)
        # --------------------------------------------------
        # 選択項目の読み込み
        mx_route    = df.loc[:, "row":"col"].values.tolist()
        ref_action  = df.loc[:, "action"].values.tolist()
        print("ref_action: ", ref_action)
        #print("----- route -----")
        #print(route)

        return mx_route, ref_action

# ----------
# Maze functions
class Field(object):
    def __init__(self, maze, start_point, goal_point):
        self.maze = maze
        self.start_point = start_point
        self.goal_point  = goal_point
        self.movable_vec = [[-1,0],[1,0],[0,-1],[0,1]]

    def display(self, point=None):
        field_data = copy.deepcopy(self.maze)
        if not point is None:
                y, x = point
                field_data[y][x] = "@@"
        else:
                point = ""
        print("================")
        for line in field_data:
                print ("\t" + "%3s " * len(line) % tuple(line))
        print("================")

    def all_display(self, mx_route):
        field_data = copy.deepcopy(self.maze)
        for i in range(len(mx_route)):
            point = mx_route[i]
            y, x = point[0], point[1]
            field_data[y][x] = "@@"
        print("================")
        for line in field_data:
                print ("\t" + "%3s " * len(line) % tuple(line))
        print("================")

    def get_actions(self, state):
        movables = []
        if state == self.start_point:
            y = state[0] + 1
            x = state[1]
            a = [[y, x]]
            return a
        else:
            for v in self.movable_vec:
                y = state[0] + v[0]
                x = state[1] + v[1]
                if not(0 < x < len(self.maze) and
                       0 <= y <= len(self.maze) - 1 and
                       self.maze[y][x] != "#" and
                       self.maze[y][x] != "S"):
                    continue
                movables.append([y,x])
            if len(movables) != 0:
                return movables
            else:
                return None

    # --------------------------------------------------
    # 次のコマの「決定」位置と報酬、ゲームフラグを決める
    # 新概念：マスク「#」位置に着くと、スタート地点に戻る
    def get_val(self, action_chess, state_chess, movables):
        yPos = action_chess[0]
        xPos = action_chess[1]
        # ----------------
        if action_chess == self.start_point:
            nxt_state = action_chess
            return nxt_state, 0, False, 0
        elif self.maze[yPos][xPos] == "#":
            #print("movables: ", movables)
            if len(movables) > 0:
                nxt_state = random.choice(movables)
            else:
                nxt_state = self.start_point
            v = -0.01        # -10
            return nxt_state, v, False, 1
        else:
            nxt_state = action_chess
            v = float(self.maze[yPos][xPos])
            if action_chess == self.goal_point: 
                return nxt_state, v, True, 0
            else: 
                return nxt_state, v, False, 0

    # --------------------------------------------------
    # 次のコマの位置を決める
    def nxtPosition(self, state_chess, action):
        # ----------------
        # 次の行動(ACTION)に進む
        # action number / row / col / 方向 / onehot
        # 0 / -1 / 0  / 上(up) / 1000
        # 1 / +1 / 0  / 下(down) / 0100
        # 2 / 0  / -1 / 左(left) / 0010
        # 3 / 0  / +1 / 右(right) / 0001
        # ----------------
        # ACTION_CHESS
        if action == 0:
            diff_row, diff_col = -1, 0
        if action == 1:
            diff_row, diff_col = 1, 0 
        if action == 2:
            diff_row, diff_col = 0, -1 
        if action == 3:
            diff_row, diff_col = 0, 1 
        # ----------------
        nxtPos    = [0,0]
        nxtPos[0] = state_chess[0] + diff_row   # y方向
        nxtPos[1] = state_chess[1] + diff_col   # x方向

        return nxtPos

# ----------
# Initialize and Generate a maze
size = 27
barriar_rate = 0.1
maze_1 = Maze(size, barriar_rate)
# init_maze : カリキュラム学習用に改造
init_maze, start_point, goal_point = maze_1.read_boardfile()
print("start_point: {}, goal_point: {}".format(start_point, goal_point))
# インスタンスの生成
maze_field = Field(init_maze, start_point, goal_point)
# ボード表示(初期表示)
maze_field.display()
# ----------
# ベストルートを読み込み
mx_route, ref_action = maze_1.read_chessfile()
# ルートを表示する
maze_field.all_display(mx_route)

#=================================================
# Calculation class(1) : Actor-Network
#=================================================
class ActorNetwork(nn.Module):

    def __init__(self,input_size,hidden_size,action_size):
        super(ActorNetwork, self).__init__()
        self.fc1 = nn.Linear(input_size,hidden_size)
        self.fc2 = nn.Linear(hidden_size,hidden_size)
        self.fc3 = nn.Linear(hidden_size,action_size)

    def forward(self,x):
        out = F.relu(self.fc1(x))
        out = F.relu(self.fc2(out))
        out = F.log_softmax(self.fc3(out), dim = 1)
        return out.squeeze(-1)

class ValueNetwork(nn.Module):

    def __init__(self,input_size,hidden_size,output_size):
        super(ValueNetwork, self).__init__()
        self.fc1 = nn.Linear(input_size,hidden_size)
        self.fc2 = nn.Linear(hidden_size,hidden_size)
        self.fc3 = nn.Linear(hidden_size,output_size)

    def forward(self,x):
        out = F.relu(self.fc1(x))
        out = F.relu(self.fc2(out))
        out = self.fc3(out)
        return out.squeeze(-1)

#=================================================
# Calculation class(2) : Agent
#=================================================
class Agent():
    def __init__(self):

        # init value network
        self.value_network = ValueNetwork(input_size = STATE_DIM, hidden_size = 128, output_size = 1)
        self.value_network_optim = torch.optim.Adam(self.value_network.parameters(), lr=0.001)

        # init actor network
        self.actor_network = ActorNetwork(input_size = STATE_DIM, hidden_size = 128, action_size = ACTION_DIM)
        self.actor_network_optim = torch.optim.Adam(self.actor_network.parameters(), lr = 0.001)

        # ---------------------------
        # 盤面データを読む
        self.mx_lm = [[0, 2],[26, 25]]
        self.maze_field = maze_field
        self.start_point = start_point
        self.goal_point = goal_point
        self.stage_info  = "default"
        # ----------------
        # リストの初期化
        self.arr_iplay      = []
        self.arr_maxscore   = []
        self.arr_maxturn    = []
        self.arr_actorloss  = []
        self.arr_criticloss = []
        # 距離の分析用累積リスト
        self.arr_maxTD      = []  # Total距離
        self.arr_sminRTD    = []  # 空間minRT距離
        self.arr_tlminRTD   = []  # タイムラインminRT距離
        # ----------------
        # 学習する
        for iCnt_play in range(EPISODE_STEP):
            maxturn, maxscore, value_loss, actor_loss = self.train(iCnt_play)
            # ----------
            self.arr_iplay.append(iCnt_play)
            self.arr_maxscore.append(maxscore)
            self.arr_maxturn.append(maxturn)
            self.arr_actorloss.append(actor_loss)
            self.arr_criticloss.append(value_loss)
            # 距離の保管(プレイベース)
            self.arr_maxTD.append(np.max(self.arr_Tdist))        # Total距離の最大値
            self.arr_sminRTD.append(np.max(self.arr_sminRT))       # 空間minRT距離の最大値
            self.arr_tlminRTD.append(np.max(self.arr_tlminRT))      # タイムラインminRT距離の最大値

        # ---------------------------
        # 学習履歴の出力
        fig = plt.figure(figsize=(12, 6))
        ax1 = fig.add_subplot(1, 2, 1)
        ax1.plot(self.arr_iplay, self.arr_actorloss)
        ax1.set_xlabel('#episode')
        ax1.set_ylabel('actor_loss')
        # -----
        ax2 = fig.add_subplot(1, 2, 2)
        ax2.plot(self.arr_iplay, self.arr_criticloss)
        ax2.set_xlabel('#episode')
        ax2.set_ylabel('critic_loss')
        # -----
        fig.tight_layout()
        #fig.savefig("./REINFORCE_img.png")
        plt.show()
        
        # ---------------------------
        # 学習結果のグラフ化
        self.show_graph()

    # ----------------
    # 学習結果のグラフ化
    def show_graph(self):

        # ------
        # 学習履歴の出力
        fig2 = plt.figure(figsize=(12, 9))
        ax1 = fig2.add_subplot(2, 2, 1)
        ax1.plot(self.arr_iplay, self.arr_maxscore)
        ax1.set_xlabel('#episode')
        ax1.set_ylabel('score')
        ax1.set_ylim([-10000, 10000])
        # -----
        ax2 = fig2.add_subplot(2, 2, 2)
        ax2.set_title('learning transition : maxTD, sminRTD distance')
        ax2.set_xlabel('#episode')
        ax2.set_ylabel('distance')
        ax2.grid(True)
        ax2.scatter(self.arr_iplay, self.arr_maxTD, label="maxTD", color="blue")
        ax2.scatter(self.arr_iplay, self.arr_sminRTD, label="sminRTD", color="red")
        ax2.legend(loc='best')
        # -----
        ax3 = fig2.add_subplot(2, 2, 3)
        ax3.plot(self.arr_iplay, self.arr_maxturn)
        ax3.set_xlabel('#episode')
        ax3.set_ylabel('turn')
        ax3.set_ylim([0, 1000])
        # -----
        ax4 = fig2.add_subplot(2, 2, 4)
        ax4.set_title('learning transition : tlminRTD distance')
        ax4.set_xlabel('#episode')
        ax4.set_ylabel('distance')
        ax4.grid(True)
        ax4.scatter(self.arr_iplay, self.arr_tlminRTD, label="tlminRTD", color="blue")
        ax4.legend(loc='best')
        # -----
        fig2.tight_layout()
        #fig2.savefig("./REINFORCE_img.png")
        plt.show()

    # ----------------
    # DRL学習する
    def train(self, iCnt_play):

        # ----------------
        # カリキュラム学習[1]
        # ボードを調整する
        # ========================================
        # stage 1
        if iCnt_play == 0:
            # メイズ面を調整する
            self.stage_info  = "stage1(9-9)"
            new_maze = copy.deepcopy(init_maze)
            # -----
            # メトリックスの調整
            self.mx_lm = [[0, 2],[12, 12]]
            # -----
            # 盤面の調整
            epsilon_board = 0.7
            for i in range(1, size-1):
                for j in range(1, size-1):
                    if new_maze[i][j] == '#' and random.random() < epsilon_board:
                        new_maze[i][j] = -30
            # -----
            # 境界の調整
            for i in range(size):
                new_maze[i][12] = '#'
            for j in range(size):
                new_maze[12][j] = '#'
            # インスタンスを生成する
            self.maze_field = Field(new_maze, start_point, goal_point)
            # ボード表示(調整後表示)
            self.maze_field.display()
        # ========================================
        # stage 2
        if iCnt_play == 100:
            # メイズ面を調整する
            self.stage_info  = "stage2(12-12)"
            new_maze = copy.deepcopy(init_maze)
            # -----
            # メトリックスの調整
            self.mx_lm = [[0, 2],[15, 15]]
            # -----
            # 盤面の調整
            epsilon_board = 0.5
            for i in range(1, size-1):
                for j in range(1, size-1):
                    if new_maze[i][j] == '#' and random.random() < epsilon_board:
                        new_maze[i][j] = -30
            # -----
            # 境界の調整
            for i in range(size):
                new_maze[i][15] = '#'
            for j in range(size):
                new_maze[15][j] = '#'
            # インスタンスを生成する
            self.maze_field = Field(new_maze, start_point, goal_point)
            # ボード表示(調整後表示)
            self.maze_field.display()
        # ========================================
        # stage 3
        if iCnt_play == 500:
            # メイズ面を調整する
            self.stage_info  = "stage3(14-14)"
            new_maze = copy.deepcopy(init_maze)
            # -----
            # メトリックスの調整
            self.mx_lm = [[0, 2],[18, 18]]
            # -----
            # 盤面の調整
            epsilon_board = 0.5
            for i in range(1, size-1):
                for j in range(1, size-1):
                    if new_maze[i][j] == '#' and random.random() < epsilon_board:
                        new_maze[i][j] = -30
            # -----
            # 境界の調整
            for i in range(size):
                new_maze[i][18] = '#'
            for j in range(size):
                new_maze[18][j] = '#'
            # インスタンスを生成する
            self.maze_field = Field(new_maze, start_point, goal_point)
            # ボード表示(調整後表示)
            self.maze_field.display()
        # ========================================
        # stage 4
        # if next_state_chess[0] >= 20 and next_state_chess[1] >= 20:
        if iCnt_play == 1000:
            # メイズ面を調整する
            self.stage_info  = "stage4(17-17)"
            new_maze = copy.deepcopy(init_maze)
            # -----
            # メトリックスの調整
            self.mx_lm = [[0, 2],[21, 21]]
            # -----
            # 盤面の調整
            epsilon_board = 0.3
            for i in range(1, size-1):
                for j in range(1, size-1):
                    if new_maze[i][j] == '#' and random.random() < epsilon_board:
                        new_maze[i][j] = -30
            # -----
            # 境界の調整
            for i in range(size):
                new_maze[i][21] = '#'
            for j in range(size):
                new_maze[21][j] = '#'
            # インスタンスを生成する
            self.maze_field = Field(new_maze, start_point, goal_point)
            # ボード表示(調整後表示)
            self.maze_field.display()
        # ========================================
        # カリキュラム学習[2A]
        # 強制的に最適ルートを入力する
        # epsilon for play
        if iCnt_play <= 500:
            self.epsilon_play   = 0.30
        elif iCnt_play > 500 and iCnt_play <= 1000:
            self.epsilon_play   = 0.30
        elif iCnt_play > 1000 and iCnt_play <= 1500:
            self.epsilon_play   = 0.30
        else:
            self.epsilon_play   = 0.30
        # -----
        # ROLL_OUT(ゲームのバッチ実行)
        states, actions, rewards, final_r, maxturn, maxscore, state_chess = self.roll_out(iCnt_play)
        actions_var = Variable(torch.Tensor(actions))
        states_var = Variable(torch.Tensor(states).view(-1,STATE_DIM))

        # train actor network
        self.actor_network_optim.zero_grad()
        log_softmax_actions = self.actor_network(states_var)
        vs = self.value_network(states_var).detach()
        # calculate qs
        qs = Variable(torch.Tensor(self.discount_reward(rewards,0.98,final_r)))

        advantages = qs - vs
        self.actor_network_loss = - torch.mean(torch.sum(log_softmax_actions*actions_var,1)* advantages)
        self.actor_network_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.actor_network.parameters(),0.5)
        self.actor_network_optim.step()

        # train value network
        self.value_network_optim.zero_grad()
        target_values = qs
        values = self.value_network(states_var)
        criterion = nn.MSELoss()
        self.value_network_loss = criterion(values,target_values)
        self.value_network_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.value_network.parameters(),0.5)
        self.value_network_optim.step()
        
        # loss amount
        value_loss = round(self.value_network_loss.item(), 3)
        actor_loss = round(self.actor_network_loss.item(), 3)
        
        # ----------
        # 学習結果の出力(2)
        if iCnt_play%5 ==0:
            print("iCnt_play:{}, stage:{}, state_chess:{}, value_loss:{}, actor_loss:{}, maxturn:{}, maxscore:{}".format(iCnt_play, self.stage_info, state_chess, value_loss, actor_loss, maxturn, maxscore))       

        return maxturn, maxscore, value_loss, actor_loss
    
        
    # ゲームを実行する
    def roll_out(self, iCnt_play):

        states = []
        actions = []
        rewards = []
        # ----------
        # リストの初期化
        self.arr_Tdist     = []    # Total距離
        self.arr_sminRT    = []    # 空間minRT距離
        self.arr_tlminRT   = []    # タイムラインminRT距離
        # ----------
        # ゲームの初期化
        state_chess = self.start_point
        num_bump    = 0
        state = calc_address(state_chess[0], state_chess[1], num_bump, self.mx_lm)
        # 移動稼働なオプション
        movables = self.maze_field.get_actions(state_chess)
        # ----------
        sum_bump = 0
        score = 0
        is_done = False
        final_r = 0
        for iCnt_turn in range(SAMPLE_NUMS):
            states.append(state)
            log_softmax_action = self.actor_network(Variable(torch.Tensor([state])))
            softmax_action = torch.exp(log_softmax_action)
            action = np.random.choice(ACTION_DIM,p=softmax_action.cpu().data.numpy()[0])
            # ----------------
            # カリキュラム学習[2B]
            # 強制的に最適ルートを入力する
            # stage 1
            if iCnt_play == 0 or iCnt_play == 2 or iCnt_play == 5 or iCnt_play == 8 or random.random() < self.epsilon_play:
                if iCnt_turn <= len(mx_route) - 1:
                    action = ref_action[iCnt_turn]
            # stage 2
            if iCnt_play == 100 or iCnt_play == 102 or iCnt_play == 105 or iCnt_play == 108  or iCnt_play == 112 or random.random() < self.epsilon_play:
                if iCnt_turn <= len(mx_route) - 1:
                    action = ref_action[iCnt_turn]  
            # stage 3
            if iCnt_play == 500 or iCnt_play == 502 or iCnt_play == 505 or iCnt_play == 508 or iCnt_play == 512  or random.random() < self.epsilon_play:
                if iCnt_turn <= len(mx_route) - 1:
                    action = ref_action[iCnt_turn]  
            # stage 4
            if iCnt_play == 1000 or iCnt_play == 1002 or iCnt_play == 1005 or iCnt_play == 1008 or iCnt_play == 1012 or random.random() < self.epsilon_play:
                if iCnt_turn <= len(mx_route) - 1:
                    action = ref_action[iCnt_turn]  
            # ----------
            # ONTHOT化
            one_hot_action = [int(k == action) for k in range(ACTION_DIM)]
            action_chess = self.maze_field.nxtPosition(state_chess, action)
            # 次の報酬を得る（#に当たったら、STUCKPOINTを生成する）
            next_state_chess, reward, done, num_bump = self.maze_field.get_val(action_chess, state_chess, movables)
            # 状態の表記を変更(ONEHOTプラス距離)
            next_state = calc_address(next_state_chess[0], next_state_chess[1], num_bump, self.mx_lm)
            # ----------------
            # カリキュラム学習[3]
            # stage 1
            if iCnt_play < 100:
                if next_state_chess[0] >= 9 and next_state_chess[1] >= 9:
                    done = True
                    reward = 5000 - 5*iCnt_turn - 100*sum_bump
            # stage 2
            if iCnt_play >= 100 and iCnt_play < 500:
                if next_state_chess[0] >= 12 and next_state_chess[1] >= 12:
                    done = True
                    reward = 5000 - 5*iCnt_turn - 100*sum_bump
            # stage 3 
            if iCnt_play >= 500 and iCnt_play < 1000:
                if next_state_chess[0] >= 14 and next_state_chess[1] >= 14:
                    done = True
                    reward = 5000 - 5*iCnt_turn - 100*sum_bump
            # stage 4
            if iCnt_play >= 1000:
                if next_state_chess[0] >= 17 and next_state_chess[1] >= 17:
                # if next_state_chess[0] == self.goal_point[0] and next_state_chess[1] == self.goal_point[1]:
                    done = True
                    reward = 5000 - 5*iCnt_turn - 100*sum_bump
            # ----------
            # 移動稼働なオプション
            next_movables = self.maze_field.get_actions(next_state_chess)
            # ----------
            # 距離の計算
            T_Dist  = state[3] + state[4]
            dist_sminRT  = state[5]
            dist_tlminRT = timeline_minRT(state_chess[0], state_chess[1], iCnt_turn)
            # ----------
            self.arr_Tdist.append(T_Dist-8.0)       # Total距離
            self.arr_sminRT.append(dist_sminRT)    # 空間minRT距離
            self.arr_tlminRT.append(dist_tlminRT)    # タイムラインminRT距離
            # ----------
            # [重要]フィードバック制御あり
            reward = reward - 30 * dist_sminRT
            # ----------
            # リストへの追加
            actions.append(one_hot_action)
            rewards.append(reward)
            final_state = next_state
            state_chess = next_state_chess
            state = next_state
            movables = next_movables
            score += reward
            sum_bump += num_bump
            # ----------
            # 学習結果の出力(1)
            #print("iCnt_play:{}, iCnt_turn:{}, state_chess:{}, action, reward:{}, done:{}".format(iCnt_play, iCnt_turn, state_chess, action, reward, done))       
            if done == True:
                is_done = True
                break
        # ----------
        if is_done == False:
            final_r = self.value_network(Variable(torch.Tensor([final_state]))).data.numpy()
        # ----------
        # 学習統計
        maxturn  = len(rewards)
        maxscore = np.sum(rewards)

        return states, actions, rewards, final_r, maxturn, maxscore, state_chess

    def discount_reward(self, r, gamma, final_r):
        discounted_r = np.zeros_like(r)
        running_add = final_r
        for t in reversed(range(0, len(r))):
            running_add = running_add * gamma + r[t]
            discounted_r[t] = running_add
        return discounted_r

if __name__ == '__main__':

    # ----------
    # Hyper Parameters
    STATE_DIM       = 6
    ACTION_DIM      = 4
    EPISODE_STEP    = 1500
    SAMPLE_NUMS     = 5000

    # ----------
    # 強化学習を実行する
    Agent()

```

D先生 : “ありゃー、このプログラムでは**強制学習を頻繁に使っています**。ちょっと無理がありますねえ・・・。”

![imageRL5-2-9](https://reinforce.github.io/images/imageRL5-2-9.jpg) 

QEU:FOUNDER ; “そのココロは、まずはともあれプログラムを動かそうと・・・。そして結果をドン！”

**（学習曲線）**

![imageRL5-2-10](https://reinforce.github.io/images/imageRL5-2-10.jpg) 

**（パフォーマンス推移）**

![imageRL5-2-11](https://reinforce.github.io/images/imageRL5-2-11.jpg) 

D先生 : “ははは、だめだこりゃ・・・。まともだったのは300ゲーム（プレイ）までだったですね・・・”

QEU:FOUNDER ; “とりあえず、1500ゲームを終わらせたい一心で・・・（笑）。”

D先生 : “使っているのPCが貧弱で、スキームがA3Cだとすると、しようがないですね。ε-Greedy（DQN with Experience Replay）を使わないんですか？”

QEU:FOUNDER ; “A3Cのトライアルを一通り終わらせてからにしましょう。”

D先生 : “そもそも、AGVで3次元強化学習を使う必要があるのかがわからないんですよねェ・・・。”


## ～　まとめ　～

### ・・・　つづきです　・・・

QEU:FOUNDER ： “我々の結論です。まず議論すべきは、生産力です。お金のはなしは、「お金を刷っても破綻しない」まででよろしい。”

<iframe width="560" height="315" src="https://www.youtube.com/embed/PztqSYO6pV8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

D部長: “**生産力ってわかりにくい。**”

![imageRL5-2-12](https://reinforce.github.io/images/imageRL5-2-12.jpg) 

QEU:FOUNDER ： “じゃあ、コレを見てくれ・・・。”

<iframe width="560" height="315" src="https://www.youtube.com/embed/axg2JnVSQLY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

QEU:FOUNDER ; “わかった？”

![imageRL5-2-13](https://reinforce.github.io/images/imageRL5-2-13.jpg) 

D先生: “**本質は創造力ですね。感動できるものであれば、たとえ小さなものでも、なんでも・・・。**”

QEU:FOUNDER ; “「MMTワァワァ」って、公共のSNSを使ってお金の話ばかりしてみっともない。それではなにも解決しないよ・・・。”

![imageRL5-2-14](https://reinforce.github.io/images/imageRL5-2-14.jpg) 

D先生: “人を洗脳して「坪を売っている」みたいに、商売としては楽だいうメリットはありますね。”


