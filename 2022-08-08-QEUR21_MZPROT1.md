## QEUR21_MZPROT1: 「カメラ目線」は大切！

## ～　2次元メイズの知見が生きてきます　～

### ・・・　前回のつづきです　・・・


D先生 : “う～ん・・・。なんというかなァ・・・。このプログラムをベースとして強化学習をするんですか？これでは3D画面を出力する意味がないんじゃない？あまりにもフラフラ画面が動くのが不快ですね・・・。”

![imageRL6-2-1](https://reinforce.github.io/images/imageRL6-2-1.jpg) 

QEU:FOUNDER ： “コマに付属するカメラの3D画面を如何に制御するのか？このポリシーが明確でなかったのが、以前に三次元メイズで失敗した主要な原因の一つだよね。今回は、もうちょっと賢くやりましょう。次の作業（タスク）は、「画面をどのように制御するのか・・・」についてです。”

D先生 : “どのようにカメラ（3D画面）を制御すればいいんでしょうかね？”

![imageRL6-2-2](https://reinforce.github.io/images/imageRL6-2-2.jpg) 

QEU:FOUNDER ; “**すでに計算した２次元メイズで得た情報を使えばいい**んじゃないですか？このときにはACTION変数って、とても使えるよね・・・。”

```python
            # ----------------
            # 次の行動(ACTION-８方向)に進む
            # action number / row / col / 方向/ ベクトル
            # 0 / -1 / 0  / 上上(up-up) / [-1, 0]
            # 1 / -1 / -1  / 上左(up-left) / [-1, -1]
            # 2 / 0 / -1  / 左左(left-left) / [0, -1]
            # 3 / +1 / -1  / 左下(down-left) / [+1, -1]
            # 4 / +1 / 0  / 下下(down-down) / [+1, 0]
            # 5 / +1 / +1  / 下右(down-right) / [+1, +1]
            # 6 / 0  / +1 / 右右(right-right) / [0, +1]
            # 7 / -1  / +1 / 上右(up-right) / [-1, +1]

```

D先生 ; “行動（ACTION）は進行方向を示すので、カメラの方向を設定するのに適切です。・・・つまり、今回の強化学習ではコマの移動とカメラの方向を分離するということですか？すなわち、カメラの方向はあえて学習しないと・・・。”

QEU:FOUNDER ; “そういうこと・・・。さて、前回の2次元メイズのゲームの学習を完了させておきましょう。このデータを使って、カメラの移動をどのように設定するとよいのかを見てみましょう。”

**（学習曲線）**

![imageRL6-2-3](https://reinforce.github.io/images/imageRL6-2-3.jpg) 

**（パフォーマンス推移）**

![imageRL6-2-4](https://reinforce.github.io/images/imageRL6-2-4.jpg) 

D先生 ; “あとは、プログラムを動かして考えましょう。プログラムをドン！！”

```python
# ----------------
# メイズ(MAZE)ゲームの強化学習システム
# step2 : 3Dメイズの準備段階です(DQNに方針変更、8direction)
# step2 : Drawing_3DMazeProto_pyTorch(DQN_size27).py
# step2 : 学習済のデータを読み込み、最適ルートを評価する
# ---------------- 
# 数値計算のﾗｲﾌﾞﾗﾘ読み込み
import numpy as np
import pandas as pd
import math, copy, random, time
from collections import deque, Counter
from scipy.special import softmax
from IPython.display import clear_output
# ----------------
import torch
import torch.nn.functional
import torch.utils.data
# ---------------- 
import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = (16, 10)

# =================================================
# Game board difinition 
# =================================================
foldername  = "./MAZE_test/"

# =================================================
# difinition of STATE CALCULATION function
# =================================================
# タイムライン用minRTメトリックスを計算する
def timeline_minRT(a_row, a_col, iCnt_turn):
    len_route = len(mx_route)
    if iCnt_turn <= len_route - 1:
        pos_rt  = mx_route[iCnt_turn]
    else:
        pos_rt  = mx_route[len_route - 1]
    # -----
    tlEuc   = (a_row - pos_rt[0])**2 + (a_col - pos_rt[1])**2
    dist_tlminRT = np.sqrt(tlEuc)
        
    return round(dist_tlminRT,5)

# -------
# 空間用minRTメトリックスを計算する
def space_minRT(pos_route, a_row, a_col):
    len_route = len(pos_route)
    arr_sminRT = []
    for i in range(len_route):
        pos_rt  = pos_route[i]
        sqEuc   = (a_row - pos_rt[0])**2 + (a_col - pos_rt[1])**2
        distEuc = np.sqrt(sqEuc)
        arr_sminRT.append(round(distEuc,5))   
    dist_sminRT = np.min(arr_sminRT)
        
    return dist_sminRT

# -------
# 状態の表記を変更(XY座標へ)、空間用minRTメトリックスつき
def calc_address(a_row, a_col, mx_lm, iCnt_turn):

    add_state = np.zeros(2)
    for i in range(2):
        pos_lm  = mx_lm[i]
        sqEuc   = (a_row - pos_lm[0])**2 + (a_col - pos_lm[1])**2
        distEuc = np.sqrt(sqEuc)
        add_state[i] = round(distEuc,5)
             
    # 空間用minRTメトリックスを計算する
    # Aルート(標準)
    dist_sminRTA = space_minRT(mx_route, a_row, a_col)
    dist_sminRTA = round(dist_sminRTA,5)
    # Bルート(標準との差異分)
    dist_sminRTB = space_minRT(mx_routeB, a_row, a_col)
    dist_sminRTB = round(dist_sminRTB,5) - dist_sminRTA
    # Cルート(標準との差異分)
    dist_sminRTC = space_minRT(mx_routeC, a_row, a_col)
    dist_sminRTC = round(dist_sminRTC,5) - dist_sminRTA
    # タイムライン用minRTメトリックスを計算する
    dist_tlminRT = timeline_minRT(a_row, a_col, iCnt_turn)

    # --------------------------------------------------
    # 距離とりまとめリストを生成する
    arr_RTDists = np.zeros(5)
    arr_RTDists[0] = add_state[0] + add_state[1]     # Total距離
    arr_RTDists[1] = dist_sminRTA    # 空間minRT距離
    arr_RTDists[2] = dist_tlminRT    # タイムラインminRT距離
    arr_RTDists[3] = dist_sminRTB    # 空間用minRT距離(B)
    arr_RTDists[4] = dist_sminRTC    # 空間用minRT距離(C)

    # STATEを生成する
    state = np.hstack([[a_row, a_col], add_state, [dist_sminRTA, dist_sminRTB, dist_sminRTC]])
    
    return state, arr_RTDists

# ----------
# Maze Class
class Maze(object):
    def __init__(self, size=27, blocks_rate=0.1):
        self.size = size if size > 3 else 27
        self.blocks = int((size ** 2) * blocks_rate) 
        self.s_list = []
        self.maze_list = []
        self.e_list = []

    def create_mid_lines(self, k):
        if k == 0: self.maze_list.append(self.s_list)
        elif k == self.size - 1: self.maze_list.append(self.e_list)
        else:
            tmp_list = []
            for l in range(0,self.size):
                if l == 0: tmp_list.extend("#")
                elif l == self.size-1: tmp_list.extend("#")
                else:
                    a = random.randint(-1, 0)
                    tmp_list.extend([a])
            self.maze_list.append(tmp_list)

    def insert_blocks(self, k, s_r, e_r):
        b_y = random.randint(1, self.size-2)
        b_x = random.randint(1, self.size-2)
        if [b_y, b_x] == [1, s_r] or [b_y, b_x] == [self.size - 2, e_r]: k = k-1
        else: self.maze_list[b_y][b_x] = "#"
            
    def generate_maze(self): 
        s_r = random.randint(1, int((self.size / 2)) - 1)
        for i in range(0, self.size):
            if i == s_r: self.s_list.extend("S")
            else: self.s_list.extend("#")
        start_point = [0, s_r]

        e_r = random.randint(int((self.size / 2)) + 1, self.size - 2)
        for j in range(0, self.size):
            if j == e_r: self.e_list.extend([50])
            else: self.e_list.extend("#")
        goal_point = [self.size - 1, e_r]

        for k in range(0, self.size):
            self.create_mid_lines(k)
        
        for k in range(self.blocks):
            self.insert_blocks(k, s_r, e_r)

        return self.maze_list, start_point, goal_point

    # --------------------------------------------------
    # 盤面のCSVファイルを読み込み表示する
    def read_boardfile(self):
        # CSVファイルの読み込み
        code_csvout = "maze_board27.csv"       # file name
        file_readcsv = foldername + code_csvout   # standard(training) file name  
        df = pd.read_csv(file_readcsv)
        #max_play = len(df)
        # print("データ量 max_play",max_play)
        # print(df)
        # --------------------------------------------------
        # 選択項目の読み込み
        maze = df.loc[:, "0":"26"].values.tolist()
        start_point = [0,0]
        goal_point  = [0,0]
        for i in range(size):
            for j in range(size):
                if maze[i][j] == 'S':
                    start_point = [i, j]
                if maze[i][j] == '5000':
                    goal_point  = [i, j]

        self.maze_list   = maze
        self.start_point = start_point
        self.goal_point  = goal_point
        print("goal_point", goal_point)
        print("----- mx_maze(read_boardfile) -----")
        print(maze)

        return self.maze_list, start_point, goal_point

    # --------------------------------------------------
    # コマの最短ルートのCSVファイルを読み込み表示する（標準Aルート）
    def read_chessfile(self):
        # CSVファイルの読み込み
        code_csvout = "chess_route27.csv"       # file name
        file_readcsv = foldername + code_csvout   # standard(training) file name  
        df = pd.read_csv(file_readcsv)
        #max_play = len(df)
        # print("データ量 max_play",max_play)
        # print(df)
        # --------------------------------------------------
        # 選択項目の読み込み
        mx_route    = df.loc[:, "row":"col"].values.tolist()
        ref_action  = df.loc[:, "action"].values.tolist()
        print("ref_action: ", ref_action)
        #print("----- route -----")
        #print(route)

        return mx_route, ref_action

    # --------------------------------------------------
    # コマの最短ルートのCSVファイルを読み込み表示する（限界BCルート）
    def read_chessfileBC(self, code_csvout):

        # CSVファイルの読み込み
        file_readcsv = foldername + code_csvout   # standard(training) file name  
        df = pd.read_csv(file_readcsv)
        #max_play = len(df)
        # print("データ量 max_play",max_play)
        # print(df)
        # --------------------------------------------------
        # 選択項目の読み込み
        mx_route    = df.loc[:, "row":"col"].values.tolist()
        #print("----- route -----")
        #print(route)

        return mx_route

# ----------
# Maze functions
class Field(object):
    def __init__(self, maze, start_point, goal_point):
        # ----------------
        # 次の行動(ACTION-８方向)に進む
        # action number / row / col / 方向/ ベクトル(変則ONEHOT)
        # 0 / -1 / 0  / 上上(up-up) / [-1, 0]
        # 1 / -1 / -1  / 上左(up-left) / [-1, -1]
        # 2 / 0 / -1  / 左左(left-left) / [0, -1]
        # 3 / +1 / -1  / 左下(down-left) / [+1, -1]
        # 4 / +1 / 0  / 下下(down-down) / [+1, 0]
        # 5 / +1 / +1  / 下右(down-right) / [+1, +1]
        # 6 / 0  / +1 / 右右(right-right) / [0, +1]
        # 7 / -1  / +1 / 上右(up-right) / [-1, +1]
        # ----------------
        self.maze = maze
        self.start_point = start_point
        self.goal_point  = goal_point
        self.movable_vec = [[-1, 0],[-1, -1],[0, -1],[1, -1],[1, 0],[1, 1],[0, 1],[-1, 1]]

    def display(self, point=None):
        field_data = copy.deepcopy(self.maze)
        if not point is None:
                y, x = point
                field_data[y][x] = "@@"
        else:
                point = ""
        print("================")
        for line in field_data:
                print ("\t" + "%3s " * len(line) % tuple(line))
        print("================")

    def all_display(self, mx_route):
        field_data = copy.deepcopy(self.maze)
        for i in range(len(mx_route)):
            point = mx_route[i]
            y, x = point[0], point[1]
            field_data[y][x] = "@@"
        print("================")
        for line in field_data:
                print ("\t" + "%3s " * len(line) % tuple(line))
        print("================")

    def get_actions(self, state):
        movables = []
        if state == self.start_point:
            y = state[0] + 1
            x = state[1]
            a = [[y, x]]
            return a
        else:
            for v in self.movable_vec:
                y = state[0] + v[0]
                x = state[1] + v[1]
                if not(0 < x < len(self.maze) and
                       0 <= y <= len(self.maze) - 1 and
                       self.maze[y][x] != "#" and
                       self.maze[y][x] != "S"):
                    continue
                movables.append([y,x])
            if len(movables) != 0:
                return movables
            else:
                return None

    # --------------------------------------------------
    # 次のコマの「決定」位置と報酬、ゲームフラグを決める
    # 新概念：マスク「#」位置に着くと、スタート地点に戻る
    def get_val(self, action_chess, state_chess, movables):
        yPos = action_chess[0]
        xPos = action_chess[1]
        # ----------------
        if action_chess == self.start_point:
            nxt_state = action_chess
            return nxt_state, 0, False, 0
        elif self.maze[yPos][xPos] == "#":
            #print("movables: ", movables)
            if len(movables) > 0:
                nxt_state = random.choice(movables)
            else:
                nxt_state = self.start_point
            v = -0.1        # -10
            return nxt_state, v, False, 1
        else:
            nxt_state = action_chess
            v = float(self.maze[yPos][xPos])
            if action_chess == self.goal_point: 
                return nxt_state, v, True, 0
            else: 
                return nxt_state, v, False, 0

# ----------
# Initialize and Generate a maze
size = 27
barriar_rate = 0.1
maze_1 = Maze(size, barriar_rate)
# init_maze : カリキュラム学習用に改造
init_maze, start_point, goal_point = maze_1.read_boardfile()
print("start_point: {}, goal_point: {}".format(start_point, goal_point))
# インスタンスの生成
maze_field = Field(init_maze, start_point, goal_point)
# ボード表示(初期表示)
maze_field.display()
# ----------
# 最短ルートを読み込み(Aルート)
mx_route, ref_action = maze_1.read_chessfile()
# ルートを表示する
maze_field.all_display(mx_route)
# ----------
# コマの最短ルートのCSVファイルを読み込み表示する（限界BCルート）
# Bルート
mx_routeB = maze_1.read_chessfileBC("chess_route27B.csv")
# Cルート
mx_routeC = maze_1.read_chessfileBC("chess_route27C.csv")
#print("--- mx_routeC ---")
#print(mx_routeC)


# ----------
# 入力用:Pytorchモデルのファイル名
comment_input_model = "initial"
code_input_model = "model_2DMaze_DQNER_{0}.pt".format(comment_input_model)  # モデルの指定
file_input_model = foldername + code_input_model  # ファイルパス名の生成


#=================================================
# Deep Learning Model class            
#=================================================
# PyTorchのDLの定義
class Net(torch.nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = torch.nn.Linear(dim_input, 128)
        self.fc1.weight.data.normal_(0, 0.1)
        self.fc2 = torch.nn.Linear(128, 128)
        self.fc2.weight.data.normal_(0, 0.1)
        self.fc3 = torch.nn.Linear(128, 128)
        self.fc3.weight.data.normal_(0, 0.1)
        self.fc4 = torch.nn.Linear(128, dim_output)
        self.fc4.weight.data.normal_(0, 0.1)
        
    def forward(self, x):
        x = torch.nn.functional.relu(self.fc1(x))
        x = torch.nn.functional.relu(self.fc2(x))
        x = torch.nn.functional.relu(self.fc3(x))
        x = self.fc4(x)
        return x

# =======================================
# =======================================
# Solving the maze in Deep Q-learning
class DQN_Solver:
    def __init__(self, state_size, action_size):
        self.state_size = state_size # list size of state
        self.action_size = action_size # list size of action
        #self.memory = deque(maxlen=5000) # memory space
        #self.gamma  = 0.96 # discount rate
        #self.epsilon = 0.99 # randomness of choosing random action or the best one
        #self.e_decay = 0.9992 # epsilon decay rate
        #self.e_min  = 0.01 # minimum rate of epsilon
        self.learning_rate = 0.002 # learning rate of neural network
        # --------------------------------------------------
        # crate instance for input
        self.eval_net, self.target_net = Net(), Net()                           # Netを利用して２つのニューラルネットをつくる
        #self.learn_step_counter = 0
        # --------------------------------------------------
        # Save and load the model via state_dict
        self.eval_net.load_state_dict(torch.load(file_input_model))
        # --------------------------------------------------
        # set validaiton mode
        self.eval_net.eval()
        # --------------------------------------------------
        # set training parameters
        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=self.learning_rate)
        self.criterion = torch.nn.MSELoss()


    # choosing action depending on epsilon
    def choose_action(self, state, movables):

        # ----------------
        # 次の行動(ACTION-８方向)に進む
        # action number / row / col / 方向/ ベクトル
        # 0 / -1 / 0  / 上上(up-up) / [-1, 0]
        # 1 / -1 / -1  / 上左(up-left) / [-1, -1]
        # 2 / 0 / -1  / 左左(left-left) / [0, -1]
        # 3 / +1 / -1  / 左下(down-left) / [+1, -1]
        # 4 / +1 / 0  / 下下(down-down) / [+1, 0]
        # 5 / +1 / +1  / 下右(down-right) / [+1, +1]
        # 6 / 0  / +1 / 右右(right-right) / [0, +1]
        # 7 / -1  / +1 / 上右(up-right) / [-1, +1]
        # ----------------
        action_chess = [0,0]
        acType, Qvalue, iCnt = "machine", -0.001, 0
        for a_order in movables:

            # 変則ONE-HOT化
            diff_row = a_order[0] - state[0]
            diff_col = a_order[1] - state[1]
            # ----------------
            # 状態(STATE)行動(ACTION)ベクトルを生成する
            np_action = np.hstack([state, [diff_row, diff_col]])
            #print("np_action: ",np_action)
            if iCnt == 0:
                mx_input   = [np_action]  # 状態(S)行動(A)マトリックス
            else:
                mx_input   = np.concatenate([mx_input, [np_action]], axis=0)     # 状態(S)行動(A)マトリックス
            iCnt = iCnt + 1
        # print("----- mx_input -----")
        # print(mx_input)
        # --------------------------------------------------
        # generate new 'x'
        x_input_tensor = torch.tensor(mx_input).float()
        # predict 'y'
        with torch.no_grad():
            y_pred_tensor = self.eval_net(x_input_tensor)
        # convert tensor to numpy
        y_pred       = y_pred_tensor.data.numpy().flatten()
        Qvalue       = np.max(y_pred)
        temp_order   = np.argmax(y_pred)
        action_chess = movables[temp_order]
        #print("state:{0}, y_pred:{1}, temp_order:{2}, action_chess:{3}".format(state, y_pred, temp_order, action_chess))

        return acType, action_chess, Qvalue


#=================================================
# Calculation class(4) : Agent
#=================================================
class Agent():
    # -----
    def __init__(self):

        # ---------------------------
        # 盤面データを読む
        self.mx_lm         = [[0, 2],[26, 25]]
        self.maze_field    = maze_field
        self.start_point   = start_point
        self.goal_point    = goal_point

        # ========================================
        # エピソードを実行する
        maxturn, maxscore, flag_goal = self.get_episode()
        print("maxturn: {}, maxscore: {}, flag_goal: {}".format(maxturn, maxscore, flag_goal))
        # ---------------------------
        # ベストの解決ルート
        print("mx_BRoute: ", self.mx_BRoute)
        # ルートを表示する
        maze_field.all_display(self.mx_BRoute)
        # ---------------------------
        # グラフを表示する
        self.show_graph()

    # --------------------------------------------------
    # エピソードを運用する
    def get_episode(self):

        # 記録用パラメタ類(ターンベース)
        self.arr_iturn     = []    # ターン・カウンタリスト
        self.arr_orders    = []    # 指示リスト(アドレス)
        self.arr_acType    = []    # 指示のタイプ
        self.arr_scores    = []    # ゲームスコアリスト
        self.arr_dones     = []    # ゲームオーバーフラグ
        self.arr_predQV    = []    # Q値のリスト
        # ---------------------------
        # 記録用パラメタ類(ターンベース)
        self.arr_Tdist     = []    # Total距離
        self.arr_sminRT    = []    # 空間minRT距離
        self.arr_tlminRT   = []    # タイムラインminRT距離
        self.arr_sminRTB   = []    # 空間用minRT距離(B)
        self.arr_sminRTC   = []    # 空間用minRT距離(C)
        # ---------------------------
        # ベストの解決ルート
        self.mx_BRoute    = []    # ベストのルート
        # ---------------------------
        # ゲームのリセット
        state_chess = start_point
        score, iCnt_turn, action, done = 0, 0, 0, False
        # 状態(STATE)を生成する
        state, arr_RTDists = calc_address(state_chess[0], state_chess[1], self.mx_lm, iCnt_turn) 
        movables = maze_field.get_actions(state_chess)
        # ---------------------------
        for iCnt_turn in range(NUMS_TURNS - 1):

            # 次の行動(ACTION)を発行する
            acType, action_chess, Qvalue = dql_solver.choose_action(state, movables)
            temp_action_chess, reward, done, num_bump = maze_field.get_val(action_chess, state_chess, movables)
            next_state_chess  = action_chess
            next_movables     = maze_field.get_actions(next_state_chess)
            # ----------------
            # 次の行動(ACTION-８方向)に進む
            # action number / row / col / 方向/ ベクトル
            # 0 / -1 / 0  / 上上(up-up) / [-1, 0]
            # 1 / -1 / -1  / 上左(up-left) / [-1, -1]
            # 2 / 0 / -1  / 左左(left-left) / [0, -1]
            # 3 / +1 / -1  / 左下(down-left) / [+1, -1]
            # 4 / +1 / 0  / 下下(down-down) / [+1, 0]
            # 5 / +1 / +1  / 下右(down-right) / [+1, +1]
            # 6 / 0  / +1 / 右右(right-right) / [0, +1]
            # 7 / -1  / +1 / 上右(up-right) / [-1, +1]
            # 変則ONE-HOT化
            diff_row = action_chess[0] - state_chess[0]
            diff_col = action_chess[1] - state_chess[1]
            # 0 / -1 / 0  / 上上(up-up) / [-1, 0]
            if diff_row < 0 and int(diff_col) == 0:
                action = 0
            # 1 / -1 / -1  / 上左(up-left) / [-1, -1]
            elif diff_row < 0 and diff_col < 0:
                action = 1
            # 2 / 0 / -1  / 左左(left-left) / [0, -1]
            elif int(diff_row) == 0 and diff_col < 0:
                action = 2
            # 3 / +1 / -1  / 左下(down-left) / [+1, -1]
            elif diff_row > 0 and diff_col < 0:
                action = 3
            # 4 / +1 / 0  / 下下(down-down) / [+1, 0]
            elif diff_row > 0 and int(diff_col) == 0:
                action = 4
            # 5 / +1 / +1  / 下右(down-right) / [+1, +1]
            elif diff_row > 0 and diff_col > 0:
                action = 5
            # 6 / 0  / +1 / 右右(right-right) / [0, +1]
            elif int(diff_row) == 0 and diff_col > 0:
                action = 6
            # 7 / -1  / +1 / 上右(up-right) / [-1, +1]
            elif diff_row < 0 and diff_col > 0:
                action = 7
            print("iCnt_turn:{}, action:{}, state_chess:{}, movables:{}".format(iCnt_turn, action, state_chess, movables))
            # ----------------
            T_Dist        = arr_RTDists[0]    # Total距離
            dist_sminRT   = arr_RTDists[1]    # 空間minRT距離
            dist_tlminRT  = arr_RTDists[2]    # タイムラインminRT距離
            dist_sminRTB  = arr_RTDists[3]    # 空間用minRT距離(B)
            dist_sminRTC  = arr_RTDists[4]    # 空間用minRT距離(C)
            # ----------------
            if next_state_chess[0] >= 25 and next_state_chess[1] >= 25:
                done = True
                reward = 10000
            # ----------------
            # 状態(STATE)を生成する
            next_state, next_arr_RTDists = calc_address(next_state_chess[0], next_state_chess[1], self.mx_lm, iCnt_turn + 1)  
            # ---------------------------
            # 記録用リストを追記する
            self.arr_iturn.append(iCnt_turn)    # ターン・カウンタリスト
            self.arr_orders.append(action_chess)      # 指示リスト(アドレス)
            self.arr_acType.append(acType)      # 指示のタイプ
            self.arr_scores.append(reward)      # ゲームスコアリスト   
            self.arr_dones.append(done)         # ゲームオーバーフラグ
            self.arr_predQV.append(Qvalue)      # Q値のリスト
            # ---------------------------
            # 記録用パラメタ類(ターンベース)
            self.arr_Tdist.append(T_Dist-15.0)       # Total距離
            self.arr_sminRT.append(dist_sminRT)     # 空間minRT距離
            self.arr_tlminRT.append(dist_tlminRT)    # タイムラインminRT距離
            self.arr_sminRTB.append(dist_sminRTB)    # 空間用minRT距離(B)
            self.arr_sminRTC.append(dist_sminRTC)    # 空間用minRT距離(C)
            # ---------------------------
            # ベストの解決ルート
            self.mx_BRoute.append(action_chess)    # ベストのルート
            # ---------------------------
            state       = next_state
            state_chess = next_state_chess
            arr_RTDists = next_arr_RTDists
            movables    = next_movables
            score       = score + reward
            if done:
                break
        # ---------------------------
        # エピソードの実行結果を出力する
        maxturn   = iCnt_turn
        maxscore  = int(score)
        flag_goal = done

        return maxturn, maxscore, flag_goal

    # ----------------
    # 学習結果のグラフ化
    def show_graph(self):

        fig2 = plt.figure(figsize=(14, 12))
        # -----
        ax1 = fig2.add_subplot(2, 2, 1)
        ax1.set_title('DQN parameter transition: QValue')
        ax1.set_xlabel('#episode')
        ax1.set_ylabel('QValue')
        ax1.plot(self.arr_iturn, self.arr_predQV, label="QValue", color="blue")
        ax1.grid(True)
        ax1.legend(loc='best')
        # -----
        ax2 = fig2.add_subplot(2, 2, 2)
        ax2.set_title('learning transition : distance')
        ax2.set_xlabel('#episode')
        ax2.set_ylabel('Distance')
        ax2.grid(True)
        ax2.scatter(self.arr_iturn, self.arr_Tdist, label="maxTD", color="blue")
        ax2.scatter(self.arr_iturn, self.arr_sminRT, label="sminRTD", color="red")
        ax2.scatter(self.arr_iturn, self.arr_tlminRT, label="tlminRTD", color="green")
        ax2.legend(loc='best')
        # -----
        ax3 = fig2.add_subplot(2, 2, 3)
        ax3.set_title('learning transition : SminRTD distance[limit-B]')
        ax3.set_xlabel('#episode')
        ax3.set_ylabel('distance')
        ax3.grid(True)
        ax3.plot(self.arr_iturn, self.arr_sminRTB, label="Dist_B", color="blue")
        ax3.legend(loc='best')
        # -----
        ax4 = fig2.add_subplot(2, 2, 4)
        ax4.set_title('learning transition : SminRTD distance[limit-C]')
        ax4.set_xlabel('#episode')
        ax4.set_ylabel('distance')
        ax4.grid(True)
        ax4.plot(self.arr_iturn, self.arr_sminRTC, label="Dist_C", color="blue")
        ax4.legend(loc='best')
        # -----
        fig2.tight_layout()
        #fig.savefig("./AAC_img.png")
        plt.show()

if __name__ == '__main__':

    # ----------
    # Hyper Parameters
    state_size  = 7
    action_size = 2
    dim_input   = state_size + action_size
    dim_output  = 1
    dql_solver  = DQN_Solver(state_size, action_size)

    # ----------
    # Game Parameters
    NUMS_TURNS  = 1000

    # ----------
    # 強化学習を実行する
    Agent()

```

QEU:FOUNDER ; “それでは、**ベストの経路**を見てみましょう。”

![imageRL6-2-5](https://reinforce.github.io/images/imageRL6-2-5.jpg) 

D先生 ; “昔に設定したベストの経路とは違いましたね・・・。”

QEU:FOUNDER ; “そりゃそうでしょ・・・。むかしは命令は4方向とし、現在は8方向に設定しているんですから・・・。学習結果を経路ではなく、命令(ACTION)群でみると以下のようになります。”

![imageRL6-2-6](https://reinforce.github.io/images/imageRL6-2-6.jpg) 

D先生 ; “つまり、**カメラを向ける方向を「2次元メイズの強化学習で得られた最適行動(ACTION)の向きに対応させる」**わけですね・・・。”

QEU:FOUNDER ; “そうそう・・・。それが一番合理的だし、あとの強化学習が速くなりますよ。あと、これはオマケだが、Q値や各種距離がゲーム内でどのように変化するのかをプロットしてみました。”

![imageRL6-2-7](https://reinforce.github.io/images/imageRL6-2-7.jpg) 

D先生 : “今回の目的には直接関係ないですが・・・。なるほど・・・、おもしろい結果です。”


## ～　まとめ　～

C部長 ： “それではイケメンバトルにいきましょう。”

![imageRL6-2-8](https://reinforce.github.io/images/imageRL6-2-8.jpg) 

C部長 ： “前回の論文読みでちょっと疲れ気味ですが、なにかありますか？”

D先生 ： “いや・・・。久々にオッサンにしましょう・・・。”

<iframe width="560" height="315" src="https://www.youtube.com/embed/nLrC0aWXHtI" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> 

D先生 : “タフですね、この人は・・・。”

