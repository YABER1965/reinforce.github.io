## QEUR21_MZDYNQ4: QEUの考えるModel-Basedとは(Qテーブル版)

## ～　やっと、「新しい強化学習」の準備ができた・・・　～

QEU：FOUNDER ; “では、我々なりの「Model-based」をやってみましょう。疑似Qテーブルを作成しましょう。“

![imageRL7-5-1](https://reinforce.github.io/images/imageRL7-5-1.jpg) 

D先生 : “えっ？いわゆる初期値っていうやつ？なんか、矛盾してません？Q-learningという強化学習は、結局のところは「Q-Tableを求める」ためにやっているんでしょ？”

**(今回のプロジェクトにおける「モデル」とは)**
- 同じ問題を解くための、Qテーブル解法とDQN解法がある
- Goal距離とminRT距離で、XY座標の代替が可能である
- Goal距離とminRT距離を使えば、最短ルート（移動命令群）の計算が可能である

QEU:FOUNDER ; “・・・だって、「今回のモデル」を使えば疑似Qテーブルを計算できるんだもん・・・。疑似Qテーブルの計算方法（↑）に異論がある人もいるだろうが、これはQ-learningの計算方法からきているんです。”

![imageRL7-5-2](https://reinforce.github.io/images/imageRL7-5-2.jpg) 

D先生 : “SARSAでは疑似Qテーブルができないわけですね・・・。”

QEU：FOUNDER ; “・・・だって、**「Goal距離とminRT距離を使えば、各位置の最適ルート（移動命令群）の計算が可能である」**んだもん・・・。Qmax式では、最適行動(a)のQ値が必要なのであって、その他の行動におけるQ値はQmax値以下であればなんでも良い。だから、小生はそれらのQ値を一律に「-10」にしたわけ・・・。”

![imageRL7-5-3](https://reinforce.github.io/images/imageRL7-5-3.jpg) 

D先生 : “つまり、FOUNDERにとって必要な情報は赤色のプロットの情報だけのわけね。そういう考え方もありますね・・・。”

QEU：FOUNDER ; “小生のモデルの考え方ね・・・。そうすると、何が可能となるのか・・・。実験用のプログラムをドン！！”

```python
# ----------------
# メイズ(MAZE)ゲームの強化学習システム
# step4B : 3Dメイズの再度準備段階です
# step4B : QMB_3DMazeDynaQ_Keras(DQN_size27).py
# step4B : TABLEでDyna-Qをやってみます（予備実験, インプットが4要素）
# ---------------- 
# 数値計算のﾗｲﾌﾞﾗﾘ読み込み
import numpy as np
import pandas as pd
import math, copy, random, time
from collections import defaultdict, deque, Counter
from IPython.display import clear_output
# ---------------- 
# Kerasをインポートする
from tensorflow.python.keras.models import Sequential
from tensorflow.python.keras.layers import Dense
from tensorflow.python.keras.models import load_model
# ---------------- 
#import seaborn as sns
import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = (16, 10)

# =================================================
# Game board difinition 
# =================================================
foldername  = "./MAZE_test/"

# =================================================
# difinition of STATE CALCULATION function
# =================================================
# 空間用minRTメトリックスを計算する
def space_minRT(pos_route, a_row, a_col):
    len_route = len(pos_route)
    arr_sminRT = []
    for i in range(len_route):
        pos_rt  = pos_route[i]
        sqEuc   = (a_row - pos_rt[0])**2 + (a_col - pos_rt[1])**2
        distEuc = np.sqrt(sqEuc)
        arr_sminRT.append(round(distEuc,5))   
    dist_sminRT = np.min(arr_sminRT)
        
    return dist_sminRT

# 状態の表記を変更(XY座標へ)、空間用minRTメトリックスつき
def calc_address(a_row, a_col, mx_lm):

    add_state = np.zeros(4)
    for i in range(len(mx_lm)):
        pos_lm  = mx_lm[i]
        sqEuc   = (a_row - pos_lm[0])**2 + (a_col - pos_lm[1])**2
        distEuc = np.sqrt(sqEuc)
        add_state[i] = round(distEuc,5)
    # --------------------------------------------------
    # 空間用minRTメトリックスを計算する
    # Aルート(標準)
    dist_sminRTA = space_minRT(mx_route, a_row, a_col)
    dist_sminRTA = round(dist_sminRTA,5)
    # Bルート(標準との差異分)
    dist_sminRTB = space_minRT(mx_routeB, a_row, a_col)
    dist_sminRTB = round(dist_sminRTB,5) - dist_sminRTA
    # Cルート(標準との差異分)
    dist_sminRTC = space_minRT(mx_routeC, a_row, a_col)
    dist_sminRTC = round(dist_sminRTC,5) - dist_sminRTA
    # Aルート値の符合化
    if add_state[2] > add_state[3]:
        dist_sminRTA = -1 * dist_sminRTA
    # --------------------------------------------------
    # 距離とりまとめリストを生成する
    arr_RTDists = np.zeros(4)
    arr_RTDists[0] = add_state[1]    # Goal距離
    arr_RTDists[1] = dist_sminRTA    # 空間minRT距離
    arr_RTDists[2] = dist_sminRTB    # 空間用minRT距離(B)
    arr_RTDists[3] = dist_sminRTC    # 空間用minRT距離(C)
    # --------------------------------------------------
    # STATEを生成する
    state = np.hstack([[a_row, a_col], [add_state[0], add_state[1]], [dist_sminRTA, dist_sminRTB, dist_sminRTC]])
    
    return state, arr_RTDists
    
# Qテーブル用アクションを決める
def calc_QTaction(i, j, movables, mx_lm):

    arr_action  = []
    for a_order in movables:
        # action number / row / col / 方向 / onehot
        # 0 / -1 / 0  / 上(up) / 1000
        # 1 / +1 / 0  / 下(down) / 0100
        # 2 / 0  / -1 / 左(left) / 0010
        # 3 / 0  / +1 / 右(right) / 0001
        # ----------
        yPos, xPos = i, j 
        if a_order == 0:
            yPos, xPos = i-1, j 
        elif a_order == 1:
            yPos, xPos = i+1, j 
        elif a_order == 2:
            yPos, xPos = i, j-1 
        elif a_order == 3:
            yPos, xPos = i, j+1 
        # 状態の表記を変更(XY座標へ)、空間用minRTメトリックスつき
        state, arr_RTDists = calc_address(yPos, xPos, mx_lm)
        # ゴール距離
        goal_Dist   = round(arr_RTDists[0],5)
        # 空間用minRTメトリックス(Aルート)を計算する
        minRTA_Dist = round(arr_RTDists[1],5)
        # 評価指数を計算する
        val_action  = 100/((goal_Dist/10)**2+minRTA_Dist**2+1.0)
        # リストに追加する
        arr_action.append(round(val_action,5))
    # -----
    temp_action = np.argmax(arr_action)
    action      = movables[temp_action]

    return action

# ----------
# Maze Class
class Maze(object):
    def __init__(self, size=27, blocks_rate=0.1):
        self.size = size if size > 3 else 27
        self.blocks = int((size ** 2) * blocks_rate) 
        self.s_list = []
        self.maze_list = []
        self.e_list = []

    def create_mid_lines(self, k):
        if k == 0: self.maze_list.append(self.s_list)
        elif k == self.size - 1: self.maze_list.append(self.e_list)
        else:
            tmp_list = []
            for l in range(0,self.size):
                if l == 0: tmp_list.extend("#")
                elif l == self.size-1: tmp_list.extend("#")
                else:
                    a = random.randint(-1, 0)
                    tmp_list.extend([a])
            self.maze_list.append(tmp_list)

    def insert_blocks(self, k, s_r, e_r):
        b_y = random.randint(1, self.size-2)
        b_x = random.randint(1, self.size-2)
        if [b_y, b_x] == [1, s_r] or [b_y, b_x] == [self.size - 2, e_r]: k = k-1
        else: self.maze_list[b_y][b_x] = "#"
            
    def generate_maze(self): 
        s_r = random.randint(1, int((self.size / 2)) - 1)
        for i in range(0, self.size):
            if i == s_r: self.s_list.extend("S")
            else: self.s_list.extend("#")
        start_point = [0, s_r]

        e_r = random.randint(int((self.size / 2)) + 1, self.size - 2)
        for j in range(0, self.size):
            if j == e_r: self.e_list.extend([50])
            else: self.e_list.extend("#")
        goal_point = [self.size - 1, e_r]

        for k in range(0, self.size):
            self.create_mid_lines(k)
        
        for k in range(self.blocks):
            self.insert_blocks(k, s_r, e_r)

        return self.maze_list, start_point, goal_point

    # --------------------------------------------------
    # 盤面のCSVファイルを読み込み表示する
    def read_boardfile(self):
        # CSVファイルの読み込み
        code_csvout = "maze_board27.csv"       # file name
        file_readcsv = foldername + code_csvout   # standard(training) file name  
        df = pd.read_csv(file_readcsv)
        #max_play = len(df)
        # print("データ量 max_play",max_play)
        # print(df)
        # --------------------------------------------------
        # 選択項目の読み込み
        maze = df.loc[:, "0":"26"].values.tolist()
        start_point = ( 0, 0 )
        goal_point  = ( 0, 0 )
        for i in range(size):
            for j in range(size):
                if maze[i][j] == 'S':
                    start_point = ( i, j )
                if maze[i][j] == '5000':
                    goal_point  = ( i, j )

        self.maze_list   = maze
        self.start_point = start_point
        self.goal_point  = goal_point
        print("goal_point", goal_point)
        print("----- mx_maze(read_boardfile) -----")
        print(maze)

        return self.maze_list, start_point, goal_point

    # --------------------------------------------------
    # コマの最短ルートのCSVファイルを読み込み表示する（標準Aルート）
    def read_chessfile(self):
        # CSVファイルの読み込み
        code_csvout = "chess_route27.csv"       # file name
        file_readcsv = foldername + code_csvout   # standard(training) file name  
        df = pd.read_csv(file_readcsv)
        #max_play = len(df)
        # print("データ量 max_play",max_play)
        # print(df)
        # --------------------------------------------------
        # 選択項目の読み込み
        mx_route    = df.loc[:, "row":"col"].values.tolist()
        ref_action  = df.loc[:, "action"].values.tolist()
        print("ref_action: ", ref_action)
        #print("----- route -----")
        #print(route)

        return mx_route, ref_action

    # --------------------------------------------------
    # コマの最短ルートのCSVファイルを読み込み表示する（限界BCルート）
    def read_chessfileBC(self, code_csvout):

        # CSVファイルの読み込み
        file_readcsv = foldername + code_csvout   # standard(training) file name  
        df = pd.read_csv(file_readcsv)
        # --------------------------------------------------
        # 選択項目の読み込み
        mx_route    = df.loc[:, "row":"col"].values.tolist()
        #print("----- route -----")
        #print(route)

        return mx_route

# ----------
# Maze functions
class Field(object):
    def __init__(self, maze, start_point, goal_point):

        # ----------------
        # 次の行動(ACTION)に進む
        # action number / row / col / 方向 / onehot
        # 0 / -1 / 0  / 上(up) / 1000
        # 1 / +1 / 0  / 下(down) / 0100
        # 2 / 0  / -1 / 左(left) / 0010
        # 3 / 0  / +1 / 右(right) / 0001
        # ----------------
        self.maze = maze
        self.start_point = start_point
        self.goal_point  = goal_point
        self.movable_vec = [[-1,0],[1,0],[0,-1],[0,1]]

    def display(self, point=None):
        field_data = copy.deepcopy(self.maze)
        if not point is None:
                y, x = point
                field_data[y][x] = "@@"
        else:
                point = ""
        print("================")
        for line in field_data:
                print ("\t" + "%3s " * len(line) % tuple(line))
        print("================")

    def all_display(self, mx_route):
        field_data = copy.deepcopy(self.maze)
        for i in range(len(mx_route)):
            point = mx_route[i]
            y, x = point[0], point[1]
            field_data[y][x] = "@@"
        print("================")
        for line in field_data:
                print ("\t" + "%3s " * len(line) % tuple(line))
        print("================")

    # --------------------------------------------------
    # 次のアクション(movable)を生成する
    def get_actions(self, state):
    
        movables = []
        if state == self.start_point:
            action = 1
            return [action]
        else:
            for i in range(4):
                pos_movable = self.movable_vec[i]
                y = state[0] + pos_movable[0]
                x = state[1] + pos_movable[1]
                if 0 < x < len(self.maze) and 0 < y < len(self.maze) and self.maze[y][x] != "#" and self.maze[y][x] != "S":
                    movables.append(i)
            if len(movables) != 0:
                return movables
            else:
                return None

    # --------------------------------------------------
    # 次のコマの「決定」位置と報酬、ゲームフラグを決める
    # 新概念：マスク「#」位置に着くと、スタート地点に戻る
    def get_val(self, action, state_chess):
        pos_movable = self.movable_vec[action]
        yPos = state_chess[0] + pos_movable[0]
        xPos = state_chess[1] + pos_movable[1]
        next_state = ( yPos, xPos )
        # ----------------
        if state_chess == self.start_point:
            return next_state, 0, False
        else:
            v = float(self.maze[yPos][xPos])
            if next_state == self.goal_point: 
                return next_state, v, True
            else: 
                return next_state, v, False

# ----------
# Initialize and Generate a maze
size    = 27
barriar_rate = 0.1
env     = Maze(size, barriar_rate)
# init_maze : カリキュラム学習用に改造
init_maze, start_point, goal_point = env.read_boardfile()
print("start_point: {}, goal_point: {}".format(start_point, goal_point))
# インスタンスの生成
maze_field = Field(init_maze, start_point, goal_point)
# ボード表示(初期表示)
maze_field.display()
# ----------
# 最短ルートを読み込み(Aルート)
mx_route, ref_action = env.read_chessfile()
# ルートを表示する
maze_field.all_display(mx_route)
# ----------
# コマの最短ルートのCSVファイルを読み込み表示する（限界BCルート）
# Bルート
mx_routeB = env.read_chessfileBC("chess_route27B.csv")
# Cルート
mx_routeC = env.read_chessfileBC("chess_route27C.csv")
#print("--- mx_routeC ---")
#print(mx_routeC)


# ---------------------------
# 各種パラメタ
mx_lm   = [(0, 2),(26, 25),(26, 0),(0, 26)]
action_space = 4
gamma   = 0.95
e_epsilon = 0.30    # randomness of choosing random action or the best one
e_decay = 0.992     # epsilon decay rate
e_min   = 0.01  # minimum rate of epsilon
alpha   = 0.85  # learning rate

# =======================================
# Q-Table based reinforcement learning
# =======================================
# Solving the maze in Q-learning
class Agent:
    def __init__(self, init_maze, model):

        # Q-Tableの初期化
        self.Q_table  = defaultdict(lambda: np.zeros(action_space))
        # Q値の分布を人工的に生成する
        for i in range(size):
            for j in range(size):
                if init_maze[i][j] != 'S' and init_maze[i][j] != '#': 
                
                    # 移動可能な状態(movables)を生成する
                    movables = maze_field.get_actions([i, j])
                    # Qテーブル用アクションを決める
                    action   = calc_QTaction(i, j, movables, mx_lm)
                    # --------
                    # リストに追加する
                    # 状態の表記を変更(XY座標へ)、空間用minRTメトリックスつき
                    state, arr_RTDists = calc_address(i, j, mx_lm)
                    Xnew = np.array([arr_RTDists])
                    ynew = model.predict(Xnew)
                    print("(i,j): ({},{}), action: {}, X: {}, Predicted: {}, ".format(i, j, action, Xnew[0], ynew[0][0]))
                    # --------
                    # Qテーブルに追加する
                    if action == 0:
                        self.Q_table[(i,j)] = np.array([ynew[0][0], -10, -10, -10])
                    elif action == 1:
                        self.Q_table[(i,j)] = np.array([-10, ynew[0][0], -10, -10])
                    elif action == 2:
                        self.Q_table[(i,j)] = np.array([-10, -10, ynew[0][0], -10])
                    elif action == 3:
                        self.Q_table[(i,j)] = np.array([-10, -10, -10, ynew[0][0]])
        print("--- self.Q_table ---") 
        print(self.Q_table)            
        # --------
        self.arr_iplay  = [] 
        self.QL_reward, self.QL_maxturn, self.QL_epsilon = [], [], []
        self.DQ_reward, self.DQ_maxturn, self.DQ_epsilon = [], [], []
        # -----
        # Q-Learning
        self.arr_iplay, self.QL_reward, self.QL_maxturn, self.QL_epsilon = self.run_q_learning(num_episodes, num_turns)
        self.QQL_table  = self.Q_table
   
        # ---------------------------
        # 画像出力用関数
        self.show_graph()
        
   
    def choose_action(self, state, movables, epsilon):
        if random.random() < epsilon:
            return np.random.choice(movables)
        else:
            values  = []
            for a_order in movables:
                values.append(self.Q_table[state][a_order])
            qmax_idx  = np.argmax(values)
            action    = movables[qmax_idx]
            #print("movables: {}, values: {}".format(movables, values))
            return action

    def q_update_value(self, state, action, reward, next_state, next_movables, done):
        if done:
            self.Q_table[state][action] += alpha * (reward - self.Q_table[state][action])
        else:
            values  = []
            for a_order in next_movables:
                values.append(self.Q_table[next_state][a_order])
            self.Q_table[state][action] += alpha * (reward + gamma * np.max(values) - self.Q_table[state][action])
        return self.Q_table
            
    def run_q_learning(self, num_episodes, num_turns):

        # 初期化
        epsilon = e_epsilon
        model   = {}
        arr_iplay, arr_reward, arr_maxturn, arr_epsilon = [], [], [], []
        # ------
        for i in range(num_episodes):
            # 状態(STATE)を生成する
            state   = start_point
            rewards, done   = 0, False
            iCnt_turn = 0
            # 移動可能な状態(movables)を生成する
            movables = maze_field.get_actions(state)
            # ------
            while True:
                action = self.choose_action(state, movables, epsilon)
                next_state, reward, done = maze_field.get_val(action, state)
                next_movables = maze_field.get_actions(next_state)
                # ------
                # 都合上ゴールをちょっとずらす
                if next_state[0]>=25 and next_state[1]>=25:
                    done   = True
                    reward = 25000 - iCnt_turn
                    print("episode:{}, turn:{}, state:{}, action:{}, reward:{}".format(i, iCnt_turn, state, action, reward))
                # ------
                rewards += reward
                self.q_update_value(state, action, reward, next_state, next_movables, done)
                model[(state, action)] = (reward, next_state, next_movables, done)
                # ------
                if iCnt_turn%1000 == 0:
                    print("episode:{}, turn:{}, state:{}, action:{}, reward:{}".format(i, iCnt_turn, state, action, reward))
                # ------
                if done == True:
                    break
                elif iCnt_turn > num_turns:
                    break
                state    = next_state
                movables = next_movables
                iCnt_turn = iCnt_turn + 1
            # ----------
            # リストに要素を追加します
            arr_iplay.append(i+1)
            arr_reward.append(rewards)
            arr_maxturn.append(iCnt_turn)
            arr_epsilon.append(epsilon)
            # ----------
            # しばらくすれば表示が消えます
            if i%50 == 0:
                #time.sleep(SLEEP_TIME)
                clear_output(wait=True)
            # ----------
            # イプシロンを更新します
            if epsilon > e_min:
                epsilon = round(epsilon*e_decay, 5)
            print(f'episode: {i + 1}/{num_episodes}, epsilon: {epsilon}, rewards: {rewards}, maxturn: {iCnt_turn}')

        return arr_iplay, arr_reward, arr_maxturn, arr_epsilon
        

    # ---------------------------
    # 画像出力用関数
    def show_graph(self):

        fig2 = plt.figure(figsize=(14, 12))
        # -----
        ax1 = fig2.add_subplot(2, 2, 1)
        ax1.set_title('REWARD, e_epsilon :{}'.format(e_epsilon))
        ax1.set_xlabel('#episode')
        ax1.set_ylabel('reward')
        ax1.plot(self.arr_iplay, self.QL_reward, label="QL", color="blue")
        ax1.grid(True)
        ax1.legend(loc='best')
        # -----
        ax2 = fig2.add_subplot(2, 2, 2)
        ax2.set_title('MAX_TURN, e_epsilon :{}'.format(e_epsilon))
        ax2.set_xlabel('#episode')
        ax2.set_ylabel('maxturn')
        ax2.grid(True)
        ax2.plot(self.arr_iplay, self.QL_maxturn, label="QL", color="blue")
        ax2.legend(loc='best')
        # -----
        ax3 = fig2.add_subplot(2, 2, 3)
        ax3.set_title('EPSILON, e_epsilon :{}'.format(e_epsilon))
        ax3.set_xlabel('#episode')
        ax3.set_ylabel('epsilon')
        ax3.grid(True)
        ax3.plot(self.arr_iplay, self.QL_epsilon, label="QL", color="blue")
        ax3.legend(loc='best')
        # -----
        fig2.tight_layout()
        #fig.savefig("./AAC_img.png")
        plt.show()


# ---------------------------
# パラメタ(その３)
num_episodes    = 100
num_turns       = 25000

# ---------------------------
# モデルの読み込み
code_modelout = "DynaQ_Testing4A.h5"       # file name
file_modelout = foldername + code_modelout   # standard(training) file name  
model = load_model(file_modelout)
model.summary()

# ---------------- 
# 実行
agent = Agent(init_maze, model)
# Qテーブルを呼び出す
#print("--- agent.QQL_table ---")
#print(agent.QQL_table)

# ---------------- 
# Initialize and Generate a maze
Q_mean = np.zeros([size, size])
Q_max  = np.zeros([size, size])
for i in range(size):
    for j in range(size):
        arr_QV = agent.QQL_table[(i,j)]
        Q_mean[i,j] = round(np.mean(arr_QV),3)
        Q_max[i,j]  = round(np.max(arr_QV),3)    
#print("--- Q_mean ---")
#print(Q_mean) 

# ---------------- 
# 座標変換をして、Q値の分布を把握してみる
arr_iRow, arr_jCol, arr_Goal    = [], [], []
arr_minRT, arr_Ymean, arr_Ymax  = [], [], []
# ------
for i in range(size):
    for j in range(size):
        # 状態の表記を変更(XY座標へ)、空間用minRTメトリックスつき
        state, arr_RTDists = calc_address(i, j, mx_lm)
        goal_Dist   = arr_RTDists[0]
        minRT_Dist  = arr_RTDists[1]
        y_mean     = Q_mean[i,j]
        y_max      = Q_max[i,j]
        # -----
        # リストに追加する
        arr_iRow.append(i)
        arr_jCol.append(j)
        arr_Goal.append(goal_Dist)
        arr_minRT.append(minRT_Dist)
        arr_Ymean.append(y_mean)
        arr_Ymax.append(y_max)
# ------
# Goal-minRT座標系
fig4 = plt.figure(figsize=(14, 9))
ax4 = fig4.add_subplot(projection='3d')
ax4.set_title('Goal-minRT Coodinate, e_epsilon :{}'.format(e_epsilon), size = 20)
ax4.set_xlabel("Goal", size = 14)
ax4.set_ylabel("minRT", size = 14)
ax4.set_zlabel("QV", size = 14)
ax4.scatter(arr_Goal, arr_minRT, arr_Ymean, color="blue")
ax4.scatter(arr_Goal, arr_minRT, arr_Ymax, color="red")
plt.show()

```

QEU：FOUNDER ; “まずは、普通に高いε値で計算してみましょう。”

**（パフォーマンス推移）**

![imageRL7-5-4](https://reinforce.github.io/images/imageRL7-5-4.jpg) 

**（Q値の分布）**

![imageRL7-5-5](https://reinforce.github.io/images/imageRL7-5-5.jpg) 

D先生 : “普通の結果ですね・・・。”

QEU:FOUNDER ; “我々がせっかく準備した「初期値（Qテーブル）」が書き換わっているからね・・・。同じに見えるのは当たり前でしょう。・・・それでは、εの初期値を0.5にしてみるとどうなるか・・・。”

D先生 : “ちょっと低すぎでは・・・。計算が暴走するんじゃないかな？”

QEU：FOUNDER ; “さてどうなるか・・・。繰り返しゲーム数はあえて100回に抑えました。”

**（パフォーマンス推移）**

![imageRL7-5-6](https://reinforce.github.io/images/imageRL7-5-6.jpg) 


**（Q値の分布）**

![imageRL7-5-7](https://reinforce.github.io/images/imageRL7-5-7.jpg) 

D先生 : “あれ？ほとんど問題ないじゃないですか？FOUNDERはQテーブルを乱暴に設定したが、これでもいいのか・・・。”

QEU：FOUNDER ; “だから、**最適行動（a）以外の命令におけるQ値はQmax値以下であれば、なんでもいいんだ**って・・・。それでは、もっとエゲツなく、εの値を0.3に設定してみましょう。

**（パフォーマンス推移）**

![imageRL7-5-8](https://reinforce.github.io/images/imageRL7-5-8.jpg) 

**（Q値の分布）**

![imageRL7-5-9](https://reinforce.github.io/images/imageRL7-5-9.jpg) 

D先生 : “モデル化の目的が**「計算量を徹底的に削減する」**のであるならば、このやり方が最も良いですね。”

QEU：FOUNDER ; “以前の、ボルツマン選択のときよりも合理的でしょ？もちろん、このように初期Q値を設定できるケースはそれほど多くはないとは思います。でも、自動搬送機（AGV）のケースでは、普通にあるとは思いますよ。”

D先生 : “シンプルな発想に好感が持てますね・・・。次は、DQN（ディープラーニングを使った強化学習）に進むんでしょ？”

QEU:FOUNDER ; “基本的に同じやりかただが、Q関数に行動の要素が追加されて、関数の当てはめが難しくなりますね。まあ、pytorchのピンチヒッターのKeras大明神に頑張ってもらいましょう。すいませ～ん。カンパをください！！”
 
### [＞寄付のお願い(Donate me)＜](https://www.paypal.com/paypalme/QEUglobal?v=1&utm_source=unp&utm_medium=email&utm_campaign=RT000481&utm_unptid=29844400-7613-11ec-ac72-3cfdfef0498d&ppid=RT000481&cnac=HK&rsta=en_GB%28en-HK%29&cust=5QPFDMW9B2T7Q&unptid=29844400-7613-11ec-ac72-3cfdfef0498d&calc=f860991d89600&unp_tpcid=ppme-social-business-profile-creat-ed&page=main%3Aemail%3ART000481&pgrp=main%3Aemail&e=cl&mchn=em&s=ci&mail=sys&appVersion=1.71.0&xt=104038)
 
D先生 ：“よろしくお願いします。”



## ～　まとめ　～

D部長: “あ～あ、**「保守」**ねぇ・・・。”

<iframe width="560" height="315" src="https://www.youtube.com/embed/SYwo9DVYXjQ" ti-tle="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; en-crypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

QEU:FOUNDER ; “小生は保守だよ。少なくとも、自分ではそう思っています。”

### おっさん（＠QCサークル講話）；「従業員の皆さんにはテレビを見てください。皆が同じように考えてください。」

### オッサン（＠車中、N社検査不正について）：　「“検査不正”っていうのはなァ、（組織外に不正を）漏らしたヤツが悪いんだよ・・・」

### オッサン（海外工場のあいさつにて）：「私の使命はこの会社で終身雇用制を実現することにある・・・。」

D先生: “アンタが保守ならば、これらのオッサン共（↑）もバリバリ保守でしょうに・・・。でも、「あの事件」が起きる前は、こんなオッサンでも、それなりに**「自分には良識がある」**って鼻の孔を膨らませて自慢していたものだった。”

QEU:FOUNDER ; “いやいや保身は大切だよ、身に染みて思います・・・。**保身って、傍目からは良識に見えちゃう**・・・（笑）。反対に、この人みたいにホントの事を言う人は・・・？”

<iframe width="560" height="315" src="https://www.youtube.com/embed/NNl3Z-hhdT8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

D部長: “嫌われるよね・・・。”

QEU:FOUNDER ; “あくまで**「（オレが）メシ食うための立場」**なんだし、みんな、ゆる～く行こうよ・・・。”

![imageRL7-5-10](https://reinforce.github.io/images/imageRL7-5-10.jpg) 

D先生: “Y先生、「空気よめ」、「本当のことを言うなよ」っていう感じ・・・。”

QEU:FOUNDER ： “**「保守が生きづらい時代」**になったものだ。某Youtubeチャンネルで、憂国の士がお嘆きになることでしょう。”

