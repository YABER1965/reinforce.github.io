## QEUR21_MZDNQ3: モデルを関数化(Q_model)する

## ～　しばらく、「テーブル」にこだわります　～

### ・・・　前回の話を少し蒸し返しましょう　・・・

D先生 : “そのゲーム（環境）を攻略するためのノウハウを数値化する能力があるのが「minRT距離」でしょ？ゴルフのスイングでいえば、手の動き、腰の捩じり方、足の動きのそれぞれにminRT距離を使えばいいんだと思います。・・・でも、我々は前回のトライアルで、なぜうまく行かなかったんでしょうね？”

**(今回のプロジェクトにおける「モデル」とは)**
- 同じ問題を解くための、Qテーブル解法とDQN解法がある
- Goal距離とminRT距離で、XY座標の代替が可能である
- Goal距離とminRT距離を使えば、最短ルート（移動命令群）の計算が可能である

QEU:FOUNDER ; “2つあると思います。当時の我々は**「モデル論の中心にはQ値がある」**ことを理解していなかった。ここでは、Q値を数値モデルにしてみましょう。つまるところ、回帰（regression）ということ・・・。”

![imageRL7-4-1](https://reinforce.github.io/images/imageRL7-4-1.jpg) 
 
QEU:FOUNDER ： “すでに説明したことだが、この美しい図ができたのはminRT距離を「符号化」したからです。ここに、あえて再度強調させてください。さて、今回はKerasで当てはめをやるよ。プログラムをドン！！”

```python
# ----------------
# メイズ(MAZE)ゲームの強化学習システム
# step3C : 3Dメイズの再度準備段階です
# step3C : DLno2_3DMazeDynaQ_Keras(DQN_size27).py
# step3C : TABLEでDyna-Qをやってみます（予備実験, インプットが4要素）
# ---------------- 
# 数値計算のﾗｲﾌﾞﾗﾘ読み込み
import numpy as np
import pandas as pd
import math, copy, random, time
from collections import defaultdict, deque, Counter
from scipy.special import softmax
from IPython.display import clear_output
# ---------------- 
#import seaborn as sns
import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = (16, 10)

# =================================================
# Game board difinition 
# =================================================
foldername  = "./MAZE_test/"

# =================================================
# difinition of STATE CALCULATION function
# =================================================
# 空間用minRTメトリックスを計算する
def space_minRT(pos_route, a_row, a_col):
    len_route = len(pos_route)
    arr_sminRT = []
    for i in range(len_route):
        pos_rt  = pos_route[i]
        sqEuc   = (a_row - pos_rt[0])**2 + (a_col - pos_rt[1])**2
        distEuc = np.sqrt(sqEuc)
        arr_sminRT.append(round(distEuc,5))   
    dist_sminRT = np.min(arr_sminRT)
        
    return dist_sminRT

# 状態の表記を変更(XY座標へ)、空間用minRTメトリックスつき
def calc_address(a_row, a_col, mx_lm):

    add_state = np.zeros(4)
    for i in range(len(mx_lm)):
        pos_lm  = mx_lm[i]
        sqEuc   = (a_row - pos_lm[0])**2 + (a_col - pos_lm[1])**2
        distEuc = np.sqrt(sqEuc)
        add_state[i] = round(distEuc,5)
    # --------------------------------------------------
    # 空間用minRTメトリックスを計算する
    # Aルート(標準)
    dist_sminRTA = space_minRT(mx_route, a_row, a_col)
    dist_sminRTA = round(dist_sminRTA,5)
    # Bルート(標準との差異分)
    dist_sminRTB = space_minRT(mx_routeB, a_row, a_col)
    dist_sminRTB = round(dist_sminRTB,5) - dist_sminRTA
    # Cルート(標準との差異分)
    dist_sminRTC = space_minRT(mx_routeC, a_row, a_col)
    dist_sminRTC = round(dist_sminRTC,5) - dist_sminRTA
    # Aルート値の符合化
    if add_state[2] > add_state[3]:
        dist_sminRTA = -1 * dist_sminRTA
    # --------------------------------------------------
    # 距離とりまとめリストを生成する
    arr_RTDists = np.zeros(4)
    arr_RTDists[0] = add_state[1]    # Goal距離
    arr_RTDists[1] = dist_sminRTA    # 空間minRT距離
    arr_RTDists[2] = dist_sminRTB    # 空間用minRT距離(B)
    arr_RTDists[3] = dist_sminRTC    # 空間用minRT距離(C)
    # --------------------------------------------------
    # STATEを生成する
    state = np.hstack([[a_row, a_col], [add_state[0], add_state[1]], [dist_sminRTA, dist_sminRTB, dist_sminRTC]])
    
    return state, arr_RTDists
    
# ----------
# Maze Class
class Maze(object):
    def __init__(self, size=27, blocks_rate=0.1):
        self.size = size if size > 3 else 27
        self.blocks = int((size ** 2) * blocks_rate) 
        self.s_list = []
        self.maze_list = []
        self.e_list = []

    def create_mid_lines(self, k):
        if k == 0: self.maze_list.append(self.s_list)
        elif k == self.size - 1: self.maze_list.append(self.e_list)
        else:
            tmp_list = []
            for l in range(0,self.size):
                if l == 0: tmp_list.extend("#")
                elif l == self.size-1: tmp_list.extend("#")
                else:
                    a = random.randint(-1, 0)
                    tmp_list.extend([a])
            self.maze_list.append(tmp_list)

    def insert_blocks(self, k, s_r, e_r):
        b_y = random.randint(1, self.size-2)
        b_x = random.randint(1, self.size-2)
        if [b_y, b_x] == [1, s_r] or [b_y, b_x] == [self.size - 2, e_r]: k = k-1
        else: self.maze_list[b_y][b_x] = "#"
            
    def generate_maze(self): 
        s_r = random.randint(1, int((self.size / 2)) - 1)
        for i in range(0, self.size):
            if i == s_r: self.s_list.extend("S")
            else: self.s_list.extend("#")
        start_point = [0, s_r]

        e_r = random.randint(int((self.size / 2)) + 1, self.size - 2)
        for j in range(0, self.size):
            if j == e_r: self.e_list.extend([50])
            else: self.e_list.extend("#")
        goal_point = [self.size - 1, e_r]

        for k in range(0, self.size):
            self.create_mid_lines(k)
        
        for k in range(self.blocks):
            self.insert_blocks(k, s_r, e_r)

        return self.maze_list, start_point, goal_point

    # --------------------------------------------------
    # 盤面のCSVファイルを読み込み表示する
    def read_boardfile(self):
        # CSVファイルの読み込み
        code_csvout = "maze_board27.csv"       # file name
        file_readcsv = foldername + code_csvout   # standard(training) file name  
        df = pd.read_csv(file_readcsv)
        #max_play = len(df)
        # print("データ量 max_play",max_play)
        # print(df)
        # --------------------------------------------------
        # 選択項目の読み込み
        maze = df.loc[:, "0":"26"].values.tolist()
        start_point = ( 0, 0 )
        goal_point  = ( 0, 0 )
        for i in range(size):
            for j in range(size):
                if maze[i][j] == 'S':
                    start_point = ( i, j )
                if maze[i][j] == '5000':
                    goal_point  = ( i, j )

        self.maze_list   = maze
        self.start_point = start_point
        self.goal_point  = goal_point
        print("goal_point", goal_point)
        print("----- mx_maze(read_boardfile) -----")
        print(maze)

        return self.maze_list, start_point, goal_point

    # --------------------------------------------------
    # コマの最短ルートのCSVファイルを読み込み表示する（標準Aルート）
    def read_chessfile(self):
        # CSVファイルの読み込み
        code_csvout = "chess_route27.csv"       # file name
        file_readcsv = foldername + code_csvout   # standard(training) file name  
        df = pd.read_csv(file_readcsv)
        #max_play = len(df)
        # print("データ量 max_play",max_play)
        # print(df)
        # --------------------------------------------------
        # 選択項目の読み込み
        mx_route    = df.loc[:, "row":"col"].values.tolist()
        ref_action  = df.loc[:, "action"].values.tolist()
        print("ref_action: ", ref_action)
        #print("----- route -----")
        #print(route)

        return mx_route, ref_action

    # --------------------------------------------------
    # コマの最短ルートのCSVファイルを読み込み表示する（限界BCルート）
    def read_chessfileBC(self, code_csvout):

        # CSVファイルの読み込み
        file_readcsv = foldername + code_csvout   # standard(training) file name  
        df = pd.read_csv(file_readcsv)
        #max_play = len(df)
        # print("データ量 max_play",max_play)
        # print(df)
        # --------------------------------------------------
        # 選択項目の読み込み
        mx_route    = df.loc[:, "row":"col"].values.tolist()
        #print("----- route -----")
        #print(route)

        return mx_route

# ----------
# Maze functions
class Field(object):
    def __init__(self, maze, start_point, goal_point):

        # ----------------
        # 次の行動(ACTION)に進む
        # action number / row / col / 方向 / onehot
        # 0 / -1 / 0  / 上(up) / 1000
        # 1 / +1 / 0  / 下(down) / 0100
        # 2 / 0  / -1 / 左(left) / 0010
        # 3 / 0  / +1 / 右(right) / 0001
        # ----------------
        self.maze = maze
        self.start_point = start_point
        self.goal_point  = goal_point
        self.movable_vec = [[-1,0],[1,0],[0,-1],[0,1]]

    def display(self, point=None):
        field_data = copy.deepcopy(self.maze)
        if not point is None:
                y, x = point
                field_data[y][x] = "@@"
        else:
                point = ""
        print("================")
        for line in field_data:
                print ("\t" + "%3s " * len(line) % tuple(line))
        print("================")

    def all_display(self, mx_route):
        field_data = copy.deepcopy(self.maze)
        for i in range(len(mx_route)):
            point = mx_route[i]
            y, x = point[0], point[1]
            field_data[y][x] = "@@"
        print("================")
        for line in field_data:
                print ("\t" + "%3s " * len(line) % tuple(line))
        print("================")

    # --------------------------------------------------
    # 次のアクション(movable)を生成する
    def get_actions(self, state):
        # ----------------
        # 次の行動(ACTION)に進む
        # action number / row / col / 方向 / onehot
        # 0 / -1 / 0  / 上(up) / 1000
        # 1 / +1 / 0  / 下(down) / 0100
        # 2 / 0  / -1 / 左(left) / 0010
        # 3 / 0  / +1 / 右(right) / 0001
        # ----------------
        movables = []
        if state == self.start_point:
            action = 1
            return [action]
        else:
            for i in range(4):
                pos_movable = self.movable_vec[i]
                y = state[0] + pos_movable[0]
                x = state[1] + pos_movable[1]
                if 0 < x < len(self.maze) and 0 < y < len(self.maze) and self.maze[y][x] != "#" and self.maze[y][x] != "S":
                    movables.append(i)
            if len(movables) != 0:
                return movables
            else:
                return None

    # --------------------------------------------------
    # 次のコマの「決定」位置と報酬、ゲームフラグを決める
    # 新概念：マスク「#」位置に着くと、スタート地点に戻る
    def get_val(self, action, state_chess):
        pos_movable = self.movable_vec[action]
        yPos = state_chess[0] + pos_movable[0]
        xPos = state_chess[1] + pos_movable[1]
        next_state = ( yPos, xPos )
        # ----------------
        if state_chess == self.start_point:
            return next_state, 0, False
        else:
            v = float(self.maze[yPos][xPos])
            if next_state == self.goal_point: 
                return next_state, v, True
            else: 
                return next_state, v, False

# ----------
# Initialize and Generate a maze
size    = 27
barriar_rate = 0.1
env     = Maze(size, barriar_rate)
# init_maze : カリキュラム学習用に改造
init_maze, start_point, goal_point = env.read_boardfile()
print("start_point: {}, goal_point: {}".format(start_point, goal_point))
# インスタンスの生成
maze_field = Field(init_maze, start_point, goal_point)
# ボード表示(初期表示)
maze_field.display()
# ----------
# 最短ルートを読み込み(Aルート)
mx_route, ref_action = env.read_chessfile()
# ルートを表示する
maze_field.all_display(mx_route)
# ----------
# コマの最短ルートのCSVファイルを読み込み表示する（限界BCルート）
# Bルート
mx_routeB = env.read_chessfileBC("chess_route27B.csv")
# Cルート
mx_routeC = env.read_chessfileBC("chess_route27C.csv")

# ---------------------------
# 各種パラメタ
mx_lm   = [(0, 2),(26, 25),(26, 0),(0, 26)]
action_space = 4
gamma   = 0.95
e_epsilon = 0.97    # randomness of choosing random action or the best one
e_decay = 0.992     # epsilon decay rate
e_min   = 0.01  # minimum rate of epsilon
alpha   = 0.85  # learning rate

# =======================================
# Q-Table based reinforcement learning
# =======================================
# Solving the maze in Q-learning
class Agent:
    def __init__(self):

        self.arr_iplay  = [] 
        self.QL_reward, self.QL_maxturn, self.QL_epsilon = [], [], []
        self.DQ_reward, self.DQ_maxturn, self.DQ_epsilon = [], [], []
        # -----
        # Q-Learning
        self.Q_table  = defaultdict(lambda: np.zeros(action_space))
        flg_planning  = False
        self.arr_iplay, self.QL_reward, self.QL_maxturn, self.QL_epsilon = self.run_q_learning(num_episodes, num_turns, n_planning, flg_planning)
        self.QQL_table  = self.Q_table
        
    def choose_action(self, state, movables, epsilon):
        if random.random() < epsilon:
            return np.random.choice(movables)
        else:
            values  = []
            for a_order in movables:
                values.append(self.Q_table[state][a_order])
            qmax_idx  = np.argmax(values)
            action    = movables[qmax_idx]
            #print("movables: {}, values: {}".format(movables, values))
            return action

    def q_update_value(self, state, action, reward, next_state, next_movables, done):
        if done:
            self.Q_table[state][action] += alpha * (reward - self.Q_table[state][action])
        else:
            values  = []
            for a_order in next_movables:
                values.append(self.Q_table[next_state][a_order])
            self.Q_table[state][action] += alpha * (reward + gamma * np.max(values) - self.Q_table[state][action])
        return self.Q_table
            
    def run_q_learning(self, num_episodes, num_turns, n_planning, flg_planning):

        # 初期化
        epsilon = e_epsilon
        model   = {}
        arr_iplay, arr_reward, arr_maxturn, arr_epsilon = [], [], [], []
        # ------
        for i in range(num_episodes):
            # 状態(STATE)を生成する
            state   = start_point
            rewards, done   = 0, False
            iCnt_turn = 0
            # 移動可能な状態(movables)を生成する
            movables = maze_field.get_actions(state)
            # ------
            while True:
                action = self.choose_action(state, movables, epsilon)
                next_state, reward, done = maze_field.get_val(action, state)
                next_movables = maze_field.get_actions(next_state)
                # ------
                # 都合上ゴールをちょっとずらす
                if next_state[0]>=25 and next_state[1]>=25:
                    done   = True
                    reward = 25000 - iCnt_turn
                    print("episode:{}, turn:{}, state:{}, action:{}, reward:{}".format(i, iCnt_turn, state, action, reward))
                # ------
                rewards += reward
                self.q_update_value(state, action, reward, next_state, next_movables, done)
                model[(state, action)] = (reward, next_state, next_movables, done)
                if i > 10 and flg_planning == True:
                    for _ in range(n_planning):
                        (S, A), (R, S_, M_, done_) = random.choice(list(model.items()))
                        self.q_update_value(S, A, R, S_, M_, done_)
                # ------
                if iCnt_turn%1000 == 0:
                    print("episode:{}, turn:{}, state:{}, action:{}, reward:{}".format(i, iCnt_turn, state, action, reward))
                # ------
                if done == True:
                    break
                elif iCnt_turn > num_turns:
                    break
                state    = next_state
                movables = next_movables
                iCnt_turn = iCnt_turn + 1
            # ----------
            # リストに要素を追加します
            arr_iplay.append(i+1)
            arr_reward.append(rewards)
            arr_maxturn.append(iCnt_turn)
            arr_epsilon.append(epsilon)
            # ----------
            # しばらくすれば表示が消えます
            if i%50 == 0:
                #time.sleep(SLEEP_TIME)
                clear_output(wait=True)
            # ----------
            # イプシロンを更新します
            if epsilon > e_min:
                epsilon = round(epsilon*e_decay, 5)
            print(f'episode: {i + 1}/{num_episodes}, epsilon: {epsilon}, rewards: {rewards}, maxturn: {iCnt_turn}')

        return arr_iplay, arr_reward, arr_maxturn, arr_epsilon
        
# ---------------------------
# パラメタ(その３)
num_episodes    = 300
num_turns       = 25000
n_planning      = 10
# ------
# 実行
agent = Agent()
# Qテーブルを呼び出す
#print("--- agent.QQL_table ---")
#print(agent.QQL_table)
# ------
# Initialize and Generate a maze
Q_mean = np.zeros([size, size])
Q_max  = np.zeros([size, size])
for i in range(size):
    for j in range(size):
        arr_QV = agent.QQL_table[(i,j)]
        Q_mean[i,j] = round(np.mean(arr_QV),3)
        Q_max[i,j]  = round(np.max(arr_QV),3)    

# ---------------- 
# 座標変換をして、Q値の分布を把握してみる
arr_iRow, arr_jCol, arr_Goal    = [], [], []
arr_minRT, arr_Ymean, arr_Ymax  = [], [], []
# ------
for i in range(size):
    for j in range(size):
        # 状態の表記を変更(XY座標へ)、空間用minRTメトリックスつき
        state, arr_RTDists = calc_address(i, j, mx_lm)
        goal_Dist   = arr_RTDists[0]
        minRT_Dist  = arr_RTDists[1]
        y_mean     = Q_mean[i,j]
        y_max      = Q_max[i,j]
        # -----
        # リストに追加する
        arr_iRow.append(i)
        arr_jCol.append(j)
        arr_Goal.append(goal_Dist)
        arr_minRT.append(minRT_Dist)
        arr_Ymean.append(y_mean)
        arr_Ymax.append(y_max)
# ------
# Goal-minRT座標系
fig4 = plt.figure(figsize=(14, 9))
ax4 = fig4.add_subplot(projection='3d')
ax4.set_title('Goal-minRT Coodinate', size = 20)
ax4.set_xlabel("Goal", size = 14)
ax4.set_ylabel("minRT", size = 14)
ax4.set_zlabel("QV", size = 14)
ax4.scatter(arr_Goal, arr_minRT, arr_Ymean, color="blue")
ax4.scatter(arr_Goal, arr_minRT, arr_Ymax, color="red")
plt.show()

# =================================================
# Deep Learning
# =================================================
# Kerasをインポートする
from tensorflow.python.keras.models import Sequential
from tensorflow.python.keras.layers import Dense

# ---------------- 
# init_maze : カリキュラム学習用に改造
#init_maze, start_point, goal_point = env.read_boardfile()
# 座標変換をして、Q値の分布を把握してみる
arr_Ymean   = []
iCnt    = 0
for i in range(size):
    for j in range(size):
        if init_maze[i][j] != 'S' and init_maze[i][j] != '#': 
            # 状態の表記を変更(XY座標へ)、空間用minRTメトリックスつき
            state, arr_RTDists = calc_address(i, j, mx_lm)
            goal_Dist   = arr_RTDists[0]
            minRTA_Dist = arr_RTDists[1]
            minRTB_Dist = arr_RTDists[2]
            minRTC_Dist = arr_RTDists[3]
            y_mean    = Q_mean[i,j]
            y_max     = Q_max[i,j]
            # -----
            # リストに追加する
            if iCnt == 0:
                mx_Xs = [[goal_Dist, minRTA_Dist, minRTB_Dist, minRTC_Dist]]
            else:
                mx_Xs = np.concatenate([mx_Xs, [[goal_Dist, minRTA_Dist, minRTB_Dist, minRTC_Dist]]], axis=0)
            arr_Ymean.append(y_mean)
            iCnt    = iCnt + 1
# --------
mx_Xs = np.array(mx_Xs)
#print("-- mx_Xs --")
#print(mx_Xs)
temp_Ymean = np.array(arr_Ymean)
arr_Ymean  = np.reshape(temp_Ymean, (-1,1))
#print("arr_Ymean: ", arr_Ymean)
# --------
model = Sequential()
model.add(Dense(500, input_dim=4, kernel_initializer='normal', activation='relu'))
model.add(Dense(500, activation='relu'))
model.add(Dense(250, activation='relu'))
model.add(Dense(1, activation='linear'))
model.summary()
model.compile(loss='mse', optimizer='adam', metrics=['mse','mae'])
history = model.fit(mx_Xs, arr_Ymean, epochs=2000, batch_size=80,  verbose=1, validation_split=0.2)
# --------
print(history.history.keys())
# "Loss"
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

# ---------------- 
Xnew = mx_Xs
ynew = model.predict(Xnew)
for i in range(len(Xnew)):
    print("X=: {}, Raw=: {}, Predicted=: {}".format(Xnew[i], arr_Ymean[i][0], ynew[i][0]))
# --------
# torch can only train on Variable, so convert them to Variable
fig2 = plt.figure(figsize=(10, 7))
ax2 = fig2.add_subplot(projection='3d')
x1 = Xnew[:,0]
x2 = Xnew[:,1]
y_mean = arr_Ymean[:,0]
y_pred = ynew[:,0]
ax2.scatter(x1, x2, y_mean, color="blue")
ax2.scatter(x1, x2, y_pred, color="red")
plt.show()

```

D先生: “あれ？FOUNDERはJeremy Howardに倣い、PyTorch推しじゃないんですか？。”

<iframe width="560" height="315" src="https://www.youtube.com/embed/XHyASP49ses" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

QEU:FOUNDER ; “そうだよ。今だって基本はネ・・・。でもね・・・。今回のプロジェクトでは、PyTorchはなぜか当てはまりが悪い。もちろん、小生のノウハウの欠如を認めたうえでね・・・。まずは、ゴール距離と符号化minRT距離（2次元）だけで当てはめた結果をみてみましょう。”

![imageRL7-4-2](https://reinforce.github.io/images/imageRL7-4-2.jpg) 

QEU:FOUNDER ; “次は、**BルートとCルートのminRT距離を追加（4次元）しましょう**。ドン！！”

![imageRL7-4-3](https://reinforce.github.io/images/imageRL7-4-3.jpg) 

D先生 : “見た目は大きな差がないですが、**validation誤差は明らかに良化**しています。Overfitも少し小さいですし・・・。・・・で・・・、この結果を何に使うんですか？DQN with experience replayに？”

QEU:FOUNDER ; “まあ、今回のプロジェクトに限ってはPyTorchはやめとこうかとも思っています。プログラムの変更が大変になるけど・・・（笑）。あとね・・・、次のステップとして、Q_Tableにも使ってみたいと思っているんだ。小生の「モデルの理解」のために・・・。”

D先生 : “はあ？”


## ～　まとめ　～

D先生: “イケメンバトルの時間ですよね？例の低調の・・・。”

QEU：FOUNDER ; “イケメンじゃなくて、**プリティの時代**なのかねえ・・・。”

<iframe width="560" height="315" src="https://www.youtube.com/embed/QCbzJ6qxVaU" ti-tle="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; en-crypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

QEU：FOUNDER ; “いやいや、オッサンもやるねえ。・・・ということで、彼のwikiを見ていたんですが”

D先生 : “どれどれ・・・？”

![imageRL7-4-4](https://reinforce.github.io/images/imageRL7-4-4.jpg) 

D先生 : “どれどれ・・・？はっ！？新宗教とのかかわりが・・・。けしから～ん！！”

QEU：FOUNDER ; “D先生はこの宗教の事、しってる？”

![imageRL7-4-5](https://reinforce.github.io/images/imageRL7-4-5.jpg) 

D先生 : “知りません・・・。ここは昔、朝にスポンサーのラジオ番組があったぐらいは知っていますが・・・。Wikiによれば、ココはなんか・・・、奥の深いところですねえ・・・。”

QEU：FOUNDER ; “それでは、我々はノーコメントということで・・・（笑）。”

