## QEUR21_2DMZA3:　A3Cはもっといいよ・・・

## ～  またもや土壇場での方針変更でした  ～

### ・・・　A3Cのパワーを思い知れ・・・！　・・・

QEU:FOUNDER ： “もともとはAACの予定だったが、Asynchronous Advantage Actor-Critics(A3C)に変えるわ・・・。”

![imageRL4-12-1](https://reinforce.github.io/images/imageRL4-12-1.jpg) 

D先生 : “またスキームを変えるの？”

![imageRL4-12-2](https://reinforce.github.io/images/imageRL4-12-2.jpg) 

QEU:FOUNDER ： “「手法のよしあし」というよりも、コードを見て使えるかどうかを考えていました。ちょうど、イメージに合ったネタ・コードがあったので・・・。やっぱり、C国のエンジニアが作ったコードはいいね。”

D先生 : “AAC（A2C）とA3Cは何が違うんですか？”

![imageRL4-12-3](https://reinforce.github.io/images/imageRL4-12-3.jpg) 

QEU:FOUNDER ： “調べたけど、よくわからん・・・（笑）。**バラツキ（分散）を減らすために「Advantageという関数（↑）を使いたい」**という意図が最初にあった。そして、Actor-Critics法を拡張し、DQN_with_Experience_replayのようにインプット情報をバッチでまとめて学習させたいのでA3Cスキームが生まれ、その後A2Cに改良されたとか・・・。はっきり言って、ここらへんはWebでのサンプリング知識なので、詳細はよくわかりません。小生としてはAdvantage関数を使うことが目的なので、どうでもいいです。”

![imageRL4-12-4](https://reinforce.github.io/images/imageRL4-12-4.jpg) 

D先生 : “今回のコードを見ていたんですが、例のテキスト（↑）に載っているA3Cのコードと相当違いますね。”

QEU:FOUNDER ： “Loss値の扱い方など、やりかたが相当に違うよね。A3C開発時点や「流派」などあるだろうし・・・。まあ、小生とすれば使えればいいんですよ。”

D先生 ： “プログラムに関して他に言うことは？”

QEU:FOUNDER ： “プログラムが大きく変わったので、いちいち説明したらキリがない。でも、プレイの初期で強制学習をさせていることは強調しておきたい。（強制学習が）あるとないとでは、あとの学習収束が大きくかわるよ・・・。それではプログラムをドン！！”

```python
# ----------------
# メイズ(MAZE)ゲームの強化学習システム
# stepD1 : RTを動的メトリックスとして活用する
# stepD1 : reinforce_maze_pyTorch(A3C-sminRT).py
# stepD1 : A3CとsminRT距離を導入する(フィードバックはオプションです)
# ---------------- 
# 数値計算のﾗｲﾌﾞﾗﾘ読み込み
import numpy as np
import pandas as pd
import math, copy, random, time
# -----
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable
# -----
import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = (16, 10)

# =================================================
# Game board difinition 
# =================================================
foldername  = "./MAZE_test/"
init_maze = [['#', 'S', '#', '#', '#', '#', '#', '#', '#', '#'], ['#', 0, '#', -1, 0, -1, 0, '#', '#', '#'], ['#', -1, '#', -1, -1, 0, -1, -1, 0, '#'], ['#', 0, -1, 0, 0, -1, 0, 0, '#', '#'], ['#', '#', -1, 0, 0, -1, '#', -1, 0, '#'], ['#', -1, 0, 0, -1, -1, 0, '#', -1, '#'], ['#', 0, 0, 0, -1, '#', -1, '#', -1, '#'], ['#', 0, -1, 0, -1, 0, -1, 0, -1, '#'], ['#', 0, -1, 0, 0, -1, 0, 0, -1, '#'], ['#', '#', '#', '#', '#', '#', '#', 50, '#', '#']]
mx_lm2 = [[0, 1],[9, 7]]

# --------------------------------------------------
# MAZE盤面をCSVファイルに保存する
def save_csvMAZE(maze): 
    # --------------------------------------------------
    # CSV出力用のデータフレームを作る(2)
    arr_columns = list(range(10))
    # -----
    df_csvout = pd.DataFrame(maze, columns=arr_columns)
    #print(df_csvout)
    
    # --------------------------------------------------
    # CSV ファイル (file_csvout) として出力
    code_csvout = "maze_board.csv"       # file name  
    file_csvout = foldername + code_csvout   # standard(training) file name   
    print("メトリックス保管用CSVファイル ：{0}".format(file_csvout))
    # -----
    df_csvout.to_csv(file_csvout, index=True)

# MAZE盤面をCSVファイルに保存する
#save_csvMAZE(init_maze)

# =================================================
# difinition of STATE CALCULATION function
# =================================================
# タイムライン用minRTメトリックスを計算する
def timeline_minRT(a_row, a_col, iCnt_turn):
    len_route = len(mx_route)
    if iCnt_turn <= len_route - 1:
        pos_rt  = mx_route[iCnt_turn]
    else:
        pos_rt  = mx_route[len_route - 1]
    # -----
    tlEuc   = (a_row - pos_rt[0])**2 + (a_col - pos_rt[1])**2
    dist_tlminRT = np.sqrt(tlEuc)
        
    return round(dist_tlminRT,5)

# -------
# 空間用minRTメトリックスを計算する
def space_minRT(a_row, a_col):
    len_route = len(mx_route)
    arr_sminRT = []
    for i in range(len_route):
        pos_rt  = mx_route[i]
        sqEuc   = (a_row - pos_rt[0])**2 + (a_col - pos_rt[1])**2
        distEuc = np.sqrt(sqEuc)
        arr_sminRT.append(round(distEuc,5))   
    dist_sminRT = np.min(arr_sminRT)
        
    return dist_sminRT

# -------
# 状態の表記を変更(STATE-6_landmarkへ)、空間用minRTメトリックスつき
def calc_address(a_row, a_col):

    temp_state = np.zeros([size, size])
    temp_state[a_row, a_col] = 1.0
    onehot_state = temp_state.flatten()
    
    add_state  = np.zeros(2)
    for i in range(2):
        pos_lm  = mx_lm2[i]
        sqEuc   = (a_row - pos_lm[0])**2 + (a_col - pos_lm[1])**2
        distEuc = np.sqrt(sqEuc)
        add_state[i] = round(distEuc,5)
             
    # 空間用minRTメトリックスを計算する
    dist_sminRT  = space_minRT(a_row, a_col)

    state = np.hstack([onehot_state, [dist_sminRT], add_state])

    return state

# ----------
# Maze Class
class Maze(object):
    def __init__(self, size=10, blocks_rate=0.1):
        self.size = size if size > 3 else 10
        self.blocks = int((size ** 2) * blocks_rate) 
        self.s_list = []
        self.maze_list = []
        self.e_list = []

    def create_mid_lines(self, k):
        if k == 0: self.maze_list.append(self.s_list)
        elif k == self.size - 1: self.maze_list.append(self.e_list)
        else:
            tmp_list = []
            for l in range(0,self.size):
                if l == 0: tmp_list.extend("#")
                elif l == self.size-1: tmp_list.extend("#")
                else:
                    a = random.randint(-1, 0)
                    tmp_list.extend([a])
            self.maze_list.append(tmp_list)

    def insert_blocks(self, k, s_r, e_r):
        b_y = random.randint(1, self.size-2)
        b_x = random.randint(1, self.size-2)
        if [b_y, b_x] == [1, s_r] or [b_y, b_x] == [self.size - 2, e_r]: k = k-1
        else: self.maze_list[b_y][b_x] = "#"
            
    def generate_maze(self): 
        s_r = random.randint(1, int((self.size / 2)) - 1)
        for i in range(0, self.size):
            if i == s_r: self.s_list.extend("S")
            else: self.s_list.extend("#")
        start_point = [0, s_r]

        e_r = random.randint(int((self.size / 2)) + 1, self.size - 2)
        for j in range(0, self.size):
            if j == e_r: self.e_list.extend([50])
            else: self.e_list.extend("#")
        goal_point = [self.size - 1, e_r]

        for k in range(0, self.size):
            self.create_mid_lines(k)
        
        for k in range(self.blocks):
            self.insert_blocks(k, s_r, e_r)

        return self.maze_list, start_point, goal_point

    # --------------------------------------------------
    # 盤面のCSVファイルを読み込み表示する
    def read_boardfile(self):
        # CSVファイルの読み込み
        code_csvout = "maze_board.csv"       # file name
        file_readcsv = foldername + code_csvout   # standard(training) file name  
        df = pd.read_csv(file_readcsv)
        #max_play = len(df)
        # print("データ量 max_play",max_play)
        # print(df)
        # --------------------------------------------------
        # 選択項目の読み込み
        maze = df.loc[:, "0":"9"].values.tolist()
        start_point = [0,0]
        goal_point  = [0,0]
        for i in range(size):
            for j in range(size):
                if maze[i][j] == 'S':
                    start_point = [i, j]
                if maze[i][j] == '50':
                    goal_point  = [i, j]

        self.maze_list   = maze
        self.start_point = start_point
        self.goal_point  = goal_point
        #print("----- mx_maze(read_boardfile) -----")
        #print(mx_maze)

        return self.maze_list, start_point, goal_point

    # --------------------------------------------------
    # コマの最短ルートのCSVファイルを読み込み表示する
    def read_chessfile(self):
        # CSVファイルの読み込み
        code_csvout = "chess_route.csv"       # file name
        file_readcsv = foldername + code_csvout   # standard(training) file name  
        df = pd.read_csv(file_readcsv)
        #max_play = len(df)
        # print("データ量 max_play",max_play)
        # print(df)
        # --------------------------------------------------
        # 選択項目の読み込み
        mx_route    = df.loc[:, "row":"col"].values.tolist()
        ref_action  = df.loc[:, "action"].values.tolist()
        print("ref_action: ", ref_action)
        #print("----- route -----")
        #print(route)

        return mx_route, ref_action

# ----------
# Maze functions
class Field(object):
    def __init__(self, maze, start_point, goal_point):
        self.maze = maze
        self.start_point = start_point
        self.goal_point  = goal_point
        self.movable_vec = [[-1,0],[1,0],[0,-1],[0,1]]

    def display(self, point=None):
        field_data = copy.deepcopy(self.maze)
        if not point is None:
                y, x = point
                field_data[y][x] = "@@"
        else:
                point = ""
        print("================")
        for line in field_data:
                print ("\t" + "%3s " * len(line) % tuple(line))
        print("================")

    def all_display(self, mx_route):
        field_data = copy.deepcopy(self.maze)
        for i in range(len(mx_route)):
            point = mx_route[i]
            y, x = point[0], point[1]
            field_data[y][x] = "@@"
        print("================")
        for line in field_data:
                print ("\t" + "%3s " * len(line) % tuple(line))
        print("================")

    def get_actions(self, state):
        movables = []
        if state == self.start_point:
            y = state[0] + 1
            x = state[1]
            a = [[y, x]]
            return a
        else:
            for v in self.movable_vec:
                y = state[0] + v[0]
                x = state[1] + v[1]
                if not(0 < x < len(self.maze) and
                       0 <= y <= len(self.maze) - 1 and
                       maze[y][x] != "#" and
                       maze[y][x] != "S"):
                    continue
                movables.append([y,x])
            if len(movables) != 0:
                return movables
            else:
                return None

    # --------------------------------------------------
    # 次のコマの「決定」位置と報酬、ゲームフラグを決める
    # 新概念：マスク「#」位置に着くと、スタート地点に戻る
    def get_val(self, action_chess, state_chess, movables):
        yPos = action_chess[0]
        xPos = action_chess[1]
        # ----------------
        if action_chess == self.start_point:
            nxt_state = action_chess
            return nxt_state, 0, False, 0
        elif self.maze[yPos][xPos] == "#":
            #print("movables: ", movables)
            if len(movables) > 0:
                nxt_state = random.choice(movables)
            else:
                nxt_state = self.start_point
            v = -0.00        # -10
            return nxt_state, v, False, 1
        else:
            nxt_state = action_chess
            v = float(self.maze[yPos][xPos])
            if action_chess == self.goal_point: 
                return nxt_state, v, True, 0
            else: 
                return nxt_state, v, False, 0

    # --------------------------------------------------
    # 次のコマの位置を決める
    def nxtPosition(self, state_chess, action):
        # ----------------
        # 次の行動(ACTION)に進む
        # action number / row / col / 方向 / onehot
        # 0 / -1 / 0  / 上(up) / 1000
        # 1 / +1 / 0  / 下(down) / 0100
        # 2 / 0  / -1 / 左(left) / 0010
        # 3 / 0  / +1 / 右(right) / 0001
        # ----------------
        # ACTION_CHESS
        if action == 0:
            diff_row, diff_col = -1, 0
        if action == 1:
            diff_row, diff_col = 1, 0 
        if action == 2:
            diff_row, diff_col = 0, -1 
        if action == 3:
            diff_row, diff_col = 0, 1 
        # ----------------
        nxtPos    = [0,0]
        nxtPos[0] = state_chess[0] + diff_row   # y方向
        nxtPos[1] = state_chess[1] + diff_col   # x方向

        return nxtPos

# ----------
# Generate a maze
size = 10
barriar_rate = 0.1

maze_1 = Maze(size, barriar_rate)
#maze, start_point, goal_point = maze_1.generate_maze()
maze, start_point, goal_point = maze_1.read_boardfile()
maze_field = Field(maze, start_point, goal_point)
# 普通の表示
maze_field.display()

# 最短ルートを読み込み
mx_route, ref_action = maze_1.read_chessfile()
# ルートの表示(個別表示)
#for i in range(len(mx_route)):
#    maze_field.display(mx_route[i])
# ルートの表示(全表示)
maze_field.all_display(mx_route)

#=================================================
# Calculation class(1) : Actor-Network
#=================================================
class ActorNetwork(nn.Module):

    def __init__(self,input_size,hidden_size,action_size):
        super(ActorNetwork, self).__init__()
        self.fc1 = nn.Linear(input_size,hidden_size)
        self.fc2 = nn.Linear(hidden_size,hidden_size)
        self.fc3 = nn.Linear(hidden_size,action_size)

    def forward(self,x):
        out = F.relu(self.fc1(x))
        out = F.relu(self.fc2(out))
        out = F.log_softmax(self.fc3(out), dim = 1)
        return out

class ValueNetwork(nn.Module):

    def __init__(self,input_size,hidden_size,output_size):
        super(ValueNetwork, self).__init__()
        self.fc1 = nn.Linear(input_size,hidden_size)
        self.fc2 = nn.Linear(hidden_size,hidden_size)
        self.fc3 = nn.Linear(hidden_size,output_size)

    def forward(self,x):
        out = F.relu(self.fc1(x))
        out = F.relu(self.fc2(out))
        out = self.fc3(out)
        return out

#=================================================
# Calculation class(2) : Agent
#=================================================
class Agent():
    def __init__(self):

        # init value network
        self.value_network = ValueNetwork(input_size = STATE_DIM, hidden_size = 140, output_size = 1)
        self.value_network_optim = torch.optim.Adam(self.value_network.parameters(), lr=0.001)

        # init actor network
        self.actor_network = ActorNetwork(input_size = STATE_DIM, hidden_size = 140, action_size = ACTION_DIM)
        self.actor_network_optim = torch.optim.Adam(self.actor_network.parameters(), lr = 0.001)

        # ---------------------------
        # 盤面データを読む
        self.maze = maze
        self.start_point = start_point
        self.goal_point = goal_point
        # ----------------
        # リストの初期化
        self.arr_iplay      = []
        self.arr_maxscore   = []
        self.arr_maxturn    = []
        self.arr_actorloss  = []
        self.arr_criticloss = []
        # 距離の分析用累積リスト
        self.arr_maxTD      = []  # Total距離
        self.arr_sminRTD    = []  # 空間minRT距離
        self.arr_tlminRTD   = []  # タイムラインminRT距離
        # ----------------
        # 学習する
        for iCnt_play in range(EPISODE_STEP):
            maxturn, maxscore, value_loss, actor_loss = self.train(iCnt_play)
            # ----------
            self.arr_iplay.append(iCnt_play)
            self.arr_maxscore.append(maxscore)
            self.arr_maxturn.append(maxturn)
            self.arr_actorloss.append(actor_loss)
            self.arr_criticloss.append(value_loss)
            # 距離の保管(プレイベース)
            self.arr_maxTD.append(np.max(self.arr_Tdist))        # Total距離の最大値
            self.arr_sminRTD.append(np.max(self.arr_sminRT))       # 空間minRT距離の最大値
            self.arr_tlminRTD.append(np.max(self.arr_tlminRT))      # タイムラインminRT距離の最大値

        # ---------------------------
        # 学習履歴の出力
        fig = plt.figure(figsize=(12, 6))
        ax1 = fig.add_subplot(1, 2, 1)
        ax1.plot(self.arr_iplay, self.arr_actorloss)
        ax1.set_xlabel('#episode')
        ax1.set_ylabel('actor_loss')
        # -----
        ax2 = fig.add_subplot(1, 2, 2)
        ax2.plot(self.arr_iplay, self.arr_criticloss)
        ax2.set_xlabel('#episode')
        ax2.set_ylabel('critic_loss')
        # -----
        fig.tight_layout()
        #fig.savefig("./REINFORCE_img.png")
        plt.show()
        
        # ---------------------------
        # 学習結果のグラフ化
        self.show_graph()

    # ----------------
    # 学習結果のグラフ化
    def show_graph(self):

        # ------
        # 学習履歴の出力
        fig2 = plt.figure(figsize=(12, 9))
        ax1 = fig2.add_subplot(2, 2, 1)
        ax1.plot(self.arr_iplay, self.arr_maxscore)
        ax1.set_xlabel('#episode')
        ax1.set_ylabel('score')
        ax1.set_ylim([-10000, 10000])
        # -----
        ax2 = fig2.add_subplot(2, 2, 2)
        ax2.set_title('learning transition : maxTD, sminRTD distance')
        ax2.set_xlabel('#episode')
        ax2.set_ylabel('distance')
        ax2.grid(True)
        ax2.scatter(self.arr_iplay, self.arr_maxTD, label="maxTD", color="blue")
        ax2.scatter(self.arr_iplay, self.arr_sminRTD, label="sminRTD", color="red")
        ax2.legend(loc='best')
        # -----
        ax3 = fig2.add_subplot(2, 2, 3)
        ax3.plot(self.arr_iplay, self.arr_maxturn)
        ax3.set_xlabel('#episode')
        ax3.set_ylabel('turn')
        ax3.set_ylim([0, 1000])
        # -----
        ax4 = fig2.add_subplot(2, 2, 4)
        ax4.set_title('learning transition : tlminRTD distance')
        ax4.set_xlabel('#episode')
        ax4.set_ylabel('distance')
        ax4.grid(True)
        ax4.scatter(self.arr_iplay, self.arr_tlminRTD, label="tlminRTD", color="blue")
        ax4.legend(loc='best')
        # -----
        fig2.tight_layout()
        #fig2.savefig("./REINFORCE_img.png")
        plt.show()

    # ----------------
    # DRL学習する
    def train(self, iCnt_play):

        # ROLL_OUT(ゲームのバッチ実行)
        states, actions, rewards, final_r, maxturn, maxscore = self.roll_out(iCnt_play)
        actions_var = Variable(torch.Tensor(actions))
        states_var = Variable(torch.Tensor(states).view(-1,STATE_DIM))

        # train actor network
        self.actor_network_optim.zero_grad()
        log_softmax_actions = self.actor_network(states_var)
        vs = self.value_network(states_var).detach()
        # calculate qs
        qs = Variable(torch.Tensor(self.discount_reward(rewards,0.98,final_r)))

        advantages = qs - vs
        self.actor_network_loss = - torch.mean(torch.sum(log_softmax_actions*actions_var,1)* ad-vantages)
        self.actor_network_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.actor_network.parameters(),0.5)
        self.actor_network_optim.step()

        # train value network
        self.value_network_optim.zero_grad()
        target_values = qs
        values = self.value_network(states_var)
        criterion = nn.MSELoss()
        self.value_network_loss = criterion(values,target_values)
        self.value_network_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.value_network.parameters(),0.5)
        self.value_network_optim.step()
        
        # loss amount
        value_loss = round(self.value_network_loss.item(), 3)
        actor_loss = round(self.actor_network_loss.item(), 3)
        
        # ----------
        # 学習結果の出力(2)
        if iCnt_play%5 ==0:
            print("iCnt_play:{}, value_network_loss:{}, actor_network_loss:{}, maxturn:{}, maxscore:{}".format(iCnt_play, value_loss, actor_loss, maxturn, maxscore))       

        return maxturn, maxscore, value_loss, actor_loss
    
    # ゲームを実行する
    def roll_out(self, iCnt_play):

        states = []
        actions = []
        rewards = []
        # ----------
        # リストの初期化
        self.arr_Tdist     = []    # Total距離
        self.arr_sminRT    = []    # 空間minRT距離
        self.arr_tlminRT   = []    # タイムラインminRT距離
        # ----------
        # ゲームの初期化
        state_chess = self.start_point
        state = calc_address(state_chess[0], state_chess[1])
        # 移動稼働なオプション
        movables = maze_field.get_actions(state_chess)
        # ----------
        sum_bump = 0
        score = 0
        is_done = False
        final_r = 0
        for iCnt_turn in range(SAMPLE_NUMS):
            states.append(state)
            log_softmax_action = self.actor_network(Variable(torch.Tensor([state])))
            softmax_action = torch.exp(log_softmax_action)
            action = np.random.choice(ACTION_DIM,p=softmax_action.cpu().data.numpy()[0])
            # ----------------
            # 強制的に最適ルートを入力する
            if iCnt_play == 0 or iCnt_play == 2 or iCnt_play == 5 or iCnt_play == 8 or iCnt_play == 12 or iCnt_play == 15:
                if iCnt_turn <= len(mx_route) - 1:
                    action = ref_action[iCnt_turn]
            # ----------
            # ONTHOT化
            one_hot_action = [int(k == action) for k in range(ACTION_DIM)]
            action_chess = maze_field.nxtPosition(state_chess, action)
            # 次の報酬を得る（#に当たったら、STUCKPOINTを生成する）
            next_state_chess, reward, done, num_bump = maze_field.get_val(action_chess, state_chess, movables)
            # 状態の表記を変更(ONEHOTプラス距離)
            next_state = calc_address(next_state_chess[0], next_state_chess[1])
            if next_state_chess[0] == 9 and next_state_chess[1] == 7:
                done = True
                reward = 8000 - 5*iCnt_turn - 100*sum_bump
            # ----------
            # 移動稼働なオプション
            next_movables = maze_field.get_actions(next_state_chess)
            # ----------
            # 距離の計算
            T_Dist  = state[101] + state[102]
            dist_sminRT  = state[100]
            dist_tlminRT = timeline_minRT(state_chess[0], state_chess[1], iCnt_turn)
            # ----------
            self.arr_Tdist.append(T_Dist-8.0)       # Total距離
            self.arr_sminRT.append(dist_sminRT)    # 空間minRT距離
            self.arr_tlminRT.append(dist_tlminRT)    # タイムラインminRT距離
            # ----------
            # [重要]フィードバック制御なし
            #reward = reward
            # [重要]フィードバック制御あり
            reward = reward - 10 * dist_sminRT
            # ----------
            # リストへの追加
            actions.append(one_hot_action)
            rewards.append(reward)
            final_state = next_state
            state_chess = next_state_chess
            state = next_state
            movables = next_movables
            score += reward
            sum_bump += num_bump
            # ----------
            # 学習結果の出力(1)
            #print("iCnt_play:{}, iCnt_turn:{}, state_chess:{}, action, reward:{}, done:{}".format(iCnt_play, iCnt_turn, state_chess, action, reward, done))       
            if done == True:
                is_done = True
                break
        # ----------
        if is_done == False:
            final_r = self.value_network(Variable(torch.Tensor([final_state]))).data.numpy()
        # ----------
        # 学習統計
        maxturn  = len(rewards)
        maxscore = np.sum(rewards)

        return states, actions, rewards, final_r, maxturn, maxscore

    def discount_reward(self, r, gamma, final_r):
        discounted_r = np.zeros_like(r)
        running_add = final_r
        for t in reversed(range(0, len(r))):
            running_add = running_add * gamma + r[t]
            discounted_r[t] = running_add
        return discounted_r

if __name__ == '__main__':

    # ----------
    # Hyper Parameters
    STATE_DIM       = 103
    ACTION_DIM      = 4
    EPISODE_STEP    = 2000
    SAMPLE_NUMS     = 8000

    # ----------
    # 強化学習を実行する
    Agent()

```

QEU:FOUNDER ： “じゃあ、本プログラムの実行結果をみてみましょう。まずはフィードバック制御なしの場合から・・・。”

**（学習曲線）**

![imageRL4-12-6](https://reinforce.github.io/images/imageRL4-12-6.jpg) 

**（パフォーマンス推移）**

![imageRL4-12-7](https://reinforce.github.io/images/imageRL4-12-7.jpg) 

D先生 ： “なんだ・・・。フィードバック制御がなくても、うまく学習収束しているじゃないですか・・・。でも、我々が設定した最適ルートじゃない経路で、学習が収束してきているようですね。”

QEU:FOUNDER ： “次に**SminRT距離によるフィードバック制御**を付け加えましょう。”

**（学習曲線）**

![imageRL4-12-8](https://reinforce.github.io/images/imageRL4-12-8.jpg) 

**（パフォーマンス推移）**

![imageRL4-12-9](https://reinforce.github.io/images/imageRL4-12-9.jpg) 

D先生 ： “最適経路からの距離の値がゼロに近づいてきました。やはり、A3Cでも効果が認められます。よ～し、終わった！！我々の次のプロジェクトはいよいよ3Dメイズになります。(3Dメイズは)DQNを使うんですか？それともA3Cを使うんですか？”

QEU:FOUNDER ： “D先生は、どちらがいいと思う？”

D先生 ： “DQNの場合には、DL関数がシンプルになるので学習が楽だといえます。状態（STATE）も、わざわざONEHOTにしなくても良い可能性が高いです。一方、A3Cの場合には強制学習が有効になりますから非常に収束が早くなる可能性があります。”

QEU:FOUNDER ： “パワフルなPC上でA3Cを使って学習したあと、量産時にREINFORCEに乗せ換えることができるかなあ・・・。前回やったように・・・。”

![imageRL4-12-10](https://reinforce.github.io/images/imageRL4-12-10.jpg) 

D先生 ： “A3CでもValue側のロジックを完全に切り離すことはできるんですか？”

QEU:FOUNDER ： “「完全に」は無理でしょう・・・。でも、「Value側を学習させなければいい」と考えればやれないことはないです。”

D先生 ： “なるほど・・・。だから、FOUNDERは今回のコードが気に入ったわけですね。”

QEU:FOUNDER ： “このコードでは、Actor側とValue側を切り離しやすいでしょ (笑)？”

## ～　まとめ　～

C部長 ： “2Dメイズのシリーズはこれで終わりですか？”

QEU:FOUNDER ： “これで、やっとめでたく終わりです。次は3Dメイズ（迷路）です。自動搬送技術に進みます。”

![imageRL4-12-11](https://reinforce.github.io/images/imageRL4-12-11.jpg) 

D先生 : “前回の話に戻ってすいません。なぜ、FOUNDERはあの人たち（↑）の考え方から離れたんですか？昔、ファンだったことを知っていたものですから、ちょっと不思議で・・・。”

![imageRL4-12-12](https://reinforce.github.io/images/imageRL4-12-12.jpg) 

QEU:FOUNDER ： “あの人たちは「お金の話」ばかり言い過ぎです。イヤな「土のニオイ」がしますわ・・・。あんな議論ばかりすると、「本当に重要なこと（↓）」の議論をすることができません。”

![imageRL4-12-13](https://reinforce.github.io/images/imageRL4-12-13.jpg) 

D先生 : “つまり**「生産力」**の話ですね。”

QEU:FOUNDER ： “**生産力強化政策と貨幣政策を組み合わせて、初めて意味がある**んです。この人（↓）のように・・・。”

![imageRL4-12-14](https://reinforce.github.io/images/imageRL4-12-14.jpg) 

QEU:FOUNDER ： “すなわち、イケメン（↓）のようにね・・・。右側の・・・。”

![imageRL4-12-15](https://reinforce.github.io/images/imageRL4-12-15.jpg) 

D先生 : “あのイケメンは、昔から敢えてMMTって言葉を使いませんから・・・。”

QEU:FOUNDER ： “結局、やるべきことは昭和の経済政策と同じでしょ？「ＭＭＴが正しい（ワァワァ）！！」って言う人って、自分のやりたいことをやったあとに増税するだけでしょう。優先政策を議論せずに闇雲にMMTを支持することって、「サギにのっかる」ようなものです。”

