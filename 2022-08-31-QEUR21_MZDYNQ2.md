## QEUR21_MZDYNQ2:　モデルとは何モノぞ！？

## ～　「model-based」を買いかぶりするな・・・　～

QEU:FOUNDER ; “Dyna-Qをやってみました。どんな感じですか？”

![imageRL7-3-1](https://reinforce.github.io/images/imageRL7-3-1.jpg) 

D先生 ; “「あ～あ・・・」って感じです。以前、model-basedに関するWeb文章を読んでいたんですが、「強化学習が実用ベースにならないのは、model-basedがうまく行っていないから」といっていました。”

QEU:FOUNDER ; “元の文章を後で紹介してくれない？・・・でも、その意味はよくわかりますよ。いかにDX（デジタルツイン）でコストは低いとはいえ、すべての学習を経験ベース(model-free)でやるのはちょっとおかしいよね。なんにでも、「ノウハウ」というのがあるもんだし・・・。”

![imageRL7-3-2](https://reinforce.github.io/images/imageRL7-3-2.jpg) 

QEU:FOUNDER ; “ゴルフでいうとこういう感じ（↑）でね・・・。”

D先生 : “そのノウハウを数値化する能力があるのが**「minRT距離」**でしょ？ゴルフのスイングでいえば、手の動き、腰の捩じり方、足の動きのそれぞれにminRT距離を使えばいいんだと思います。・・・でも、我々は前回のトライアルで、なぜうまく行かなかったんでしょうね？”

**(今回のプロジェクトにおけるモデルとは)**
- **同じ問題を解くための、Qテーブル解法とDQN解法がある**
- **Goal距離とminRT距離で、XY座標の代替が可能である**
- **Goal距離とminRT距離を使えば、最短ルート（移動命令群）の計算が可能である**

QEU:FOUNDER ; “2つあると思います。モデルとはQ値であることを理解していなかった。ボルツマン選択を使う前回のやり方は良くないですよ。・・・方向が逆です。あと、より重要なことは、minRT距離の使い方に失敗したこと・・・。**「符号付」にすべき**だった・・・。”

D先生 : “ここのところは、ちょっと良くわからないですね。”

QEU:FOUNDER ; “じゃあ、これから実験しましょう・・・。プログラムの後半をドン！！省略した前半のコードは、前回とほぼ同じです。”

```python
 # ----------------
# メイズ(MAZE)ゲームの強化学習システム
# step2A : 3Dメイズの再度準備段階です
# step2A : Table2_3DMazeQynaQ_pyTorch(DQN_size27).py
# step2A : TABLEでDyna-Qをやってみます（予備実験） -> さらなる解析へ
# ---------------- 
# 数値計算のﾗｲﾌﾞﾗﾘ読み込み
import numpy as np
import pandas as pd
import math, copy, random, time
from collections import defaultdict, deque, Counter
from scipy.special import softmax
from IPython.display import clear_output
# ---------------- 
import seaborn as sns
import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = (16, 10)

# =================================================
# Game board difinition 
# =================================================
foldername  = "./MAZE_test/"

# =================================================
# difinition of STATE CALCULATION function
# =================================================
# 空間用minRTメトリックスを計算する
def space_minRT(pos_route, a_row, a_col):
    len_route = len(pos_route)
    arr_sminRT = []
    for i in range(len_route):
        pos_rt  = pos_route[i]
        sqEuc   = (a_row - pos_rt[0])**2 + (a_col - pos_rt[1])**2
        distEuc = np.sqrt(sqEuc)
        arr_sminRT.append(round(distEuc,5))   
    dist_sminRT = np.min(arr_sminRT)
        
    return dist_sminRT

# 状態の表記を変更(XY座標へ)、空間用minRTメトリックスつき
def calc_address(a_row, a_col, mx_lm):

    add_state = np.zeros(4)
    for i in range(len(mx_lm)):
        pos_lm  = mx_lm[i]
        sqEuc   = (a_row - pos_lm[0])**2 + (a_col - pos_lm[1])**2
        distEuc = np.sqrt(sqEuc)
        add_state[i] = round(distEuc,5)
    # --------------------------------------------------
    # 空間用minRTメトリックスを計算する
    # Aルート(標準)
    dist_sminRTA = space_minRT(mx_route, a_row, a_col)
    dist_sminRTA = round(dist_sminRTA,5)
    # Bルート(標準との差異分)
    dist_sminRTB = space_minRT(mx_routeB, a_row, a_col)
    dist_sminRTB = round(dist_sminRTB,5) - dist_sminRTA
    # Cルート(標準との差異分)
    dist_sminRTC = space_minRT(mx_routeC, a_row, a_col)
    dist_sminRTC = round(dist_sminRTC,5) - dist_sminRTA
    # Aルート修正
    if add_state[2] > add_state[3]:
        dist_sminRTA = -1 * dist_sminRTA
    # --------------------------------------------------
    # 距離とりまとめリストを生成する
    arr_RTDists = np.zeros(4)
    arr_RTDists[0] = add_state[1]    # Goal距離
    arr_RTDists[1] = dist_sminRTA    # 空間minRT距離
    arr_RTDists[2] = dist_sminRTB    # 空間用minRT距離(B)
    arr_RTDists[3] = dist_sminRTC    # 空間用minRT距離(C)
    # --------------------------------------------------
    # STATEを生成する
    state = np.hstack([[a_row, a_col], [add_state[0], add_state[1]], [dist_sminRTA, dist_sminRTB, dist_sminRTC]])
    
    return state, arr_RTDists
    
# ・・・　省略　・・・

# ---------------------------
# パラメタ(その３)
num_episodes    = 300
num_turns       = 25000
n_planning      = 10
# ------
# 実行
agent = Agent()
# Qテーブルを呼び出す
#print("--- agent.QQL_table ---")
#print(agent.QQL_table)
# ------
# Initialize and Generate a maze
Q_mean = np.zeros([size, size])
Q_max  = np.zeros([size, size])
for i in range(size):
    for j in range(size):
        arr_QV = agent.QQL_table[(i,j)]
        Q_mean[i,j] = round(np.mean(arr_QV),3)
        Q_max[i,j]  = round(np.max(arr_QV),3)    
#print("--- Q_mean ---")
#print(Q_mean)     
# ------
# HEAT MAPを呼び出す
# 平均
plt.figure(figsize=(17, 10))
plt.title('Q_Value(MEAN)')
sns.heatmap(Q_mean, annot=True, cbar=False, cmap='Blues', fmt='.1f')
plt.show()
# ------
# 最大
plt.figure(figsize=(17, 10))
plt.title('Q_Value(MAX)')
sns.heatmap(Q_max, annot=True, cbar=False, cmap='hot', fmt='.1f')
plt.show()
# ------
# [参考]差分を見る
Q_diff = Q_max - Q_mean
plt.figure(figsize=(17, 10))
plt.title('Q_Value(DIFF)')
sns.heatmap(Q_diff, annot=True, cbar=True, cmap='hot', fmt='.1f')
plt.show()

# ------
# 座標変換をして、Q値の分布を把握してみる
arr_iRow = []
arr_jCol = []
arr_Goal = []
arr_minRT = []
arr_Ymean = []
arr_Yman  = []
# ------
for i in range(size):
    for j in range(size):
        # 状態の表記を変更(XY座標へ)、空間用minRTメトリックスつき
        state, arr_RTDists = calc_address(i, j, mx_lm)
        goal_Dist   = arr_RTDists[0]
        minRT_Dist  = arr_RTDists[1]
        y_mean     = Q_mean[i,j]
        y_max      = Q_max[i,j]
        # -----
        # リストに追加する
        arr_iRow.append(i)
        arr_jCol.append(j)
        arr_Goal.append(goal_Dist)
        arr_minRT.append(minRT_Dist)
        arr_Ymean.append(y_mean)
        arr_Yman.append(y_max)

# ------
# XY(i,j)座標系
fig3 = plt.figure(figsize=(14, 9))
ax3 = fig3.add_subplot(projection='3d')
ax3.set_title('X(Col)-Y(Row) Coodinate', size = 20)
ax3.set_xlabel("Row", size = 14)
ax3.set_ylabel("Col", size = 14)
ax3.set_zlabel("QV", size = 14)
ax3.scatter(arr_iRow, arr_jCol, arr_Ymean, color="blue")
ax3.scatter(arr_iRow, arr_jCol, arr_Yman, color="red")
plt.show()

# ------
# Goal-minRT座標系
fig4 = plt.figure(figsize=(14, 9))
ax4 = fig4.add_subplot(projection='3d')
ax4.set_title('Goal-minRT Coodinate', size = 20)
ax4.set_xlabel("Goal", size = 14)
ax4.set_ylabel("minRT", size = 14)
ax4.set_zlabel("QV", size = 14)
ax4.scatter(arr_Goal, arr_minRT, arr_Ymean, color="blue")
ax4.scatter(arr_Goal, arr_minRT, arr_Yman, color="red")
plt.show()

```

QEU:FOUNDER ; “このプログラムの目標は、主にX-Y座標系とGoal-minRT座標系の比較にあります。まずは前者によるQ値の分布を見てみましょう。”

![imageRL7-3-3](https://reinforce.github.io/images/imageRL7-3-3.jpg) 

D部長: “3次元散布図にしてくれたんですか・・・。前回はヒートマップで表示されていたので理解できます。”

![imageRL7-3-4](https://reinforce.github.io/images/imageRL7-3-4.jpg) 

QEU:FOUNDER ; “3D散布図において、赤プロットがmax値で、青はmean値です。今回は差異をヒートマップにしてみました。・・・意外だよね。Q値が高い軌道（標準ルート）上の値の差異はないんです。つぎは、いよいよ後者のGoal距離と「符号付」minRT距離でプロットした3D散布図だよ！”

![imageRL7-3-5](https://reinforce.github.io/images/imageRL7-3-5.jpg) 

D先生 : “いやぁ！これはキレイだ！！符号化されたので、この図が描けるのか！”

QEU:FOUNDER ; “実は、以前も「符号化なしのminRT距離」でディープラーニング関数を簡単化できないかを試して、見事に失敗していました。今から思えば、できるわけないよね・・・（笑）。今回の符号化で、初めてモデル化ができたんです。”

D先生 : “**「model-basedを買いかぶるな」**と・・・。”

QEU:FOUNDER ; “モデルとは、つまるところは関数の適切な表現方法じゃないかな？「model-based」には決まった手法があるわけではなく、テーマによって都度変化する**feature-engineeringの一種**です。”


## ～ まとめ ～
 
 
QEU:FOUNDER ： “この人のTwitterって、面白いと思わない？”
 
![imageRL7-3-6](https://reinforce.github.io/images/imageRL7-3-6.jpg) 


D先生 : “このtwitterって便利ですよね。統計が主なのですが、役に立つ知識がフリップとして紹介されたり・・・。弁理士がAIに代替される可能性92％というのは、よくイメージがわきます。AIがらみだと、こういうものが紹介されていましたよ。”

![imageRL7-3-7](https://reinforce.github.io/images/imageRL7-3-7.jpg) 
 
QEU:FOUNDER ： “理系大学生の立場は・・・（爆）。**引用元は牛津か・・・**。2030年で、いくらなんでもここまでいけるかねぇ・・・。”
 
 
C部長: “そうかと思えば、意外なデータを発見しました。”

![imageRL7-3-8](https://reinforce.github.io/images/imageRL7-3-8.jpg) 

QEU:FOUNDER ; “いやぁ、まだまだ・・・。”

![imageRL7-3-9](https://reinforce.github.io/images/imageRL7-3-9.jpg) 

D先生: “あとでアカウントを登録します。”

