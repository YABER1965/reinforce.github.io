## QEUR21_2DMZA4:　補足　～　ゴミ箱(BUMP)を設置する

## ～  異次元の・・・ ～

D先生 : “あれ？もう3次元メイズに移行したと思っていました・・・。”

QEU:FOUNDER ： “「とある宿題」があるので、2次元メイズの中で基本的な部分を完結しておきたい。A3Cの採用で**ε-Greedyなし**で学習が収束するメドがついたが、状態(STATE)としてONEHOTを使いたくないんです。例えば、今回の10ｘ10のメイズ（↓）の場合、ＯＮＥＨＯＴだと100次元になるよ・・・。もし、3次元メイズで30ｘ30にしたいとなると・・・？”

![imageRL4-13-1](https://reinforce.github.io/images/imageRL4-13-1.jpg) 

D先生 : “30 x 30 = 900次元ですが、実際のところはもっと細かくなるでしょうね。やっぱり、ランドマーク距離（↑）を計測するような方法で数個のメトリックスにまとめておきたいですね。”

QEU:FOUNDER ： “今回はそのための第一歩として「ＤＬ関数にゴミ箱を設置」します。”

D先生 : “は？なんと・・・。ゴミ箱ですか？”

![imageRL4-13-2](https://reinforce.github.io/images/imageRL4-13-2.jpg) 

QEU:FOUNDER ： “いままでは壁に接触したとき（bump=1）のときも、壁に接触せずにうまく通過できたとき（bump=0）も同じDL関数に情報を投入して学習していました。これが学習がいまいちうまく行かなかった原因だと思うんです。そこで、DL関数の中にbumpの項目を追加すると分離ができるんじゃないでしょうか。”

![imageRL4-13-3](https://reinforce.github.io/images/imageRL4-13-3.jpg) 

D先生 : “なるほどね。えっと・・・、前回の学習結果は・・・。それなりに、うまく行っていますね。”

**（学習曲線）**

![imageRL4-13-4](https://reinforce.github.io/images/imageRL4-13-4.jpg) 

 **（パフォーマンス推移）**

![imageRL4-13-5](https://reinforce.github.io/images/imageRL4-13-5.jpg) 

QEU:FOUNDER ： “かなりうまく行っていますが、もうちょっと最後の方で安定してほしいよね。それではプログラムをドン！！強調するが、**これはSTATEをONEHOTにしている**よ、テスト段階だからね・・・。”


```python
# ----------------
# メイズ(MAZE)ゲームの強化学習システム
# stepD2 : STATEをONEHOTと動的メトリックスで構成する
# stepD2 : reinforce_maze_pyTorch(bump-A3C-onehot-feedback).py
# stepD2 : Bump項をSTATEに追加して、学習を安定化させました
# ---------------- 
# 数値計算のﾗｲﾌﾞﾗﾘ読み込み
import numpy as np
import pandas as pd
import math, copy, random, time
# -----
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable
# -----
import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = (16, 10)

# =================================================
# Game board difinition 
# =================================================
foldername  = "./MAZE_test/"
init_maze = [['#', 'S', '#', '#', '#', '#', '#', '#', '#', '#'], ['#', 0, '#', -1, 0, -1, 0, '#', '#', '#'], ['#', -1, '#', -1, -1, 0, -1, -1, 0, '#'], ['#', 0, -1, 0, 0, -1, 0, 0, '#', '#'], ['#', '#', -1, 0, 0, -1, '#', -1, 0, '#'], ['#', -1, 0, 0, -1, -1, 0, '#', -1, '#'], ['#', 0, 0, 0, -1, '#', -1, '#', -1, '#'], ['#', 0, -1, 0, -1, 0, -1, 0, -1, '#'], ['#', 0, -1, 0, 0, -1, 0, 0, -1, '#'], ['#', '#', '#', '#', '#', '#', '#', 50, '#', '#']]
mx_lm2 = [[0, 1],[9, 7]]

# --------------------------------------------------
# MAZE盤面をCSVファイルに保存する
def save_csvMAZE(maze): 
    # --------------------------------------------------
    # CSV出力用のデータフレームを作る(2)
    arr_columns = list(range(10))
    # -----
    df_csvout = pd.DataFrame(maze, columns=arr_columns)
    #print(df_csvout)
    
    # --------------------------------------------------
    # CSV ファイル (file_csvout) として出力
    code_csvout = "maze_board.csv"       # file name  
    file_csvout = foldername + code_csvout   # standard(training) file name   
    print("メトリックス保管用CSVファイル ：{0}".format(file_csvout))
    # -----
    df_csvout.to_csv(file_csvout, index=True)

# MAZE盤面をCSVファイルに保存する
#save_csvMAZE(init_maze)

# =================================================
# difinition of STATE CALCULATION function
# =================================================
# タイムライン用minRTメトリックスを計算する
def timeline_minRT(a_row, a_col, iCnt_turn):
    len_route = len(mx_route)
    if iCnt_turn <= len_route - 1:
        pos_rt  = mx_route[iCnt_turn]
    else:
        pos_rt  = mx_route[len_route - 1]
    # -----
    tlEuc   = (a_row - pos_rt[0])**2 + (a_col - pos_rt[1])**2
    dist_tlminRT = np.sqrt(tlEuc)
        
    return round(dist_tlminRT,5)

# -------
# 空間用minRTメトリックスを計算する
def space_minRT(a_row, a_col):
    len_route = len(mx_route)
    arr_sminRT = []
    for i in range(len_route):
        pos_rt  = mx_route[i]
        sqEuc   = (a_row - pos_rt[0])**2 + (a_col - pos_rt[1])**2
        distEuc = np.sqrt(sqEuc)
        arr_sminRT.append(round(distEuc,5))   
    dist_sminRT = np.min(arr_sminRT)
        
    return dist_sminRT

# -------
# 状態の表記を変更(STATE-6_landmarkへ)、空間用minRTメトリックスつき
def calc_address(a_row, a_col):

    temp_state = np.zeros([size, size])
    temp_state[a_row, a_col] = 1.0
    onehot_state = temp_state.flatten()
    
    add_state  = np.zeros(2)
    for i in range(2):
        pos_lm  = mx_lm2[i]
        sqEuc   = (a_row - pos_lm[0])**2 + (a_col - pos_lm[1])**2
        distEuc = np.sqrt(sqEuc)
        add_state[i] = round(distEuc,5)
             
    # 空間用minRTメトリックスを計算する
    dist_sminRT  = space_minRT(a_row, a_col)

    state = np.hstack([onehot_state, [dist_sminRT], add_state])

    return state

# ----------
# Maze Class
class Maze(object):
    def __init__(self, size=10, blocks_rate=0.1):
        self.size = size if size > 3 else 10
        self.blocks = int((size ** 2) * blocks_rate) 
        self.s_list = []
        self.maze_list = []
        self.e_list = []

    def create_mid_lines(self, k):
        if k == 0: self.maze_list.append(self.s_list)
        elif k == self.size - 1: self.maze_list.append(self.e_list)
        else:
            tmp_list = []
            for l in range(0,self.size):
                if l == 0: tmp_list.extend("#")
                elif l == self.size-1: tmp_list.extend("#")
                else:
                    a = random.randint(-1, 0)
                    tmp_list.extend([a])
            self.maze_list.append(tmp_list)

    def insert_blocks(self, k, s_r, e_r):
        b_y = random.randint(1, self.size-2)
        b_x = random.randint(1, self.size-2)
        if [b_y, b_x] == [1, s_r] or [b_y, b_x] == [self.size - 2, e_r]: k = k-1
        else: self.maze_list[b_y][b_x] = "#"
            
    def generate_maze(self): 
        s_r = random.randint(1, int((self.size / 2)) - 1)
        for i in range(0, self.size):
            if i == s_r: self.s_list.extend("S")
            else: self.s_list.extend("#")
        start_point = [0, s_r]

        e_r = random.randint(int((self.size / 2)) + 1, self.size - 2)
        for j in range(0, self.size):
            if j == e_r: self.e_list.extend([50])
            else: self.e_list.extend("#")
        goal_point = [self.size - 1, e_r]

        for k in range(0, self.size):
            self.create_mid_lines(k)
        
        for k in range(self.blocks):
            self.insert_blocks(k, s_r, e_r)

        return self.maze_list, start_point, goal_point

    # --------------------------------------------------
    # 盤面のCSVファイルを読み込み表示する
    def read_boardfile(self):
        # CSVファイルの読み込み
        code_csvout = "maze_board.csv"       # file name
        file_readcsv = foldername + code_csvout   # standard(training) file name  
        df = pd.read_csv(file_readcsv)
        #max_play = len(df)
        # print("データ量 max_play",max_play)
        # print(df)
        # --------------------------------------------------
        # 選択項目の読み込み
        maze = df.loc[:, "0":"9"].values.tolist()
        start_point = [0,0]
        goal_point  = [0,0]
        for i in range(size):
            for j in range(size):
                if maze[i][j] == 'S':
                    start_point = [i, j]
                if maze[i][j] == '50':
                    goal_point  = [i, j]

        self.maze_list   = maze
        self.start_point = start_point
        self.goal_point  = goal_point
        #print("----- mx_maze(read_boardfile) -----")
        #print(mx_maze)

        return self.maze_list, start_point, goal_point

    # --------------------------------------------------
    # コマの最短ルートのCSVファイルを読み込み表示する
    def read_chessfile(self):
        # CSVファイルの読み込み
        code_csvout = "chess_route.csv"       # file name
        file_readcsv = foldername + code_csvout   # standard(training) file name  
        df = pd.read_csv(file_readcsv)
        #max_play = len(df)
        # print("データ量 max_play",max_play)
        # print(df)
        # --------------------------------------------------
        # 選択項目の読み込み
        mx_route    = df.loc[:, "row":"col"].values.tolist()
        ref_action  = df.loc[:, "action"].values.tolist()
        print("ref_action: ", ref_action)
        #print("----- route -----")
        #print(route)

        return mx_route, ref_action

# ----------
# Maze functions
class Field(object):
    def __init__(self, maze, start_point, goal_point):
        self.maze = maze
        self.start_point = start_point
        self.goal_point  = goal_point
        self.movable_vec = [[-1,0],[1,0],[0,-1],[0,1]]

    def display(self, point=None):
        field_data = copy.deepcopy(self.maze)
        if not point is None:
                y, x = point
                field_data[y][x] = "@@"
        else:
                point = ""
        print("================")
        for line in field_data:
                print ("\t" + "%3s " * len(line) % tuple(line))
        print("================")

    def all_display(self, mx_route):
        field_data = copy.deepcopy(self.maze)
        for i in range(len(mx_route)):
            point = mx_route[i]
            y, x = point[0], point[1]
            field_data[y][x] = "@@"
        print("================")
        for line in field_data:
                print ("\t" + "%3s " * len(line) % tuple(line))
        print("================")

    def get_actions(self, state):
        movables = []
        if state == self.start_point:
            y = state[0] + 1
            x = state[1]
            a = [[y, x]]
            return a
        else:
            for v in self.movable_vec:
                y = state[0] + v[0]
                x = state[1] + v[1]
                if not(0 < x < len(self.maze) and
                       0 <= y <= len(self.maze) - 1 and
                       maze[y][x] != "#" and
                       maze[y][x] != "S"):
                    continue
                movables.append([y,x])
            if len(movables) != 0:
                return movables
            else:
                return None

    # --------------------------------------------------
    # 次のコマの「決定」位置と報酬、ゲームフラグを決める
    # 新概念：マスク「#」位置に着くと、スタート地点に戻る
    def get_val(self, action_chess, state_chess, movables):
        yPos = action_chess[0]
        xPos = action_chess[1]
        # ----------------
        if action_chess == self.start_point:
            nxt_state = action_chess
            return nxt_state, 0, False, 0
        elif self.maze[yPos][xPos] == "#":
            #print("movables: ", movables)
            if len(movables) > 0:
                nxt_state = random.choice(movables)
            else:
                nxt_state = self.start_point
            v = -0.00        # -10
            return nxt_state, v, False, 1
        else:
            nxt_state = action_chess
            v = float(self.maze[yPos][xPos])
            if action_chess == self.goal_point: 
                return nxt_state, v, True, 0
            else: 
                return nxt_state, v, False, 0

    # --------------------------------------------------
    # 次のコマの位置を決める
    def nxtPosition(self, state_chess, action):
        # ----------------
        # 次の行動(ACTION)に進む
        # action number / row / col / 方向 / onehot
        # 0 / -1 / 0  / 上(up) / 1000
        # 1 / +1 / 0  / 下(down) / 0100
        # 2 / 0  / -1 / 左(left) / 0010
        # 3 / 0  / +1 / 右(right) / 0001
        # ----------------
        # ACTION_CHESS
        if action == 0:
            diff_row, diff_col = -1, 0
        if action == 1:
            diff_row, diff_col = 1, 0 
        if action == 2:
            diff_row, diff_col = 0, -1 
        if action == 3:
            diff_row, diff_col = 0, 1 
        # ----------------
        nxtPos    = [0,0]
        nxtPos[0] = state_chess[0] + diff_row   # y方向
        nxtPos[1] = state_chess[1] + diff_col   # x方向

        return nxtPos

# ----------
# Generate a maze
size = 10
barriar_rate = 0.1

maze_1 = Maze(size, barriar_rate)
#maze, start_point, goal_point = maze_1.generate_maze()
maze, start_point, goal_point = maze_1.read_boardfile()
maze_field = Field(maze, start_point, goal_point)
# 普通の表示
maze_field.display()

# 最短ルートを読み込み
mx_route, ref_action = maze_1.read_chessfile()
# ルートの表示(個別表示)
#for i in range(len(mx_route)):
#    maze_field.display(mx_route[i])
# ルートの表示(全表示)
maze_field.all_display(mx_route)


#=================================================
# Calculation class(1) : Actor-Network
#=================================================
class ActorNetwork(nn.Module):

    def __init__(self,input_size,hidden_size,action_size):
        super(ActorNetwork, self).__init__()
        self.fc1 = nn.Linear(input_size,hidden_size)
        self.fc2 = nn.Linear(hidden_size,hidden_size)
        self.fc3 = nn.Linear(hidden_size,action_size)

    def forward(self,x):
        out = F.relu(self.fc1(x))
        out = F.relu(self.fc2(out))
        out = F.log_softmax(self.fc3(out), dim = 1)
        return out.squeeze(-1)

class ValueNetwork(nn.Module):

    def __init__(self,input_size,hidden_size,output_size):
        super(ValueNetwork, self).__init__()
        self.fc1 = nn.Linear(input_size,hidden_size)
        self.fc2 = nn.Linear(hidden_size,hidden_size)
        self.fc3 = nn.Linear(hidden_size,output_size)

    def forward(self,x):
        out = F.relu(self.fc1(x))
        out = F.relu(self.fc2(out))
        out = self.fc3(out)
        return out.squeeze(-1)

#=================================================
# Calculation class(2) : Agent
#=================================================
class Agent():
    def __init__(self):

        # init value network
        self.value_network = ValueNetwork(input_size = STATE_DIM, hidden_size = 128, output_size = 1)
        self.value_network_optim = torch.optim.Adam(self.value_network.parameters(), lr=0.001)

        # init actor network
        self.actor_network = ActorNetwork(input_size = STATE_DIM, hidden_size = 128, action_size = ACTION_DIM)
        self.actor_network_optim = torch.optim.Adam(self.actor_network.parameters(), lr = 0.001)

        # ---------------------------
        # 盤面データを読む
        self.maze = maze
        self.start_point = start_point
        self.goal_point = goal_point
        # ----------------
        # リストの初期化
        self.arr_iplay      = []
        self.arr_maxscore   = []
        self.arr_maxturn    = []
        self.arr_actorloss  = []
        self.arr_criticloss = []
        # 距離の分析用累積リスト
        self.arr_maxTD      = []  # Total距離
        self.arr_sminRTD    = []  # 空間minRT距離
        self.arr_tlminRTD   = []  # タイムラインminRT距離
        # ----------------
        # 学習する
        for iCnt_play in range(EPISODE_STEP):
            maxturn, maxscore, value_loss, actor_loss = self.train(iCnt_play)
            # ----------
            self.arr_iplay.append(iCnt_play)
            self.arr_maxscore.append(maxscore)
            self.arr_maxturn.append(maxturn)
            self.arr_actorloss.append(actor_loss)
            self.arr_criticloss.append(value_loss)
            # 距離の保管(プレイベース)
            self.arr_maxTD.append(np.max(self.arr_Tdist))        # Total距離の最大値
            self.arr_sminRTD.append(np.max(self.arr_sminRT))       # 空間minRT距離の最大値
            self.arr_tlminRTD.append(np.max(self.arr_tlminRT))      # タイムラインminRT距離の最大値

        # ---------------------------
        # 学習履歴の出力
        fig = plt.figure(figsize=(12, 6))
        ax1 = fig.add_subplot(1, 2, 1)
        ax1.plot(self.arr_iplay, self.arr_actorloss)
        ax1.set_xlabel('#episode')
        ax1.set_ylabel('actor_loss')
        # -----
        ax2 = fig.add_subplot(1, 2, 2)
        ax2.plot(self.arr_iplay, self.arr_criticloss)
        ax2.set_xlabel('#episode')
        ax2.set_ylabel('critic_loss')
        # -----
        fig.tight_layout()
        #fig.savefig("./REINFORCE_img.png")
        plt.show()
        
        # ---------------------------
        # 学習結果のグラフ化
        self.show_graph()


    # ----------------
    # 学習結果のグラフ化
    def show_graph(self):

        # ------
        # 学習履歴の出力
        fig2 = plt.figure(figsize=(12, 9))
        ax1 = fig2.add_subplot(2, 2, 1)
        ax1.plot(self.arr_iplay, self.arr_maxscore)
        ax1.set_xlabel('#episode')
        ax1.set_ylabel('score')
        ax1.set_ylim([-10000, 10000])
        # -----
        ax2 = fig2.add_subplot(2, 2, 2)
        ax2.set_title('learning transition : maxTD, sminRTD distance')
        ax2.set_xlabel('#episode')
        ax2.set_ylabel('distance')
        ax2.grid(True)
        ax2.scatter(self.arr_iplay, self.arr_maxTD, label="maxTD", color="blue")
        ax2.scatter(self.arr_iplay, self.arr_sminRTD, label="sminRTD", color="red")
        ax2.legend(loc='best')
        # -----
        ax3 = fig2.add_subplot(2, 2, 3)
        ax3.plot(self.arr_iplay, self.arr_maxturn)
        ax3.set_xlabel('#episode')
        ax3.set_ylabel('turn')
        ax3.set_ylim([0, 1000])
        # -----
        ax4 = fig2.add_subplot(2, 2, 4)
        ax4.set_title('learning transition : tlminRTD distance')
        ax4.set_xlabel('#episode')
        ax4.set_ylabel('distance')
        ax4.grid(True)
        ax4.scatter(self.arr_iplay, self.arr_tlminRTD, label="tlminRTD", color="blue")
        ax4.legend(loc='best')
        # -----
        fig2.tight_layout()
        #fig2.savefig("./REINFORCE_img.png")
        plt.show()

    # ----------------
    # DRL学習する
    def train(self, iCnt_play):

        # ROLL_OUT(ゲームのバッチ実行)
        states, actions, rewards, final_r, maxturn, maxscore = self.roll_out(iCnt_play)
        actions_var = Variable(torch.Tensor(actions))
        states_var = Variable(torch.Tensor(states).view(-1,STATE_DIM))

        # train actor network
        self.actor_network_optim.zero_grad()
        log_softmax_actions = self.actor_network(states_var)
        vs = self.value_network(states_var).detach()
        # calculate qs
        qs = Variable(torch.Tensor(self.discount_reward(rewards,0.98,final_r)))

        advantages = qs - vs
        self.actor_network_loss = - torch.mean(torch.sum(log_softmax_actions*actions_var,1)* ad-vantages)
        self.actor_network_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.actor_network.parameters(),0.5)
        self.actor_network_optim.step()

        # train value network
        self.value_network_optim.zero_grad()
        target_values = qs
        values = self.value_network(states_var)
        criterion = nn.MSELoss()
        self.value_network_loss = criterion(values,target_values)
        self.value_network_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.value_network.parameters(),0.5)
        self.value_network_optim.step()
        
        # loss amount
        value_loss = round(self.value_network_loss.item(), 3)
        actor_loss = round(self.actor_network_loss.item(), 3)
        
        # ----------
        # 学習結果の出力(2)
        if iCnt_play%5 ==0:
            print("iCnt_play:{}, value_network_loss:{}, actor_network_loss:{}, maxturn:{}, maxscore:{}".format(iCnt_play, value_loss, actor_loss, maxturn, maxscore))       

        return maxturn, maxscore, value_loss, actor_loss
    
        
    # ゲームを実行する
    def roll_out(self, iCnt_play):

        states = []
        actions = []
        rewards = []
        # ----------
        # リストの初期化
        self.arr_Tdist     = []    # Total距離
        self.arr_sminRT    = []    # 空間minRT距離
        self.arr_tlminRT   = []    # タイムラインminRT距離
        # ----------
        # ゲームの初期化
        state_chess = self.start_point
        state = calc_address(state_chess[0], state_chess[1])
        # ----------
        # next_stateにbumpを追加する
        num_bump = 0
        state = np.hstack([state, [num_bump]])
        # 移動稼働なオプション
        movables = maze_field.get_actions(state_chess)
        # ----------
        sum_bump = 0
        score = 0
        is_done = False
        final_r = 0
        for iCnt_turn in range(SAMPLE_NUMS):
            states.append(state)
            log_softmax_action = self.actor_network(Variable(torch.Tensor([state])))
            softmax_action = torch.exp(log_softmax_action)
            action = np.random.choice(ACTION_DIM,p=softmax_action.cpu().data.numpy()[0])
            # ----------------
            # 強制的に最適ルートを入力する
            if iCnt_play == 0 or iCnt_play == 2 or iCnt_play == 5 or iCnt_play == 8 or iCnt_play == 12 or iCnt_play == 15:
                if iCnt_turn <= len(mx_route) - 1:
                    action = ref_action[iCnt_turn]
            # ----------
            # ONTHOT化
            one_hot_action = [int(k == action) for k in range(ACTION_DIM)]
            action_chess = maze_field.nxtPosition(state_chess, action)
            # 次の報酬を得る（#に当たったら、STUCKPOINTを生成する）
            next_state_chess, reward, done, num_bump = maze_field.get_val(action_chess, state_chess, movables)
            # 状態の表記を変更(ONEHOTプラス距離)
            next_state = calc_address(next_state_chess[0], next_state_chess[1])
            if next_state_chess[0] == 9 and next_state_chess[1] == 7:
                done = True
                reward = 8000 - 5*iCnt_turn - 100*sum_bump
            # ----------
            # 移動稼働なオプション
            next_movables = maze_field.get_actions(next_state_chess)
            # ----------
            # 距離の計算
            T_Dist  = state[101] + state[102]
            dist_sminRT  = state[100]
            dist_tlminRT = timeline_minRT(state_chess[0], state_chess[1], iCnt_turn)
            # ----------
            self.arr_Tdist.append(T_Dist-8.0)       # Total距離
            self.arr_sminRT.append(dist_sminRT)    # 空間minRT距離
            self.arr_tlminRT.append(dist_tlminRT)    # タイムラインminRT距離
            # [重要]フィードバック制御あり
            reward = reward - 20 * dist_sminRT
            # ----------
            # next_stateにbumpを追加する     
            next_state = np.hstack([next_state, [num_bump]])
            # ----------
            # リストへの追加
            actions.append(one_hot_action)
            rewards.append(reward)
            final_state = next_state
            state_chess = next_state_chess
            state = next_state
            movables = next_movables
            score += reward
            sum_bump += num_bump
            # ----------
            # 学習結果の出力(1)
            #print("iCnt_play:{}, iCnt_turn:{}, state_chess:{}, action, reward:{}, done:{}".format(iCnt_play, iCnt_turn, state_chess, action, reward, done))       
            if done == True:
                is_done = True
                break
        # ----------
        if is_done == False:
            final_r = self.value_network(Variable(torch.Tensor([final_state]))).data.numpy()
        # ----------
        # 学習統計
        maxturn  = len(rewards)
        maxscore = np.sum(rewards)

        return states, actions, rewards, final_r, maxturn, maxscore

    def discount_reward(self, r, gamma, final_r):
        discounted_r = np.zeros_like(r)
        running_add = final_r
        for t in reversed(range(0, len(r))):
            running_add = running_add * gamma + r[t]
            discounted_r[t] = running_add
        return discounted_r

if __name__ == '__main__':

    # ----------
    # Hyper Parameters
    STATE_DIM       = 104
    ACTION_DIM      = 4
    EPISODE_STEP    = 2000
    SAMPLE_NUMS     = 8000

    # ----------
    # 強化学習を実行する
    Agent()

```

QEU:FOUNDER ： “じゃあ、本プログラムの実行結果をみてみましょう。まずはテキスト出力から・・・。”

![imageRL4-13-6](https://reinforce.github.io/images/imageRL4-13-6.jpg) 

D先生 ： “理想的な結果に着地しているような気がしますね。”

QEU:FOUNDER ： “次にいよいよグラフの公開です。びっくりするよ・・・。”

**（学習曲線）**

![imageRL4-13-7](https://reinforce.github.io/images/imageRL4-13-7.jpg) 

**（パフォーマンス推移）**

![imageRL4-13-8](https://reinforce.github.io/images/imageRL4-13-8.jpg) 

D先生 ： “なにこれ？壊れたんですか？”

![imageRL4-13-9](https://reinforce.github.io/images/imageRL4-13-9.jpg) 

QEU:FOUNDER ： “小生も最初はそう思った・・・（笑）。成功です、コレ・・・。これが**A3C（Asynchronous Advantage Actor-Critics）の真の実力**なんでしょうね。”

D先生 ： “いままでは我々が関数をうまく設計できていなかったので、本当の力を発揮できなかったんですね。”

QEU:FOUNDER ： “ただし、これからはONEHOTによる状態（STATE）は使えないからね。”

D先生 ： “あ～アッ！！いままではONEHOTでしかうまく収束しなかったからなあ・・・？”

QEU:FOUNDER ： “次は、コレを使ってもう一度「6-Landmark」でやってみましょう。”

## ～　まとめ　～

C部長 ： “新たな段階に入ったイケメンバトル・・・。”

![imageRL4-13-10](https://reinforce.github.io/images/imageRL4-13-10.jpg) 

QEU:FOUNDER ： “こうなるとは思わなかったな・・・。”

<iframe width="560" height="315" src="https://www.youtube.com/embed/SxaH6aCf9Kg" ti-tle="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; en-crypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

D先生 : “・・・というか、多分こうなるとは言われていましたよ。住民投票を機会に方針を変えてしまえば、楽だったなのに・・・。”

<iframe width="560" height="315" src="https://www.youtube.com/embed/Izy1Va7ytbw" ti-tle="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; en-crypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

QEU:FOUNDER ： “（立場を）変えられない事情があるんでしょうね。ひょっとしたら「なんかの宗教」とかも絡むのかな？”

C部長: “野次馬的にいえば、「面白くなった」といえますね。”


