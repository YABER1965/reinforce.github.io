## QEUR21_MZPROT10: 一応、走行ルートを学習させてみた(カリキュラム学習-第2段階)

## ～　グダグダはつづく・・・　～

QEU:FOUNDER ; “トライアル段階が終わり、メイズ（迷路）の強化学習の本番を始めています。前回はスタートからすぐの段階を学習しました。もちろん、そこ「まで」はカンタンに学習できるわけで・・・。”

![imageRL6-11-1](https://reinforce.github.io/images/imageRL6-11-1.jpg) 

D先生 : “**バグ修正、パラメータ調整しながら、グダグダやる段階**に移行しました・・・。”

QEU：FOUNDER ; “ちなみに、少しだけコードを見直しました、Pythonコードをドン！！”

```python
# ----------------
# メイズ(MAZE)ゲームの強化学習システム
# step11 : [3D]RayCastと強化学習の併用
# PROGRAM NAME : Reinforcement_RunA_RayCast_Maze.py
# step11 : カメラ画像の方向は、2DMazeの学習結果で決めています。量子化用の対応がされています
# 注意： 簡単な強化学習をします。実行版A段階です(RUN_A)
# ----------------
import cv2
import numpy as np
import pandas as pd
import math, copy, random, time, sys, os
from collections import deque, Counter
from scipy.special import softmax
from IPython.display import clear_output
# ----------------
import torch
import torch.nn.functional
import torch.utils.data
# ----------------
from numba import njit
from numba.typed import List
import pygame
#from PIL import Image, ImageDraw
#from PIL import ImageFont
# ---------------- 
import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = (16, 10)

# =================================================
# Game board difinition 
# =================================================
foldername  = "./MAZE_test/"

# =================================================
# difinition of STATE CALCULATION function
# =================================================
# 空間用minRTメトリックスを計算する
def space_minRT(pos_route, a_row, a_col):
    len_route = len(pos_route)
    arr_sminRT = []
    for i in range(len_route):
        pos_rt  = pos_route[i]
        sqEuc   = (a_row - pos_rt[0])**2 + (a_col - pos_rt[1])**2
        distEuc = np.sqrt(sqEuc)
        arr_sminRT.append(round(distEuc,5))   
    dist_sminRT = np.min(arr_sminRT)
        
    return dist_sminRT


#=================================================
# [3D-Map]BOLTZMAN FUNCTION AND GET ACTION         
#=================================================
# ボルツマン選択による出現確率の調整
def calc_Boltzman(MazeXpos, MazeYpos, movablesMap, pos_lm):

    CTemp, arr_Gdists, arr_sminRTA = 600.0, [], []
    for a_order in movablesMap:

        # 命令(ACTION)をONE-HOTに変換する
        # 0 / -1 / 0  / 上上(up-up) / [-1, 0]
        if a_order == "UP":
            arr_onehot = [-1, 0]
        # 2 / 0 / -1  / 左左(left-left) / [0, -1]
        elif a_order == "LEFT":
            arr_onehot = [0, -1]
        # 4 / +1 / 0  / 下下(down-down) / [+1, 0]
        elif a_order == "DOWN":
            arr_onehot = [1, 0]
        # 6 / 0  / +1 / 右右(right-right) / [0, +1]
        elif a_order == "RIGHT":
            arr_onehot = [0, 1]
        # ----------------
        # Movablesを計算する
        MovableYpos = MazeYpos + arr_onehot[0]      # メイズのY座標
        MovableXpos = MazeXpos + arr_onehot[1]      # メイズのX座標
        # ----------------
        # ゴール距離
        sqGoal   = (MovableYpos - pos_lm[0])**2 + (MovableXpos - pos_lm[1])**2
        distGoal = np.sqrt(sqGoal)
        # 空間用minRTメトリックス-Aルート(標準)を計算する
        dist_sminRTA = space_minRT(mx_route, MovableYpos, MovableXpos)
        # リストに追加する
        arr_Gdists.append(round(distGoal,5))
        arr_sminRTA.append(round(dist_sminRTA,5))
    arr_Gdists  = np.array(arr_Gdists)
    arr_sminRTA = np.array(arr_sminRTA)

    arr_probs = softmax(CTemp/(2*arr_Gdists*arr_Gdists+arr_sminRTA*arr_sminRTA+1.0))
    #print("movablesMap:{}, arr_probs:{}".format(movablesMap, arr_probs))

    return arr_probs


# =================================================
# difinition of MAZE-FIELD CLASS
# =================================================
# Maze Class
class Maze(object):
    def __init__(self, size=27, blocks_rate=0.1):
        self.size = size if size > 3 else 27
        self.blocks = int((size ** 2) * blocks_rate) 
        self.s_list = []
        self.maze_list = []
        self.e_list = []

    def create_mid_lines(self, k):
        if k == 0: self.maze_list.append(self.s_list)
        elif k == self.size - 1: self.maze_list.append(self.e_list)
        else:
            tmp_list = []
            for l in range(0,self.size):
                if l == 0: tmp_list.extend("#")
                elif l == self.size-1: tmp_list.extend("#")
                else:
                    a = random.randint(-1, 0)
                    tmp_list.extend([a])
            self.maze_list.append(tmp_list)

    def insert_blocks(self, k, s_r, e_r):
        b_y = random.randint(1, self.size-2)
        b_x = random.randint(1, self.size-2)
        if [b_y, b_x] == [1, s_r] or [b_y, b_x] == [self.size - 2, e_r]: k = k-1
        else: self.maze_list[b_y][b_x] = "#"
            
    def generate_maze(self): 
        s_r = random.randint(1, int((self.size / 2)) - 1)
        for i in range(0, self.size):
            if i == s_r: self.s_list.extend("S")
            else: self.s_list.extend("#")
        start_point = [0, s_r]

        e_r = random.randint(int((self.size / 2)) + 1, self.size - 2)
        for j in range(0, self.size):
            if j == e_r: self.e_list.extend([50])
            else: self.e_list.extend("#")
        goal_point = [self.size - 1, e_r]

        for k in range(0, self.size):
            self.create_mid_lines(k)
        
        for k in range(self.blocks):
            self.insert_blocks(k, s_r, e_r)

        return self.maze_list, start_point, goal_point

    # -------------------------------
    # 盤面のCSVファイルを読み込み表示する
    def read_boardfile(self):
        # CSVファイルの読み込み
        code_csvout = "maze_board27.csv"       # file name
        file_readcsv = foldername + code_csvout   # standard(training) file name  
        df = pd.read_csv(file_readcsv)
        # -------------------------------
        # 選択項目の読み込み
        maze = df.loc[:, "0":"26"].values.tolist()
        start_point, goal_point = [0,0], [0,0]
        for i in range(size):
            for j in range(size):
                if maze[i][j] == 'S':
                    start_point = [i, j]
                if maze[i][j] == '5000':
                    goal_point  = [i, j]
        # -----
        self.maze_list   = maze
        self.start_point = start_point
        self.goal_point  = goal_point
        #print("goal_point", goal_point)
        #print("----- mx_maze(read_boardfile) -----")
        #print(maze)

        return self.maze_list, start_point, goal_point

    # -------------------------------
    # コマの最短ルートのCSVファイルを読み込み表示する（標準Aルート）
    def read_chessfile(self):

        # CSVファイルの読み込み
        code_csvout = "chess_route27.csv"       # file name
        file_readcsv = foldername + code_csvout   # standard(training) file name  
        df = pd.read_csv(file_readcsv)
        # -------------------------------
        # 選択項目の読み込み
        mx_route    = df.loc[:, "row":"col"].values.tolist()
        ref_action  = df.loc[:, "action"].values.tolist()
        #print("ref_action: ", ref_action)
        #print("----- route -----")
        #print(route)

        return mx_route, ref_action

    # -------------------------------
    # コマの最短ルートのCSVファイルを読み込み表示する（限界BCルート）
    def read_chessfileBC(self, code_csvout):

        # CSVファイルの読み込み
        file_readcsv = foldername + code_csvout   # standard(training) file name  
        df = pd.read_csv(file_readcsv)
        #max_play = len(df)
        # print("データ量 max_play",max_play)
        # print(df)
        # -------------------------------
        # 選択項目の読み込み
        mx_route    = df.loc[:, "row":"col"].values.tolist()

        return mx_route

# ----------------
# Maze-Field functions
class Field(object):
    def __init__(self, maze, start_point, goal_point):
        # ----------------
        # 次の行動(ACTION-８方向)に進む
        # action_number / row / col / 方向/ ベクトル(変則ONEHOT)
        # 0 / -1 / 0  / 上上(up-up) / [-1, 0]
        # 1 / -1 / -1  / 上左(up-left) / [-1, -1]
        # 2 / 0 / -1  / 左左(left-left) / [0, -1]
        # 3 / +1 / -1  / 左下(down-left) / [+1, -1]
        # 4 / +1 / 0  / 下下(down-down) / [+1, 0]
        # 5 / +1 / +1  / 下右(down-right) / [+1, +1]
        # 6 / 0  / +1 / 右右(right-right) / [0, +1]
        # 7 / -1  / +1 / 上右(up-right) / [-1, +1]
        # ----------------
        self.maze = maze
        self.start_point = start_point
        self.goal_point  = goal_point
        self.movable_vec = [[-1, 0],[-1, -1],[0, -1],[1, -1],[1, 0],[1, 1],[0, 1],[-1, 1]]

    def display(self, point=None):
        field_data = copy.deepcopy(self.maze)
        if not point is None:
                y, x = point
                field_data[y][x] = "@@"
        else:
                point = ""
        print("================")
        for line in field_data:
                print ("\t" + "%3s " * len(line) % tuple(line))
        print("================")

    def all_display(self, mx_route):
        field_data = copy.deepcopy(self.maze)
        for i in range(len(mx_route)):
            point = mx_route[i]
            y, x = point[0], point[1]
            field_data[y][x] = "@@"
        print("================")
        for line in field_data:
                print ("\t" + "%3s " * len(line) % tuple(line))
        print("================")

    # ----------------
    # [3D-Map]movablesMap : 移動可能な命令群
    def get_ActionMaps(self, stateMap_chess, arr_MPODR):

        movablesMap = []
        for a_order in arr_MPODR:
            # マップの座標をメイズのXY座標に変換する
            MazeYpos = math.floor(stateMap_chess[0]/WIDE)   # メイズのY座標
            MazeXpos = math.floor(stateMap_chess[1]/WIDE)   # メイズのX座標
            # ----------------
            # 命令(ACTION)をONE-HOTに変換する
            # 0 / -1 / 0  / 上上(up-up) / [-1, 0]
            if a_order == "UP":
                arr_onehot = [-1, 0]
            # 2 / 0 / -1  / 左左(left-left) / [0, -1]
            elif a_order == "LEFT":
                arr_onehot = [0, -1]
            # 4 / +1 / 0  / 下下(down-down) / [+1, 0]
            elif a_order == "DOWN":
                arr_onehot = [1, 0]
            # 6 / 0  / +1 / 右右(right-right) / [0, +1]
            elif a_order == "RIGHT":
                arr_onehot = [0, 1]
            # ----------------
            # Movablesを計算する
            y = MazeYpos + arr_onehot[0]      # メイズのY座標
            x = MazeXpos + arr_onehot[1]      # メイズのX座標
            # ----------------
            if 0 < x < len(self.maze) and 0 < y < len(self.maze) and self.maze[y][x] != "#" and self.maze[y][x] != "S":
                movablesMap.append(a_order)

        return movablesMap

    # ----------------
    # [2D-Maze]movablesMaze : 移動可能な命令群
    def get_MazeActions(self, state):
        movables = []
        if state == self.start_point:
            y = state[0] + 1
            x = state[1]
            a = [[y, x]]
            return a
        else:
            for v in self.movable_vec:
                y = state[0] + v[0]
                x = state[1] + v[1]
                if not(0 < x < len(self.maze) and
                       0 <= y <= len(self.maze) - 1 and
                       self.maze[y][x] != "#" and
                       self.maze[y][x] != "S"):
                    continue
                movables.append([y,x])
            if len(movables) != 0:
                return movables
            else:
                return None

    # -------------------------------
    # 次のコマの「決定」位置と報酬、ゲームフラグを決める
    # 新概念：マスク「#」位置に着くと、スタート地点に戻る
    def get_valMaze(self, stateMaze_chess):

        yPos = stateMaze_chess[0]
        xPos = stateMaze_chess[1]
        # ----------------
        if stateMaze_chess == self.start_point:
            return 0, False
        elif self.maze[yPos][xPos] == "#":
            v = -10.0        # -10
            return v, False
        else:
            v = float(self.maze[yPos][xPos])
            if stateMaze_chess == self.goal_point: 
                return v, True
            else: 
                return v, False

# ----------------
# Initialize and Generate a maze
size, barriar_rate = 27, 0.1
maze_1 = Maze(size, barriar_rate)
# init_maze : カリキュラム学習用に改造
init_maze, start_point, goal_point = maze_1.read_boardfile()
#print("start_point: {}, goal_point: {}".format(start_point, goal_point))
# インスタンスの生成
maze_field = Field(init_maze, start_point, goal_point)
# ボード表示(初期表示)
maze_field.display()
# ----------
# 最短ルートを読み込み(Aルート)
mx_route, ref_action = maze_1.read_chessfile()
# ルートを表示する
maze_field.all_display(mx_route)
# ----------
# コマの最短ルートのCSVファイルを読み込み表示する（限界BCルート）
# Bルート
mx_routeB = maze_1.read_chessfileBC("chess_route27B.csv")
# Cルート
mx_routeC = maze_1.read_chessfileBC("chess_route27C.csv")
#print("--- mx_routeC ---")
#print(mx_routeC)


#=================================================
# [2D-Maze]Deep Learning Model class            
#=================================================
# [2D-Maze]PyTorchのDLの定義
class Net2D(torch.nn.Module):
    def __init__(self):
        super(Net2D, self).__init__()
        self.fc1 = torch.nn.Linear(dim_input2D, 128)
        self.fc1.weight.data.normal_(0, 0.1)
        self.fc2 = torch.nn.Linear(128, 128)
        self.fc2.weight.data.normal_(0, 0.1)
        self.fc3 = torch.nn.Linear(128, 128)
        self.fc3.weight.data.normal_(0, 0.1)
        self.fc4 = torch.nn.Linear(128, dim_output2D)
        self.fc4.weight.data.normal_(0, 0.1)
        
    def forward(self, x):
        x = torch.nn.functional.relu(self.fc1(x))
        x = torch.nn.functional.relu(self.fc2(x))
        x = torch.nn.functional.relu(self.fc3(x))
        x = self.fc4(x)
        return x

# =======================================
# [2DMaze-input_only]Solving the maze in Deep Q-learning
class DQN_Maze_Solver2D:

    def __init__(self, stateMaze_size, actionMaze_size):

        # 共通変数
        self.stateMaze_size  = stateMaze_size       # list size of state[2D]
        self.actionMaze_size = actionMaze_size      # list size of action[2D]
        self.learning_rate = 0.002          # learning rate of neural network
        # -------------------------------
        # crate instance for input
        self.eval_net2D = Net2D()           # Net2Dを利用して２つのニューラルネットをつくる
        # -------------------------------
        # Save and load the model via state_dict
        self.eval_net2D.load_state_dict(torch.load(file_input_2Dmodel))
        # -------------------------------
        # set validaiton mode
        self.eval_net2D.eval()

    # choosing action depending on epsilon
    def choose_actionMaze(self, stateMaze, movablesMaze):

        # ----------------
        # 次の行動(ACTION-８方向)に進む
        # action_number / row / col / 方向/ ベクトル
        # 0 / -1 / 0  / 上上(up-up) / [-1, 0]
        # 1 / -1 / -1  / 上左(up-left) / [-1, -1]
        # 2 / 0 / -1  / 左左(left-left) / [0, -1]
        # 3 / +1 / -1  / 左下(down-left) / [+1, -1]
        # 4 / +1 / 0  / 下下(down-down) / [+1, 0]
        # 5 / +1 / +1  / 下右(down-right) / [+1, +1]
        # 6 / 0  / +1 / 右右(right-right) / [0, +1]
        # 7 / -1  / +1 / 上右(up-right) / [-1, +1]
        # ----------------
        actionMaze_chess = [0,0]
        acTypeMap, QvalueMaze, iCnt = "machine", -0.001, 0
        for a_order in movablesMaze:
            # 変則ONE-HOT化
            diff_row2D = a_order[0] - stateMaze[0]
            diff_col2D = a_order[1] - stateMaze[1]
            # ----------------
            # 状態(STATE)行動(ACTION)ベクトルを生成する
            np_action2D = np.hstack([stateMaze, [diff_row2D, diff_col2D]])
            if iCnt == 0:
                mx_input   = [np_action2D]  # 状態(S)行動(A)マトリックス
            else:
                mx_input   = np.concatenate([mx_input, [np_action2D]], axis=0)     # 状態(S)行動(A)マトリックス
            iCnt = iCnt + 1
        #print("----- choose_actionMaze - mx_input -----")
        #print(mx_input)
        # -------------------------------
        # generate new 'x'
        x_input_tensor = torch.tensor(mx_input).float()
        # predict 'y'
        with torch.no_grad():
            y_pred_tensor = self.eval_net2D(x_input_tensor)
        # convert tensor to numpy
        y_pred        = y_pred_tensor.data.numpy().flatten()
        QvalueMaze    = np.max(y_pred)
        temp_order    = np.argmax(y_pred)
        actionMaze_chess  = movablesMaze[temp_order]
        #print("stateMaze:{0}, y_pred:{1}, temp_order:{2}, actionMaze_chess:{3}".format(stateMaze, y_pred, temp_order, actionMaze_chess))

        return acTypeMap, actionMaze_chess, QvalueMaze


#=================================================
# [3D-Map]Deep Learning Model class            
#=================================================
# [3D-Map]PyTorchのDLの定義
class Net3D(torch.nn.Module):
    def __init__(self):
        super(Net3D, self).__init__()
        self.fc1 = torch.nn.Linear(dim_input3D, 128)
        self.fc1.weight.data.normal_(0, 0.1)
        self.fc2 = torch.nn.Linear(128, 128)
        self.fc2.weight.data.normal_(0, 0.1)
        self.fc3 = torch.nn.Linear(128, 128)
        self.fc3.weight.data.normal_(0, 0.1)
        self.fc4 = torch.nn.Linear(128, dim_output3D)
        self.fc4.weight.data.normal_(0, 0.1)
        
    def forward(self, x):
        x = torch.nn.functional.relu(self.fc1(x))
        x = torch.nn.functional.relu(self.fc2(x))
        x = torch.nn.functional.relu(self.fc3(x))
        x = self.fc4(x)
        return x

# =======================================
# [3D_Map]Solving the maze in Deep Q-learning
class DQN_Map_Solver3D:

    def __init__(self, stateMap_size, actionMap_size):
        self.stateMap_size  = stateMap_size # list size of stateMap
        self.actionMap_size = actionMap_size # list size of actionMap
        self.memory     = deque(maxlen=5000) # memory space
        self.gamma      = 0.99 # discount rate
        self.epsilon    = 0.99 # randomness of choosing random actionMap or the best one
        self.e_decay    = 0.9992 # epsilon decay rate
        self.e_min      = 0.01 # minimum rate of epsilon
        self.learning_rate  = 0.002 # learning rate of neural network
        # -------------------------
        # crate instance for input
        self.eval_net3D, self.target_net3D = Net3D(), Net3D()           # Net3Dを利用して２つのニューラルネットをつくる
        self.learn_step_counter = 0    
        # -------------------------
        # Save and load the model via state_dict
        #self.eval_net3D.load_state_dict(torch.load(file_input_3Dmodel))
        # -------------------------
        # set validaiton mode
        self.eval_net3D.eval()
        # -------------------------
        # set training parameters
        self.optimizer = torch.optim.Adam(self.eval_net3D.parameters(), lr=self.learning_rate)
        self.criterion = torch.nn.MSELoss()

    # ----------------
    # PyTorchモデルを保存する
    def save_models(self, file_output_3Dmodel):
        torch.save(self.eval_net3D.state_dict(), file_output_3Dmodel)
        print("モデルが出力されました: {}".format(file_output_3Dmodel))

    # ----------------
    # remember stateMap, actionMap, its reward, next stateMap and next possible actionMap. doneMap means boolean for goal
    def remember_memory(self, stateMap, actionMap, reward, next_stateMap, next_movablesMap, doneMap):
        self.memory.append((stateMap, actionMap, reward, next_stateMap, next_movablesMap, doneMap))

    # ----------------
    # choosing actionMap depending on epsilon
    def choose_actionMap(self, stateMap_chess, stateMap, movablesMap, pos_lmG):

        acTypeMap, QvalueMap = "random", -0.001
        # マップの座標をメイズのXY座標に変換する
        MazeYpos = math.floor(stateMap_chess[0]/WIDE)   # メイズのY座標
        MazeXpos = math.floor(stateMap_chess[1]/WIDE)   # メイズのX座標
        #print("MazeXpos:{}, MazeYpos:{}, movablesMap:{}".format(MazeXpos, MazeYpos, movablesMap))
        # ------
        #print("movablesMap: ", movablesMap)
        if self.epsilon >= random.random():
            if MazeYpos == pos_lmG[0] and MazeXpos == pos_lmG[1]:
                #print("A-actionMap: ", "DOWN")
                actionMap = "DOWN"
                return acTypeMap, actionMap, QvalueMap
            else:
                if len(movablesMap) > 0:
                    # ボルツマン選択による出現確率の調整
                    arr_probs = calc_Boltzman(MazeXpos, MazeYpos, movablesMap, pos_lmG)
                    # randomly choosing actionMap
                    actionMap = np.random.choice(movablesMap, p=arr_probs)
                    #print("B-actionMap: ", actionMap)
                    return acTypeMap, actionMap, QvalueMap
                else:
                    #print("A-actionMap: ", "DOWN")
                    actionMap = "DOWN"
                    return acTypeMap, actionMap, QvalueMap
        else:
            # choosing the best actionMap from model.predict()
            acTypeMap, actionMap, QvalueMap = self.choose_best_actionMap(stateMap, movablesMap)
            return acTypeMap, actionMap, QvalueMap


    # choose the best action[3D] to maximize reward expectation
    def choose_best_actionMap(self, stateMap, movablesMap):

        # ----------------
        # 次の行動(ACTION-８方向)に進む
        # action_number / row / col / 方向/ ベクトル
        # 0 / -1 / 0  / 上上(up-up) / [-1, 0]
        # 1 / -1 / -1  / 上左(up-left) / [-1, -1]
        # 2 / 0 / -1  / 左左(left-left) / [0, -1]
        # 3 / +1 / -1  / 左下(down-left) / [+1, -1]
        # 4 / +1 / 0  / 下下(down-down) / [+1, 0]
        # 5 / +1 / +1  / 下右(down-right) / [+1, +1]
        # 6 / 0  / +1 / 右右(right-right) / [0, +1]
        # 7 / -1  / +1 / 上右(up-right) / [-1, +1]
        # ----------------
        acTypeMap, QvalueMap, iCnt = "machine", -0.001, 0
        for a_order in movablesMap:
            # ONE-HOTに変換する
            arr_onehot = [0, 0]
            # 0 / -1 / 0  / 上上(up-up) / [-1, 0]
            if a_order == "UP":
                arr_onehot = [-1, 0]
            # 2 / 0 / -1  / 左左(left-left) / [0, -1]
            elif a_order == "LEFT":
                arr_onehot = [0, -1]
            # 4 / +1 / 0  / 下下(down-down) / [+1, 0]
            elif a_order == "DOWN":
                arr_onehot = [1, 0]
            # 6 / 0  / +1 / 右右(right-right) / [0, +1]
            elif a_order == "RIGHT":
                arr_onehot = [0, 1]
            # ----------------
            # 状態(STATE)行動(ACTION)ベクトルを生成する
            np_actionMap = np.hstack([stateMap, arr_onehot])
            #print("np_actionMap: ",np_actionMap)
            if iCnt == 0:
                mx_input   = [np_actionMap]  # 状態(S)行動(A)マトリックス
            else:
                mx_input   = np.concatenate([mx_input, [np_actionMap]], axis=0)     # 状態(S)行動(A)マトリックス
            iCnt = iCnt + 1
        #print("----- (choose_best_actionMap) mx_input -----")
        #print(mx_input)
        # ----------------
        # generate new 'x'
        x_input_tensor = torch.tensor(mx_input).float()
        # predict 'y'
        with torch.no_grad():
            y_pred_tensor = self.eval_net3D(x_input_tensor)
        # convert tensor to numpy
        y_pred      = y_pred_tensor.data.numpy().flatten()
        QvalueMap   = np.max(y_pred)
        temp_order  = np.argmax(y_pred)
        actionMap   = movablesMap[temp_order]
        #print("stateMap:{0}, y_pred:{1}, temp_order:{2}, actionMap:{3}".format(stateMap, y_pred, temp_order, actionMap))

        return acTypeMap, actionMap, QvalueMap


    # this experience replay is going to train the model from memorized stateMaps, actionMaps and rewards
    def replay_experience(self, batch_size):

        # ターゲットNet3Dのパラメタを更新する
        if self.learn_step_counter < 5 or self.learn_step_counter % TARGET_REPLACE_ITER == 0:    # 最初5回は毎回に更新し、あとは定期的に更新する
            self.target_net3D.load_state_dict(self.eval_net3D.state_dict())         # 評価Net3DのパラメタをターゲットNet3Dに引き渡す
        self.learn_step_counter += 1   
        # -------------------------
        batch_size = min(batch_size, len(self.memory))
        minibatch = random.sample(self.memory, batch_size)
        X, Y = [], []
        # -------------------------
        for ibat in range(batch_size):
            stateMap, actionMap, reward, next_stateMap, next_movablesMap, doneMap = minibatch[ibat]
            # ----------------
            # 次の行動(ACTION-８方向)に進む
            # action_number / row / col / 方向/ ベクトル
            # 0 / -1 / 0  / 上上(up-up) / [-1, 0]
            # 1 / -1 / -1  / 上左(up-left) / [-1, -1]
            # 2 / 0 / -1  / 左左(left-left) / [0, -1]
            # 3 / +1 / -1  / 左下(down-left) / [+1, -1]
            # 4 / +1 / 0  / 下下(down-down) / [+1, 0]
            # 5 / +1 / +1  / 下右(down-right) / [+1, +1]
            # 6 / 0  / +1 / 右右(right-right) / [0, +1]
            # 7 / -1  / +1 / 上右(up-right) / [-1, +1]
            # ----------------
            # ONE-HOTに変換する
            arr_onehot = [0, 0]
            # 0 / -1 / 0  / 上上(up-up) / [-1, 0]
            if actionMap == "UP":
                arr_onehot = [-1, 0]
            # 2 / 0 / -1  / 左左(left-left) / [0, -1]
            elif actionMap == "LEFT":
                arr_onehot = [0, -1]
            # 4 / +1 / 0  / 下下(down-down) / [+1, 0]
            elif actionMap == "DOWN":
                arr_onehot = [1, 0]
            # 6 / 0  / +1 / 右右(right-right) / [0, +1]
            elif actionMap == "RIGHT":
                arr_onehot = [0, 1]
            # ----------------
            # 状態(STATE)行動(ACTION)ベクトルを生成する
            state_action_curr = np.hstack([stateMap, arr_onehot])
            #print("state_action_curr",state_action_curr)
            # ----------------
            if doneMap or len(next_movablesMap) == 0:
                target_f = reward
            else:
                next_rewards = []
                iCnt = 0
                for a_order in next_movablesMap:
                    # ONE-HOTに変換する
                    arr_onehot = [0, 0]
                    # 0 / -1 / 0  / 上上(up-up) / [-1, 0]
                    if a_order == "UP":
                        arr_onehot = [-1, 0]
                    # 2 / 0 / -1  / 左左(left-left) / [0, -1]
                    elif a_order == "LEFT":
                        arr_onehot = [0, -1]
                    # 4 / +1 / 0  / 下下(down-down) / [+1, 0]
                    elif a_order == "DOWN":
                        arr_onehot = [1, 0]
                    # 6 / 0  / +1 / 右右(right-right) / [0, +1]
                    elif a_order == "RIGHT":
                        arr_onehot = [0, 1]
                    # ----------------
                    # 状態(STATE)行動(ACTION)ベクトルを生成する
                    np_next_s_a = np.hstack([next_stateMap, arr_onehot])
                    if iCnt == 0:
                        mx_input = [np_next_s_a]  # 状態(S)行動(A)マトリックス
                    else:
                        mx_input = np.concatenate([mx_input, [np_next_s_a]], axis=0)     # 状態(S)行動(A)マトリックス
                    iCnt = iCnt + 1
                # ----------------
                # generate new 'x'
                x_input_tensor = torch.tensor(mx_input).float()
                # predict 'y'
                with torch.no_grad():
                    y_pred_tensor = self.target_net3D(x_input_tensor)
                # convert tensor to numpy
                y_pred = y_pred_tensor.data.numpy().flatten()
                np_n_r_max = np.amax(y_pred)
                target_f = reward + self.gamma * np_n_r_max
            # ----------------
            if ibat == 0:
                X = np.array([state_action_curr])  # 状態(S)行動(A)マトリックス
            else:
                X = np.concatenate([X, np.array([state_action_curr])], axis=0)  # 状態(S)行動(A)マトリックス
            Y = np.append(Y,target_f)
        # -------------------------
        # TRAINING
        # convert numpy array to tensor
        state_action_eval = torch.from_numpy(X).float()
        q_target = torch.from_numpy(Y.reshape(-1, 1)).float()
        #print("state_action_eval:",state_action_eval)
        #print("q_target:",q_target)
        # --- building model ---
        q_eval = self.eval_net3D(state_action_eval)
        # calculate loss
        loss = self.criterion(q_eval, q_target)
        # update weights
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        # Show progress
        #print('learn doneMap -- [epsilon: {0}, loss: {1}]'.format(self.epsilon, loss))
        if self.epsilon > self.e_min:
            self.epsilon *= self.e_decay

        return round(self.epsilon,5) , round(loss.data.item(),5)


# =================================================
# difinition of functions
# =================================================
# pygame初期化
pygame.init()
# ----------------
TURN, STEP, RES, VANG = math.pi/4, 15, 320, math.pi/6
PLAYERDISTANCE = (RES/2)/math.tan(VANG)
FOV, i = [], -VANG
while (i < VANG):
    FOV.append(i)
    i+=(VANG*2)/RES
WIDE, HIGH = 64, 128
# ----------------
pygame.display.set_caption("Raycast Maze")
CLOCK = pygame.time.Clock()

## ############################## WALLS ##############################
SPRITEBLOCK = pygame.image.load("walls.png")
ROWWLL, COLWLL = [None] * 64, [None] * 64
ROWBGN, COLBGN = [None] * 64, [None] * 64
ROWEND, COLEND = [None] * 64, [None] * 64
for i in range(64):
    ROWWLL[i], COLWLL[i] = pygame.Surface((1,64)), pygame.Surface((1,64))
    ROWBGN[i], COLBGN[i] = pygame.Surface((1,64)), pygame.Surface((1,64))
    ROWEND[i], COLEND[i] = pygame.Surface((1,64)), pygame.Surface((1,64))
    ROWWLL[i].blit(SPRITEBLOCK, [0,0], ( 0+i,  0, 1, 64))
    COLWLL[i].blit(SPRITEBLOCK, [0,0], (64+i,  0, 1, 64))
    ROWBGN[i].blit(SPRITEBLOCK, [0,0], ( 0+i, 64, 1, 64))
    COLBGN[i].blit(SPRITEBLOCK, [0,0], (64+i, 64, 1, 64))
    ROWEND[i].blit(SPRITEBLOCK, [0,0], ( 0+i,128, 1, 64))
    COLEND[i].blit(SPRITEBLOCK, [0,0], (64+i,128, 1, 64))
## ############################## WALLS ##############################

## #########################################################################################
## ############################# START : PRIM'S MAZE ALGORITHM #############################
## #########################################################################################

def removeWalls(x, y, maze, frontier, rows, cols):
    N,E,S,W,IN,FRONTIER = 1,2,4,8,16,32
    ins = []

    #add to possible wall removal if IS IN MAZE and DOES HAVE WALL
    if (x > 0) and ((maze[x-1][y] & IN) == IN) and ((maze[x][y] & W) == W):
        ins.append([x-1,y,E,W])
    if (x < cols-1) and ((maze[x+1][y] & IN) == IN) and ((maze[x][y] & E) == E):
        ins.append([x+1,y,W,E])
    if (y > 0) and ((maze[x][y-1] & IN) == IN) and ((maze[x][y] & N) == N):
        ins.append([x,y-1,S,N])
    if (y < rows-1) and ((maze[x][y+1] & IN) == IN) and ((maze[x][y] & S) == S):
        ins.append([x,y+1,N,S])

    #if there is a wall to remove - Select Random Room and Remove Walls adjoining cells
    if (len(ins) > 0):
        deWall = random.choice(ins)
        thisX, thisY, thisW = x, y, deWall[3]
        thatX, thatY, thatW = deWall[0], deWall[1], deWall[2]
        maze[thisX][thisY] = maze[thisX][thisY] & ~thisW
        maze[thatX][thatY] = maze[thatX][thatY] & ~thatW

def addToMaze(x, y, maze, frontier, rows, cols):
    N,E,S,W,IN,FRONTIER = 1,2,4,8,16,32

    maze[x][y] = maze[x][y] & ~FRONTIER #remove from frontier
    maze[x][y] = maze[x][y] | IN #add to maze
    removeWalls(x, y, maze, frontier, rows, cols) #remove walls
    makeFrontier(x,y,maze,frontier, rows, cols) #make new frontier

def makeFrontier(x,y,maze,frontier, rows, cols):
    N,E,S,W,IN,FRONTIER = 1,2,4,8,16,32

    if (x > 0) and ((maze[x-1][y] & IN) != IN):
        if (maze[x-1][y] & FRONTIER) != FRONTIER:
            maze[x-1][y] = maze[x-1][y] | FRONTIER
            frontier.append([x-1,y])
    if (x < cols-1) and ((maze[x+1][y] & IN) != IN):
        if (maze[x+1][y] & FRONTIER) != FRONTIER:
            maze[x+1][y] = maze[x+1][y] | FRONTIER
            frontier.append([x+1,y])
    if (y > 0) and ((maze[x][y-1] & IN) != IN):
        if (maze[x][y-1] & FRONTIER) != FRONTIER:
            maze[x][y-1] = maze[x][y-1] | FRONTIER
            frontier.append([x,y-1])
    if (y < rows-1) and ((maze[x][y+1] & IN) != IN):
        if (maze[x][y+1] & FRONTIER) != FRONTIER:
            maze[x][y+1] = maze[x][y+1] | FRONTIER
            frontier.append([x,y+1])

def makeMaze(rows,cols):
    N,E,S,W,IN,FRONTIER = 1,2,4,8,16,32
    MAZEROWS, MAZECOLS = rows, cols
    frontier = []
    maze = [[(N|E|S|W) for y in range(MAZEROWS)] for x in range(MAZECOLS)] #create maze with all walls

    addToMaze(0, 0, maze, frontier, MAZEROWS, MAZECOLS)
    while (len(frontier) > 0):
        random.shuffle(frontier)
        tmp = frontier.pop()
        addToMaze(tmp[0], tmp[1], maze, frontier, MAZEROWS, MAZECOLS)

    return maze

## #######################################################################################
## ############################# END : PRIM'S MAZE ALGORITHM #############################
## #######################################################################################

## ############################# START : TURN MAZE INTO GAMEBOARD #############################
def gameBoard(maze):
    N,E,S,W,IN,FRONTIER = 1,2,4,8,16,32

    PATHWIDTH = 2
    MCOLS, MROWS = len(maze), len(maze[0])
    COLP, ROWP = (MCOLS*PATHWIDTH)+1, (MROWS*PATHWIDTH)+1
    game = [[0 for y in range(ROWP)] for x in range(COLP)] #create empty gameboard

    for y in range(MROWS):
        for x in range(MCOLS):
            if (maze[x][y] & N):
                for i in range(PATHWIDTH+1):
                    game[(x*PATHWIDTH)+i][y*PATHWIDTH] = 1
            if (maze[x][y] & W):
                for i in range(PATHWIDTH+1):
                    game[x*PATHWIDTH][(y*PATHWIDTH)+i] = 1

    for y in range(ROWP):
        game[COLP-1][y] = 1
    for x in range(COLP):
        game[x][ROWP-1] = 1

    return game

## ############################# END : TURN MAZE INTO GAMEBOARD #############################
def makeBackground(ROWS,COLS):
    BACKGROUND = pygame.Surface((COLS*WIDE,ROWS*WIDE))
    #Whole self.BACKGROUND
    BACKGROUND.fill((128,128,128))
    #Just the Ceiling
    pygame.draw.rect(BACKGROUND, (192,192,192), [0,0, COLS*WIDE, ROWS*WIDE/2], 0)
    return BACKGROUND

def makeMap(gameboard):
    COLS, ROWS = len(gameboard), len(gameboard[0])
    MAP = pygame.Surface((COLS,ROWS))
    MAP.fill((192,192,192))
    for y in range(ROWS):
        for x in range(COLS):
            if(int(gameboard[y][x]) == 1):
                pygame.draw.rect(MAP, (0,0,0), [x,y, 1, 1], 0)
    return MAP

## #######################################################################################
## ################################# START : RAY-CASTING #################################
## #######################################################################################
def canGo(x,y,gameboard):
    COLS, ROWS = len(gameboard), len(gameboard[0])
    dx, dy = math.floor(x/WIDE), math.floor(y/WIDE)
    if (dx > 0) and (dx < COLS) and (dy > 0) and (dy < ROWS):
        if (int(gameboard[dy][dx]) == 0):
            return True
    return False

def castRay(raydir,posX,posY,playdir,gameboard):
    COLS, ROWS = len(gameboard), len(gameboard[0])
    rays, rise, run = [], math.sin(raydir), math.cos(raydir)
    if (abs(run) > 0.001):
        slope = rise/run
    else:
        slope = 10000

    if (raydir > (3*math.pi/2)):
        colRange = [math.ceil(posX/WIDE),COLS, 1] #low to high
        rowRange = [math.floor(posY/WIDE),0,-1]   #high to low
        colHit, rowHit = 0, -1
    elif (raydir > (math.pi)):
        colRange = [math.floor(posX/WIDE),0,-1]   #high to low
        rowRange = [math.floor(posY/WIDE),0,-1]   #high to low
        colHit, rowHit = -1, -1
    elif (raydir > (math.pi/2)):
        colRange = [math.floor(posX/WIDE),0,-1]   #high to low
        rowRange = [math.ceil(posY/WIDE),ROWS, 1] #low to high
        colHit, rowHit = -1, 0
    else:
        colRange = [math.ceil(posX/WIDE),COLS, 1] #low to high
        rowRange = [math.ceil(posY/WIDE),ROWS, 1] #low to high
        colHit, rowHit = 0, 0

    hitR = False
    if (abs(slope) > 0.0001):    
        for y in range(rowRange[0],rowRange[1],rowRange[2]):
            if (hitR): break
            rowY = y * WIDE
            rowX = (rowY - posY + (slope * posX)) / slope
            if (rowX >= 0) and (rowX < ROWS*WIDE):
                hitY = math.floor(rowY/WIDE)+rowHit
                hitX = math.floor(rowX/WIDE)
                if (int(gameboard[hitY][hitX]) == 1):
                    distance = math.sqrt(math.pow(posX-rowX,2) + math.pow(posY-rowY,2))
                    distance = 1 + (distance * math.cos(raydir-playdir))
                    along = round(rowX)%WIDE
                    rays.append([rowX,rowY,distance,"row",along,hitX,hitY])
                    hitR = True

    hitC = False
    if (abs(slope) < 10000):
        for x in range(colRange[0],colRange[1],colRange[2]):
            if (hitC): break
            colX = (x * WIDE)
            colY = (slope * (colX - posX)) + posY
            if (colY > 0) and (colY < COLS*WIDE):
                hitY = math.floor(colY/WIDE)
                hitX = math.floor(colX/WIDE)+colHit
                if (int(gameboard[hitY][hitX]) == 1):
                    distance = math.sqrt(math.pow(posX-colX,2) + math.pow(posY-colY,2))
                    distance = 1 + (distance * math.cos(raydir-playdir))
                    along = round(colY)%WIDE
                    rays.append([colX,colY,distance,"col",along,hitX,hitY])
                    hitC = True

    if (len(rays) == 0):
        return False
    if (len(rays) == 1):
        return rays[0]
    else:
        if (rays[0][2] < rays[1][2]):
            return rays[0]
        else:
            return rays[1]

## #######################################################################################
## ################################## END : RAY-CASTING ##################################
## #######################################################################################
# プログラムを終了する
def terminate():
    pygame.quit()
    sys.exit()

# -------------------------------
# (ROUTE)MAZE盤面を表示する
def route_display(field_data):
    for line in field_data:
            print ("\t" + "%3s " * len(line) % tuple(line))

# -------------------------------
# 盤面のCSVファイルを読み込み表示する
def read_boardfile(num_mzsize):

    # CSVファイルの読み込み
    code_csvout = "maze_board27.csv"       # file name
    file_readcsv = foldername + code_csvout   # standard(training) file name  
    df = pd.read_csv(file_readcsv)
    # -----
    # 選択項目の読み込み(mx_mazeとmx_field)
    mx_maze     = df.loc[:, "0":"26"].values.tolist()   # 学習用（すべての情報を含む）
    mx_field    = mx_maze                               # ゲーム用（0と１のみ）
    # -----
    # mx_maze : 学習用（すべての情報を含む）
    start_point, goal_point = [0, 0], [0, 0]
    for iRow in range(num_mzsize):
        for jCol in range(num_mzsize):
            if mx_maze[iRow][jCol] == "0":
                mx_maze[iRow][jCol] = 0
            if mx_maze[iRow][jCol] == "-1":
                mx_maze[iRow][jCol] = -1
            if mx_maze[iRow][jCol] == "-5":
                mx_maze[iRow][jCol] = -5
            if mx_maze[iRow][jCol] == "-10":
                mx_maze[iRow][jCol] = -10
            # -----
            if mx_maze[iRow][jCol] == "S":
                start_point = [iRow, jCol]
            if mx_maze[iRow][jCol] == 5000:
                goal_point = [iRow, jCol]
    #print("start_point:{0}, goal_point:{1}".format(start_point, goal_point))
    # -----
    # mx_field : ゲーム用（0と１のみ）
    for iRow in range(num_mzsize):
        for jCol in range(num_mzsize):
            if mx_field[iRow][jCol] == "#":
                mx_maze[iRow][jCol] = 1
            else:
                mx_maze[iRow][jCol] = 0

    return mx_maze, start_point, goal_point, mx_field

#=================================================
# 画像処理とRTメトリックス計算
#=================================================
# 画像のモザイク化
def mosaic(img, scale=0.1):
    h, w = img.shape[:2]  # 画像の大きさ

    # 画像を scale (0 < scale <= 1) 倍に縮小する。
    dst_small = cv2.resize(
        img, dsize=None, fx=scale, fy=scale, interpolation=cv2.INTER_NEAREST
    )

    # 元の大きさに拡大する。
    dst_mosaic = cv2.resize(dst_small, dsize=(w, h), interpolation=cv2.INTER_NEAREST)

    return dst_small, dst_mosaic

# ---------------------------------
# ユークリッド距離を計算する
@njit
def cal_euclid(data_color, color_datum):

    dist_euclid   = (data_color[0] - color_datum[0])**2 + (data_color[1] - color_datum[1])**2 + (data_color[2] - color_datum[2])**2
    dist_euclid   = round(np.sqrt(dist_euclid),3)

    return dist_euclid

# ---------------------------------
# 量子化処理を行う。
def cal_quantization(mx_data, color_datum):

    # ---------------------------------
    # 基準色の種類
    #color_datum_0   = [150, 150, 150]     # 灰色 -> 0 -> sigA
    #color_datum_1   = [90, 90, 90]        # 暗灰色 -> 1 -> tani
    #color_datum_2   = [5, 5, 5]           # 黒色 -> 2 -> tani
    #color_datum_3   = [10, 10, 170]       # 青色 -> 3 -> sigB
    #color_datum_4   = [170, 10, 10]       # 赤色 -> 4 -> sigA
    #color_datum_5   = [10, 170, 10]       # 緑色 -> 5 -> sigB
    # ---------------------------------
    # 画像の大きさ
    max_ypic, max_picx  = mx_data.shape[0], mx_data.shape[1]
    mx_pics_ALL = []
    mx_pics_datum_0 = np.zeros([max_ypic, max_picx])
    mx_pics_datum_1 = np.zeros([max_ypic, max_picx])
    mx_pics_datum_2 = np.zeros([max_ypic, max_picx])
    mx_pics_datum_3 = np.zeros([max_ypic, max_picx])
    mx_pics_datum_4 = np.zeros([max_ypic, max_picx])
    mx_pics_datum_5 = np.zeros([max_ypic, max_picx])
    #mx_data: (45, 64, 3)
    #print("mx_data: ", mx_data.shape)
    # ---------------------------------
    # 画素を分析して、各色のマトリックスに分配する
    for idx_y in range(max_ypic):
        for idx_x in range(max_picx):
            # -----
            arr_euclid  = []
            data_color  = mx_data[idx_y, idx_x] 
            for idx_c in range(6):
                dist_euclid   = cal_euclid(data_color, color_datum[idx_c])
                arr_euclid.append(dist_euclid)
            # -----
            color_id    = np.argmin(arr_euclid)
            if color_id == 0:
                mx_pics_datum_0[idx_y, idx_x] = 1.0
            elif color_id == 1:
                mx_pics_datum_1[idx_y, idx_x] = 1.0
            elif color_id == 2:
                mx_pics_datum_2[idx_y, idx_x] = 1.0
            elif color_id == 3:
                mx_pics_datum_3[idx_y, idx_x] = 1.0
            elif color_id == 4:
                mx_pics_datum_4[idx_y, idx_x] = 1.0
            elif color_id == 5:
                mx_pics_datum_5[idx_y, idx_x] = 1.0
            #print(idx_y, idx_x, data_color, arr_euclid)
    # ----------------
    # 1つのALLリストにまとめる
    mx_pics_ALL.append(mx_pics_datum_0)
    mx_pics_ALL.append(mx_pics_datum_1)
    mx_pics_ALL.append(mx_pics_datum_2)
    mx_pics_ALL.append(mx_pics_datum_3)
    mx_pics_ALL.append(mx_pics_datum_4)
    mx_pics_ALL.append(mx_pics_datum_5)

    return max_ypic, max_picx, mx_pics_ALL


#=================================================
# Calculation class(4) : Agent
#=================================================
class Agent(object):

    def __init__(self, SCREEN):

        # ----------------
        # 盤面データを読む
        self.mx_lm        = [[0, 2],[26, 25]]   # ゴール　元は[26, 25]
        self.arr_lmG      = self.mx_lm[1]
        self.maze_field   = maze_field
        self.start_point  = start_point
        self.goal_point   = goal_point
        # ----------------
        # マップ[3D]移動命令の種類
        self.arr_MPODR    = ["LEFT", "RIGHT", "UP", "DOWN", ]
        # -----------------
        # 記録用パラメタ類(プレイベース)
        self.arr_iplay     = []  # count game play    プレイ番号
        self.arr_maxturn   = []  # turn game play    ターン数
        self.arr_maxscore  = []  # score game play    報酬の総和
        self.arr_maxbump   = []  # bump game play    壁への衝突リスクの総和
        self.arr_distGoal  = []  # Goal distance    ゴール距離
        self.arr_sminRTA   = []  # minRT typeA     minRT距離
        self.arr_victory   = []  # victory    勝利したか
        self.arr_loss      = []  # DQN-Experience Replay学習
        self.arr_epsilon   = []  # ε-Greedy     
        # -------------------------
        # Q値の分析用(プレイベース)
        self.arr_maxQV     = []  # QV値の最大値
        self.arr_q25QV     = []  # QV値の4分の1値
        self.arr_q75QV     = []  # QV値の4分の3値
        # -------------------------
        # 距離の分析用累積リスト(プレイベース)
        self.arr_GLds      = []  # Goal距離
        self.arr_sminRTD   = []  # 空間用minRT距離(A)
        self.arr_sminRTBD  = []  # 空間用minRT距離(B)
        self.arr_sminRTCD  = []  # 空間用minRT距離(C)
        # -------------------------
        # 最終距離の分析用累積リスト(プレイベース)
        self.arr_endGoal   = []  # 最終ゴール距離      end_distGoal
        self.arr_endRTA    = []  # 最終minRT距離     end_distRTA

        # ---------------------------------
        # 基準色の種類
        # ---------------------------------
        color_datum_0   = [150, 150, 150]     # 灰色 -> 0
        color_datum_1   = [90, 90, 90]        # 暗灰色 -> 1
        color_datum_2   = [5, 5, 5]           # 黒色 -> 2
        color_datum_3   = [10, 10, 170]       # 青色 -> 3
        color_datum_4   = [170, 10, 10]       # 赤色 -> 4
        color_datum_5   = [10, 170, 10]       # 緑色 -> 5
        # -----
        self.color_datum  = []
        self.color_datum.append(color_datum_0)
        self.color_datum.append(color_datum_1)
        self.color_datum.append(color_datum_2)
        self.color_datum.append(color_datum_3)
        self.color_datum.append(color_datum_4)
        self.color_datum.append(color_datum_5)
        #print("color_datum:",self.color_datum)

        # ----------------
        # 共通変数
        CLOCK.tick(80)
        self.SCREEN     = SCREEN
        self.showMap    = True

        # ----------------
        # エピソードの実行
        for iCnt_play in range(NUMS_EPISODES):

            # カリキュラム学習
            self.mx_lm        = [[0, 2],[10, 9]]   # ゴール　元は[26, 25]
            self.arr_lmG      = self.mx_lm[1]

            # エピソードを実行する
            maxturn, maxbump, maxscore, flag_goal, end_distGoal, end_distRTA = self.get_episode(iCnt_play)
            # ========================================
            # DQN-Experience Replay学習  
            val_epsilon, val_loss = Dql3DMap_solver.replay_experience(BATCH_SIZE)
            # QValueリストを生成する
            val_maxQV = round(np.max(self.arr_predMapQV), 3)                 # QV値の最大値
            val_q25QV = round(np.percentile(self.arr_predMapQV, q=25), 3)    # QV値の4分の1値
            val_q75QV = round(np.percentile(self.arr_predMapQV, q=75), 3)    # QV値の4分の3値
            list_QVs  = [val_maxQV, val_q25QV, val_q75QV]
            # 学習結果の出力
            #if iCnt_play % 2 == 0:
            print("iPlay:{}, maxturn:{}, maxbump:{}, maxscore:{}, flag_goal:{}, end_distGoal:{}, end_distRTA:{}".format(iCnt_play, maxturn, maxbump, maxscore, flag_goal, end_distGoal, end_distRTA))
            # -----------------
            # 記録用パラメタ類(プレイベース)
            self.arr_iplay.append(iCnt_play)  # count game play    プレイ番号
            self.arr_maxturn.append(maxturn)  # turn game play    ターン数
            self.arr_maxscore.append(maxscore)  # score game play    報酬の総和
            self.arr_maxbump.append(maxbump)  # bump game play    壁への衝突リスクの総和
            self.arr_distGoal.append(end_distGoal)  # Goal distance    ゴール距離
            self.arr_sminRTA.append(end_distRTA)  # minRT typeA     minRT距離
            self.arr_victory.append(flag_goal)      # victory    勝利したか
            self.arr_loss.append(val_loss)          # DQN-Experience Replay学習
            self.arr_epsilon.append(val_epsilon)    # イプシロン       
            # ----------
            # Q値の保管(プレイベース)
            self.arr_maxQV.append(val_maxQV)        # QV値の最大値
            self.arr_q25QV.append(val_q25QV)        # QV値の4分の1値
            self.arr_q75QV.append(val_q75QV)        # QV値の4分の3値
            # ----------
            # 距離の保管(プレイベース)
            self.arr_GLds.append(np.mean(self.arr_Gdist))        # Goal距離の平均値
            self.arr_sminRTD.append(np.mean(self.arr_sminRT))       # 空間用minRT距離(A)の平均値
            self.arr_sminRTBD.append(np.mean(self.arr_sminRTB))       # 空間用minRT距離(B)の平均値
            self.arr_sminRTCD.append(np.mean(self.arr_sminRTC))       # 空間用minRT距離(C)の平均値
            # -----------------
            # 最終距離の分析用累積リスト(プレイベース)
            self.arr_endGoal.append(end_distGoal)  # 最終ゴール距離      end_distGoal
            self.arr_endRTA.append(end_distRTA)  # 最終minRT距離     end_distRTA
            # -----------------
            # しばらくすれば表示が消えます
            if iCnt_play%10 == 0:
                time.sleep(SLEEP_TIME)
                clear_output(wait=True)

        # ----------------
        # グラフ化
        # ----------------
        # 1ターンのみ、軌跡グラフを表示する
        # 壁の部分（赤色）を表示する
        mx_draw = []
        draw_size = int(27*WIDE/STEP)
        for iRow in range(draw_size):
            for jCol in range(draw_size):
                XDR_pos, YDR_pos = iRow*STEP, jCol*STEP
                dx, dy = math.floor(XDR_pos/WIDE), math.floor(YDR_pos/WIDE)
                if int(gameboard[dy][dx]) == 1:
                    mx_draw.append([XDR_pos, 1600-YDR_pos])
        # ----------------
        # 軌跡（青色）を表示する
        self.mx_DWroute = np.array(self.mx_DWroute)
        mx_draw         = np.array(mx_draw)
        # -----
        fig = plt.figure(figsize=(14, 12))
        ax0 = fig.add_subplot(2, 2, 1)
        ax0.set_title('RUNNING ROUTE : BLUE COLOR PLAYNO_{}'.format(NUMS_EPISODES))
        ax0.set_xlabel('X-Axis')
        ax0.set_ylabel('Y-Axis')
        ax0.scatter(self.mx_DWroute[:,0], self.mx_DWroute[:,1], label="ROUTE", color="blue")
        ax0.scatter(mx_draw[:,0], mx_draw[:,1], label="WALL", color="red")
        ax0.grid(True)
        ax0.legend(loc='best')
        # -----
        ax1 = fig.add_subplot(2, 2, 2)
        ax1.set_title('END GOAL Distance')
        ax1.set_xlabel('X-Axis')
        ax1.set_ylabel('Y-Axis')
        ax1.plot(self.arr_iplay, self.arr_endGoal, label="endGoal", color="blue")
        ax1.grid(True)
        ax1.legend(loc='best')
        # -----
        ax2 = fig.add_subplot(2, 2, 3)
        ax2.set_title('EPSILON')
        ax2.set_xlabel('X-Axis')
        ax2.set_ylabel('Y-Axis')
        ax2.plot(self.arr_iplay, self.arr_epsilon, label="EPSILON", color="blue")
        ax2.grid(True)
        ax2.legend(loc='best')
        # -----
        ax3 = fig.add_subplot(2, 2, 4)
        ax3.set_title('minRTA Distance')
        ax3.set_xlabel('X-Axis')
        ax3.set_ylabel('Y-Axis')
        ax3.plot(self.arr_iplay, self.arr_endRTA, label="endRTA", color="blue")
        ax3.grid(True)
        ax3.legend(loc='best')
        # -----
        fig.tight_layout()
        #fig.savefig("./AAC_img.png")
        plt.show()

        # ----------------
        # グラフを表示する
        self.show_graph()

        # ----------------
        # PyTorchモデルを保存する
        Dql3DMap_solver.save_models(file_output_3Dmodel)


    # ----------------
    # エピソードを運用する
    def get_episode(self, iCnt_play):

        # ----------------
        # 動画の設定(1)
        # fps = 12
        # movie_size = (640,640)
        # fourcc = cv2.VideoWriter_fourcc('I','4','2','0')#不同视频编码对应不同视频格式（例：'I','4','2','0' 对应avi格式）
        # ----------------
        # pygame共通変数
        self.BACKGROUND = makeBackground(ROWS,COLS)
        self.MAP        = makeMap(gameboard)
        self.MAPPOS     = pygame.Surface((COLS,ROWS))
        # ----------------
        # 記録用パラメタ類(ターンベース)
        self.arr_iturn    = []    # ターン・カウンタリスト
        self.arr_scores   = []    # ゲームスコアリスト
        self.arr_dones    = []    # ゲームオーバーフラグ
        # ----------------
        # [Map]記録用リストを追記する
        self.arr_acTypeMap = []    # 指示のタイプ
        self.arr_predMapQV = []    # Q値のリスト
        # ----------------
        # [Maze]記録用リストを追記する
        self.arr_MazeOrders = []     # 指示リスト(アドレス)
        self.arr_acTypeMaze = []     # 指示のタイプ
        self.arr_predMazeQV = []     # Q値のリスト
        # ----------------
        # 記録用パラメタ類(ターンベース)
        self.arr_Gdist    = []    # Goal距離
        self.arr_sminRT   = []    # 空間minRT距離
        self.arr_sminRTB  = []    # 空間用minRT距離(B)
        self.arr_sminRTC  = []    # 空間用minRT距離(C)
        # ----------------
        # 記録用パラメタ類(ターンベース)
        self.mx_BLUEcnt   = []    # 青(左-中央-右)
        self.mx_REDcnt    = []    # 赤(左-中央-右)
        self.arr_DWactions  = []    # 軌跡（ルート）を描くための命令データ
        self.mx_DWroute     = []    # 軌跡（ルート）を描くための座標データ

        # ----------------
        # 2種類の座標(MapとMaze)
        pos_lmS    = self.mx_lm[0]
        stateMaze_chess   = self.start_point
        stateMaze  = [0, 0, 0, 0, 0, 0, 0]
        next_stateMaze_chess   = self.start_point
        next_stateMaze = [0, 0, 0, 0, 0, 0, 0]
        stateMap_chess = [round(pos_lmS[0]*WIDE+WIDE/2), round(pos_lmS[1]*WIDE+WIDE/2)]
        stateMap   = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
        next_stateMap_chess = stateMap_chess
        next_stateMap = stateMap
        # ----------------
        arr_RTDists       = [0, 0, 0, 0, 0]
        next_arr_RTDists  = [0, 0, 0, 0, 0]
        movablesMaze      = [[0, 0],[0, 0]]
        next_movablesMaze = [[0, 0],[0, 0]]
        # ----------------
        # [MAZE]バックアップ
        self.last_ViewDirection     = math.pi/2
        self.last_ViewAngle         = 90
        self.last_actionMaze_chess  = self.start_point      # 指示リスト(アドレス)

        # ----------------
        # ゲームのリセット
        doneMap, flg_canGo, iCnt_turn    = False, True, 0
        sum_bump, sum_score, actionMap   = 0, 0, 'DOWN'
        # 2種類の状態(State)と距離(Dist)群を生成する
        ViewAngle, ViewDirection, stateMaze, stateMap, arr_RTDists = self.create_states(iCnt_turn, stateMaze_chess, stateMap_chess)
        # 移動可能な命令群
        movablesMap = ["DOWN", ]
        pos_lmG     = self.mx_lm[1]
        # ----------------
        # ゲームを実行する
        for iCnt_turn in range(NUMS_TURNS):

            # キーがインプットされました
            #time.sleep(0.01)
            # ----------------
            # 次の行動(ACTION)を発行する
            acTypeMap, actionMap, QvalueMap = Dql3DMap_solver.choose_actionMap(stateMap_chess, stateMap, movablesMap, pos_lmG)
            # 報酬(nominal_reward)を発行する
            nominal_reward, doneMap = maze_field.get_valMaze(stateMaze_chess)
            # 回転と前進に翻訳してコマを動かす
            ViewAngle, ViewDirection, next_stateMaze_chess, next_stateMaze, next_stateMap_chess, next_stateMap, next_arr_RTDists, flg_canGo = self.move_Maze_chess(stateMap_chess, actionMap, iCnt_turn)
            if flg_canGo == False:
                sum_bump = sum_bump + 1
            # ----------------
            # 移動可能な命令群
            if stateMaze_chess == self.start_point:
                next_movablesMap = ["DOWN", ]
            else:
                next_movablesMap = maze_field.get_ActionMaps(stateMap_chess, self.arr_MPODR)
            # ----------------
            if len(next_movablesMap) == 0:
                # movableに異常が発生しました(ABNORMAL)
                doneMap = True
                # 最終報酬の計算
                reward  = nominal_reward - 1.0*int(end_distGoal) - 1.0*int(end_distRTA) - sum_bump
                print("sudden break, reward:{}".format(reward))
                # ----------------
                # [Map]データを積み上げる
                Dql3DMap_solver.remember_memory(stateMap, actionMap, reward, next_stateMap, next_movablesMap, doneMap)
                break
            # ----------------
            # 記録用パラメタ類(ターンベース)
            Goal_Dist     = arr_RTDists[0]    # Goal距離
            dist_sminRT   = arr_RTDists[1]    # 空間minRT距離(A)
            dist_sminRTB  = arr_RTDists[2]    # 空間用minRT距離(B)
            dist_sminRTC  = arr_RTDists[3]    # 空間用minRT距離(C)
            # ----------------
            # パフォーマンス評価指標(reward)を発行する
            end_distGoal = round(Goal_Dist,3)
            end_distRTA  = round(dist_sminRT,3)
            # ----------------
            # [Map]記録用リストを追記する
            self.arr_iturn.append(iCnt_turn)    # ターン・カウンタリスト
            self.arr_acTypeMap.append(acTypeMap)    # 指示のタイプ
            self.arr_predMapQV.append(QvalueMap)    # Q値のリスト
            self.arr_DWactions.append(actionMap)    # 軌跡（ルート）を描くための命令データ
            self.mx_DWroute.append([stateMap_chess[1], 1600-stateMap_chess[0]])    # 軌跡（ルート）を描くための座標データ
            # ----------------
            # 記録用パラメタ類(ターンベース)
            self.arr_Gdist.append(Goal_Dist)       # Goal距離
            self.arr_sminRT.append(dist_sminRT)      # 空間minRT距離
            self.arr_sminRTB.append(dist_sminRTB)    # 空間用minRT距離(B)
            self.arr_sminRTC.append(dist_sminRTC)    # 空間用minRT距離(C)

            # ----------------
            # 動画の設定(2)
            #if iCnt_turn == 0:
            #    file_path = "./test_maze_play{}.mp4".format(iCnt_play)     #导出路径
            #    My_video = cv2.VideoWriter( file_path, fourcc, fps, movie_size )

            # ----------------
            if next_stateMaze_chess[0] >= self.arr_lmG[0]-3 and next_stateMaze_chess[1] >= self.arr_lmG[1]-3:
                # ゴールに到達しました(NORMAL)
                doneMap     = True
                sum_score   = sum_score + nominal_reward
                # ----------------
                # 最終報酬の計算
                reward  = nominal_reward + 15000 - 500*int(end_distGoal) - 500*int(end_distRTA) - sum_bump - 10*iCnt_play
                print("game over break, reward:{}".format(reward))
                # ----------------
                # [Map]データを積み上げる
                Dql3DMap_solver.remember_memory(stateMap, actionMap, reward, next_stateMap, next_movablesMap, doneMap)
                # ----------------
                # 動画設定(3)
                #My_video.release()         #释放
                break
            else:
                # その他の状態
                if iCnt_turn == NUMS_TURNS-1:
                    # ゴールに到達しました(ABNORMAL)
                    doneMap     = True
                    # 最終報酬の計算
                    reward  = nominal_reward + 8000 - 500*int(end_distGoal) - 500*int(end_distRTA) - sum_bump
                    print("sorry! no enoughgame time, reward:{}".format(reward))
                else:
                    # 中間段階
                    reward  = nominal_reward - 0.2*int(end_distGoal) - 0.1*int(end_distRTA) - 0.1*sum_bump
                # ----------------
                # [Map]データを積み上げる
                reward  = round(reward, 3)
                Dql3DMap_solver.remember_memory(stateMap, actionMap, reward, next_stateMap, next_movablesMap, doneMap)
                # ----------------
                # 動画設定(4)
                #imagestring = pygame.image.tostring(self.SCREEN.subsurface(0,0,640,640),"RGB")
                #pilImage = Image.frombytes("RGB", (640,640), imagestring)
                #img = cv2.cvtColor(np.asarray(pilImage),cv2.COLOR_RGB2BGR)
                #My_video.write(img)        #把图片写进视频
                # ----------------
                # 状態(STATE)表示
                if iCnt_turn%5 == 0:
                    print("iTurn:{}, Maze_ch:{}, Map_ch:{}, ViewAng:{}, reward:{}, arr_RTDists:{}".format(iCnt_turn, stateMaze_chess, stateMap_chess, ViewAngle, reward, arr_RTDists))
                # ----------------
                # カウントアップ
                stateMaze_chess = next_stateMaze_chess
                stateMaze   = next_stateMaze
                stateMap_chess  = next_stateMap_chess
                stateMap    = next_stateMap
                movablesMap = next_movablesMap
                arr_RTDists = next_arr_RTDists
                sum_score   = sum_score + nominal_reward

        # ----------------
        # エピソードの実行結果を出力する
        maxturn   = iCnt_turn
        maxbump   = int(sum_bump)
        maxscore  = int(sum_score)
        flag_goal = doneMap

        return maxturn, maxbump, maxscore, flag_goal, end_distGoal, end_distRTA


    # ----------------
    # 学習結果のグラフ化
    def show_graph(self):

        fig2 = plt.figure(figsize=(14, 10))
        # -----
        ax1 = fig2.add_subplot(2, 2, 1)
        ax1.set_title('DQN parameter transition: QValue')
        ax1.set_xlabel('#episode')
        ax1.set_ylabel('QValue')
        ax1.plot(self.arr_iplay, self.arr_maxQV, label="max_QV", color="blue")
        ax1.plot(self.arr_iplay, self.arr_q25QV, label="q25_QV", color="red")
        ax1.plot(self.arr_iplay, self.arr_q75QV, label="q75_QV", color="green")
        ax1.grid(True)
        ax1.legend(loc='best')
        # -----
        ax2 = fig2.add_subplot(2, 2, 2)
        ax2.set_title('learning transition : Learning loss')
        ax2.set_xlabel('#episode')
        ax2.set_ylabel('Loss amount')
        ax2.grid(True)
        ax2.plot(self.arr_iplay, self.arr_loss, label="loss", color="blue")
        ax2.legend(loc='best')
        # -----
        ax3 = fig2.add_subplot(2, 2, 3)
        # 移動平均を付ける
        y_rolling = pd.Series(self.arr_sminRTBD).rolling(window=12, center=True).mean()
        ax3.set_title('learning transition : SminRTD distance[limit-B]')
        ax3.set_xlabel('#episode')
        ax3.set_ylabel('distance')
        ax3.grid(True)
        ax3.plot(self.arr_iplay, self.arr_sminRTBD, label="limitB", color="blue")
        ax3.plot(self.arr_iplay, y_rolling, label="moving", color="red")
        ax3.legend(loc='best')
        # -----
        ax4 = fig2.add_subplot(2, 2, 4)
        # 移動平均を付ける
        y_rolling = pd.Series(self.arr_sminRTCD).rolling(window=12, center=True).mean()
        ax4.set_title('learning transition : SminRTD distance[limit-C]')
        ax4.set_xlabel('#episode')
        ax4.set_ylabel('distance')
        ax4.grid(True)
        ax4.plot(self.arr_iplay, self.arr_sminRTCD, label="limitC", color="blue")
        ax4.plot(self.arr_iplay, y_rolling, label="moving", color="red")
        ax4.legend(loc='best')
        # -----
        fig2.tight_layout()
        #fig.savefig("./AAC_img.png")
        plt.show()


    # -------------------------------
    # 2種類の状態(State)と距離(Dist)群を生成する
    def create_states(self, iCnt_turn, stateMaze_chess, stateMap_chess):

        # -------------------------------
        # スタート距離とゴール距離
        add_state = np.zeros(2)
        for i in range(2):
            pos_lm  = self.mx_lm[i]
            sqEuc   = (stateMaze_chess[0] - pos_lm[0])**2 + (stateMaze_chess[1] - pos_lm[1])**2
            distEuc = np.sqrt(sqEuc)
            add_state[i] = round(distEuc,5)
        # -------------------------------
        # 空間用minRTメトリックスを計算する
        # Aルート(標準)
        dist_sminRTA = space_minRT(mx_route, stateMaze_chess[0], stateMaze_chess[1])
        dist_sminRTA = round(dist_sminRTA,5)
        # Bルート(標準との差異分)
        dist_sminRTB = space_minRT(mx_routeB, stateMaze_chess[0], stateMaze_chess[1])
        dist_sminRTB = round(dist_sminRTB,5) - dist_sminRTA
        # Cルート(標準との差異分)
        dist_sminRTC = space_minRT(mx_routeC, stateMaze_chess[0], stateMaze_chess[1])
        dist_sminRTC = round(dist_sminRTC,5) - dist_sminRTA
        # -------------------------------
        # [2D,Maze用]STATEを生成する(7次元)
        stateMaze  = np.hstack([stateMaze_chess, add_state, [dist_sminRTA, dist_sminRTB, dist_sminRTC]])
        # -------------------------------
        # カメラのVIEW角度計算をする
        ViewAngle, ViewDirection, movablesMaze = self.calc_angleMaze(iCnt_turn, stateMaze, stateMaze_chess, stateMap_chess)
        # -------------------------------
        # 距離とりまとめリストを生成する
        arr_RTDists = np.zeros(4)
        arr_RTDists[0] = add_state[1]    # Goal距離
        arr_RTDists[1] = dist_sminRTA    # 空間minRT距離
        arr_RTDists[2] = dist_sminRTB    # 空間用minRT距離(B)
        arr_RTDists[3] = dist_sminRTC    # 空間用minRT距離(C)

        # -------------------------------
        # pygameへの表示
        self.SCREEN.fill((0,0,0))
        self.SCREEN.blit(pygame.transform.scale(self.BACKGROUND,(RES*2,RES*2)),(0,0))
        # -------------------------------
        bar = 0
        for i in FOV:
            rayDir = ViewDirection+i
            if (rayDir < 0): rayDir += 2*math.pi
            if (rayDir > 2*math.pi): rayDir -= 2*math.pi
            myRay = castRay(rayDir, stateMap_chess[1], stateMap_chess[0], ViewDirection,gameboard)
            if (myRay != False):
                dist = myRay[2]
                height  = round(PLAYERDISTANCE * HIGH/dist)
                top     = round((640 - height)/2)
                if (myRay[3] == "row"):
                    if ((myRay[5] == 1) and (myRay[6] == 0)):
                        self.SCREEN.blit(pygame.transform.scale(ROWBGN[myRay[4]],(2,height)), [bar,top, 2, height])
                    elif ((myRay[5] == (MAPSIZE*2)-1) and (myRay[6] == (MAPSIZE*2))):
                        self.SCREEN.blit(pygame.transform.scale(ROWEND[myRay[4]],(2,height)), [bar,top, 2, height])
                    else:
                        self.SCREEN.blit(pygame.transform.scale(ROWWLL[myRay[4]],(2,height)), [bar,top, 2, height])
                # -------------------------------
                else:
                    if ((myRay[5] == 0) and (myRay[6] == 1)):
                        self.SCREEN.blit(pygame.transform.scale(COLBGN[myRay[4]],(2,height)), [bar,top, 2, height])
                    elif ((myRay[5] == (MAPSIZE*2)) and (myRay[6] == (MAPSIZE*2)-1)):
                        self.SCREEN.blit(pygame.transform.scale(COLEND[myRay[4]],(2,height)), [bar,top, 2, height])
                    else:
                        self.SCREEN.blit(pygame.transform.scale(COLWLL[myRay[4]],(2,height)), [bar,top, 2, height])
            bar += 2
        # -------------------------------
        if (self.showMap):
            self.MAPPOS.blit(self.MAP,(0,0))
            pygame.draw.rect(self.MAPPOS, (0,255,0), [stateMaze_chess[1], stateMaze_chess[0], 1, 1], 0)
            self.SCREEN.blit(pygame.transform.scale(self.MAPPOS,(COLS*4,ROWS*4)),(0,0))
            pygame.draw.circle(self.SCREEN, (0,0,0), [RES,20], 20, 1)
            cenX, cenY = RES, 20
            edgX = cenX + (18 * math.cos(ViewDirection))
            edgY = cenY + (18 * math.sin(ViewDirection))
            pygame.draw.line(self.SCREEN, (0,255,0), [cenX,cenY], [edgX,edgY], 1)

        # ----------------
        # レンダリング更新
        pygame.display.update()
        # ----------------
        # 画像処理
        img_str = pygame.image.tostring(self.SCREEN, "RGB")
        # データ型式を変換する
        nparr   = np.frombuffer(img_str, np.uint8)
        img_np  = nparr.reshape(640,640,3)
        # イメージをカットする
        img_cut = img_np[115: 565, 0: 640]
        # モザイク処理、表示を行う。
        dst_small, dst_mosaic = mosaic(img_cut)
        # ----------------
        # 量子化処理を行う。
        max_ypic, max_picx, mx_pics_ALL = cal_quantization(dst_small, self.color_datum)
        mx_blue, mx_red = mx_pics_ALL[3], mx_pics_ALL[4]
        # ----------------
        # mx_data: (45, 64, 3)
        # L(左) : 0-19, C(中央) : 20-43, R(右) : 44-64
        # ----------------
        # 青色(BLUE)
        board_blue_L = np.array(mx_blue[:,:20]).flatten()
        board_blue_C = np.array(mx_blue[:,20:44]).flatten()
        board_blue_R = np.array(mx_blue[:,44:]).flatten()
        cc_L, cc_C, cc_R = Counter(board_blue_L), Counter(board_blue_C), Counter(board_blue_R)
        arr_BLUEcnt = [round(np.sqrt(cc_L[1.0]),2), round(np.sqrt(cc_C[1.0]),2), round(np.sqrt(cc_R[1.0]),2)]   
        # ----------------
        # 赤色(RED)
        board_red_L = np.array(mx_red[:,:20]).flatten()
        board_red_C = np.array(mx_red[:,20:44]).flatten()
        board_red_R = np.array(mx_red[:,44:]).flatten()
        cc_L, cc_C, cc_R = Counter(board_red_L), Counter(board_red_C), Counter(board_red_R)
        arr_REDcnt = [round(np.sqrt(cc_L[1.0]),2), round(np.sqrt(cc_C[1.0]),2), round(np.sqrt(cc_R[1.0]),2)]   
        # -------------------------------
        # [3D,Map用]STATEを生成する(13次元)
        stateMap   = np.hstack([stateMap_chess, arr_BLUEcnt, arr_REDcnt, add_state, [dist_sminRTA, dist_sminRTB, dist_sminRTC]])
        #print("stateMap: ", stateMap)
        # -------------------------------
        # 記録用パラメタ類(ターンベース)
        self.mx_BLUEcnt.append(arr_BLUEcnt)    # 青(左-中央-右)
        self.mx_REDcnt.append(arr_REDcnt)     # 赤(左-中央-右)
        
        return ViewAngle, ViewDirection, stateMaze, stateMap, arr_RTDists


    # -------------------------------
    # コマを動かす
    def move_Maze_chess(self, stateMap_chess, actionMap, iCnt_turn):

        # ----------------
        flg_canGo   = True
        # ----------------
        # DIRECTION / ROW / COL / ANGLE
        # LEFT / 0 / -1 / 180
        # RIGHT / 0 / 1 / 0
        # UP / -1 / 0 / 270
        # DOWN / 1 / 0 / 90
        # ----------------
        MapAngle = 0
        if actionMap == 'LEFT':
            MapAngle = 180
        elif actionMap == 'RIGHT':
            MapAngle = 0
        elif actionMap == 'UP':
            MapAngle = 270
        elif actionMap == 'DOWN':
            MapAngle = 90
        #print("move_Maze_chess - iCnt_turn: {}, MapAngle: {}".format(iCnt_turn, MapAngle))
        MapDirection = round(2*math.pi*MapAngle/360, 2)
        # ----------------
        # メイズのXY座標[CURR]
        MazeYpos = math.floor(stateMap_chess[0]/WIDE)   # メイズのY座標
        MazeXpos = math.floor(stateMap_chess[1]/WIDE)   # メイズのX座標
        stateMaze_chess = [MazeYpos, MazeXpos]
        # ----------------
        # コマを移動する
        tmpY = stateMap_chess[0] + (STEP * math.sin(MapDirection))
        tmpX = stateMap_chess[1] + (STEP * math.cos(MapDirection))
        if canGo(tmpX,tmpY,gameboard) == True or stateMaze_chess == self.start_point:
            next_stateMap_chess = [round(tmpY, 2), round(tmpX, 2)]
            flg_canGo   = True
        else:
            next_stateMap_chess = [-99, -99]
            flg_canGo   = False
        # ----------------
        # マップとメイズのXY座標[NEXT]
        next_MazeYpos = math.floor(next_stateMap_chess[0]/WIDE)   # メイズのY座標
        next_MazeXpos = math.floor(next_stateMap_chess[1]/WIDE)   # メイズのX座標
        next_stateMaze_chess = [next_MazeYpos, next_MazeXpos]
        # ----------------
        # 2種類の状態(State)と距離(Dist)群を生成する
        ViewAngle, ViewDirection, next_stateMaze, next_stateMap, next_arr_RTDists = self.create_states(iCnt_turn, next_stateMaze_chess, next_stateMap_chess)

        return ViewAngle, ViewDirection, next_stateMaze_chess, next_stateMaze, next_stateMap_chess, next_stateMap, next_arr_RTDists, flg_canGo


    # -------------------------------
    # [2D-Maze]カメラの角度計算をする
    def calc_angleMaze(self, iCnt_turn, stateMaze, stateMaze_chess, stateMap_chess):

        # バックアップ
        ViewDirection     = self.last_ViewDirection
        ViewAngle         = self.last_ViewAngle
        actionMaze_chess  = self.last_actionMaze_chess      # 指示リスト(アドレス)
        acTypeMaze      = "backup"      # 指示のタイプ
        QvalueMaze      = -0.111      # Q値のリスト

        # ----------------
        # 行動(ACTION)の候補を発行する
        movablesMaze = maze_field.get_MazeActions(stateMaze_chess)
        #print("calc_angleMaze - stateMaze_chess:{}, movablesMaze:{}".format(stateMaze_chess, movablesMaze))
        
        if movablesMaze != None:
        
            # 次の行動(ACTION)を発行する
            acTypeMaze, actionMaze_chess, QvalueMaze = Dqn2DMaze_solver.choose_actionMaze(stateMaze, movablesMaze)
            # ----------------
            # 次の行動(ACTION-８方向)に進む
            # action_number / row / col / 方向/ ベクトル
            # 0 / -1 / 0  / 上上(up-up) / [-1, 0]
            # 1 / -1 / -1  / 上左(up-left) / [-1, -1]
            # 2 / 0 / -1  / 左左(left-left) / [0, -1]
            # 3 / +1 / -1  / 左下(down-left) / [+1, -1]
            # 4 / +1 / 0  / 下下(down-down) / [+1, 0]
            # 5 / +1 / +1  / 下右(down-right) / [+1, +1]
            # 6 / 0  / +1 / 右右(right-right) / [0, +1]
            # 7 / -1  / +1 / 上右(up-right) / [-1, +1]
            # ----------------
            # 変則ONE-HOT化
            diff_row2D = actionMaze_chess[0] - stateMaze_chess[0]
            diff_col2D = actionMaze_chess[1] - stateMaze_chess[1]
            # ----------------
            # DIRECTION / ROW / COL / ANGLE
            # LEFT / 0 / -1 / 180
            # RIGHT / 0 / 1 / 0
            # UP / -1 / 0 / 270
            # DOWN / 1 / 0 / 90
            # ----------------
            # 0 / -1 / 0  / 上上(up-up) / [-1, 0]
            if diff_row2D < 0 and int(diff_col2D) == 0:
                ViewAngle = 270      # UP / -1 / 0 / 270 or -90
            # 1 / -1 / -1  / 上左(up-left) / [-1, -1]
            elif diff_row2D < 0 and diff_col2D < 0:
                ViewAngle = ( 270 + 180 ) * 0.5      # UP / -1 / 0 / 270 or -90, # LEFT / 0 / -1 / 180
            # 2 / 0 / -1  / 左左(left-left) / [0, -1]
            elif int(diff_row2D) == 0 and diff_col2D < 0:
                ViewAngle = 180       # LEFT / 0 / -1 / 180
            # 3 / +1 / -1  / 左下(down-left) / [+1, -1]
            elif diff_row2D > 0 and diff_col2D < 0:
                ViewAngle = ( 90 + 180 ) * 0.5      # LEFT / 0 / -1 / 180  # DOWN / 1 / 0 / 90
            # 4 / +1 / 0  / 下下(down-down) / [+1, 0]
            elif diff_row2D > 0 and int(diff_col2D) == 0:
                ViewAngle = 90        # DOWN / 1 / 0 / 90
            # 5 / +1 / +1  / 下右(down-right) / [+1, +1]
            elif diff_row2D > 0 and diff_col2D > 0:
                ViewAngle = ( 0 + 90 ) * 0.5     # DOWN / 1 / 0 / 90  # RIGHT / 0 / 1 / 0 or 360
            # 6 / 0  / +1 / 右右(right-right) / [0, +1]
            elif int(diff_row2D) == 0 and diff_col2D > 0:
                ViewAngle = 0    # RIGHT / 0 / 1 / 0
            # 7 / -1  / +1 / 上右(up-right) / [-1, +1]
            elif diff_row2D < 0 and diff_col2D > 0:
                ViewAngle = ( -90 + 0 ) * 0.5      # UP / -1 / 0 / 270 or -90  # RIGHT / 0 / 1 / 0 or 360
            # ----------------
            # ViewDirectionに変換する
            ViewDirection = 2*math.pi*ViewAngle/360
            if (ViewDirection < 0): ViewDirection = ViewDirection + 2*math.pi
            if (ViewDirection > 2*math.pi): ViewDirection = ViewDirection - 2*math.pi
            #print("iTurn:{}, ViewAngle:{}, actionMaze_chess:{}, stateMaze_chess:{}, movablesMaze:{}, done2D:{}".format(iCnt_turn, ViewAngle, actionMaze_chess, stateMaze_chess, movablesMaze, done2D))
            # ----------------
            # [Maze]記録用リストを追記する
            self.arr_MazeOrders.append(actionMaze_chess)      # 指示リスト(アドレス)
            self.arr_acTypeMaze.append(acTypeMaze)      # 指示のタイプ
            self.arr_predMazeQV.append(QvalueMaze)      # Q値のリスト
            # ----------------
            # バックアップ保管
            self.last_ViewDirection     = ViewDirection
            self.last_ViewAngle         = ViewAngle
            self.last_actionMaze_chess  = actionMaze_chess
            
        else:
        
            # バックアップ放出
            ViewDirection     = self.last_ViewDirection
            ViewAngle         = self.last_ViewAngle
            actionMaze_chess  = self.last_actionMaze_chess
        
        return ViewAngle, ViewDirection, movablesMaze

# -------------------------------
# メイン関数
if __name__ == '__main__':

    # [2D-Maze]入力用:Pytorchモデルのファイル名
    comment_input_2Dmodel = "initial"
    code_input_2Dmodel = "model_2DMaze_DQNER_{0}.pt".format(comment_input_2Dmodel)  # モデルの指定
    file_input_2Dmodel = foldername + code_input_2Dmodel  # ファイルパス名の生成
    # ----------
    # [3D-Map]入力用:Pytorchモデルのファイル名
    comment_input_3Dmodel = "initial"
    code_input_3Dmodel = "model_3DMaze_DQNER_{0}.pt".format(comment_input_3Dmodel)  # モデルの指定
    file_input_3Dmodel = foldername + code_input_3Dmodel  # ファイルパス名の生成
    # ----------
    # [3D-Map]出力用:Pytorchモデルのファイル名
    comment_output_3Dmodel = "initial"
    code_output_3Dmodel = "model_3DMaze_DQNER_{0}.pt".format(comment_output_3Dmodel)  # モデルの指定
    file_output_3Dmodel = foldername + code_output_3Dmodel  # ファイルパス名の生成

    # -------------------------------
    # Hyper Parameters[2D] : 画像(pygame)を動かすシステム
    stateMaze_size, actionMaze_size = 7, 2
    dim_input2D, dim_output2D = stateMaze_size + actionMaze_size, 1
    Dqn2DMaze_solver = DQN_Maze_Solver2D(stateMaze_size, actionMaze_size)
    # ----------
    # Hyper Parameters[3D] : コマを動かすシステム
    stateMap_size, actionMap_size = 13, 2
    dim_input3D, dim_output3D = stateMap_size + actionMap_size, 1
    Dql3DMap_solver  = DQN_Map_Solver3D(stateMap_size, actionMap_size)
    # ----------
    # Hyper Parameters
    SLEEP_TIME  = 0.01
    TARGET_REPLACE_ITER = 50           # ターゲットNet3D更新頻度
    BATCH_SIZE  = 256                  # サンプルサイズ
    # ----------
    # ORIGINAL-MAZE盤面を表示する
    MAPSIZE = 13    # ゲーム盤SIZEが27の場合にはMAPSIZE13になります
    maze = makeMaze(MAPSIZE,MAPSIZE)
    gameboard = gameBoard(maze)
    #print("--- gameboard ---")
    #print(gameboard)
    # ----------
    # 盤面のCSVファイルを読み込み表示する
    # mx_maze  : 学習用（すべての情報を含む）
    # mx_field : ゲーム用（0と１のみ）
    num_mzsize = MAPSIZE*2 + 1    # ゲーム盤SIZEが27の場合にはMAPSIZE13になります
    mx_maze, start_point, goal_point, mx_field = read_boardfile(num_mzsize)
    gameboard = mx_field

    # -------------------------------
    # 初めにMAZE盤面を表示する
    #route_display(gameboard)

    # -------------------------------
    # パラメタ設定
    NUMS_EPISODES   = 300
    NUMS_TURNS      = 350

    # -------------------------------
    # MAZEの実行
    COLS    = len(gameboard)
    ROWS    = len(gameboard[0])
    SCREEN  = pygame.display.set_mode((RES*2,RES*2))

    # -------------------------------
    # ゲームの実行
    Agent(SCREEN)

```

QEU：FOUNDER ; “次は、**第2段階として3ステップを動かしていマス・・・**。ROWをすこしづつ伸ばす段階なんですね。”

![imageRL6-11-2](https://reinforce.github.io/images/imageRL6-11-2.jpg) 

QEU：FOUNDER ; “それでは、第2段階の第1ステップの結果をドン・・・。”


**（パフォーマンス推移A-STEP1）**

![imageRL6-11-3](https://reinforce.github.io/images/imageRL6-11-3.jpg) 


**（パフォーマンス推移B-STEP1）**

![imageRL6-11-4](https://reinforce.github.io/images/imageRL6-11-4.jpg) 


D先生 : “一つのプログラムを使えば、思い通りに動いてくれる。う～ん、便利だ・・・。”

QEU：FOUNDER ; “途中、試行錯誤があったので簡単ではないがね・・・。それでは、第2段階の最終ステップである、STEP3を見てみましょう。”

**（パフォーマンス推移A-STEP3）**

![imageRL6-11-5](https://reinforce.github.io/images/imageRL6-11-5.jpg) 

**（パフォーマンス推移B-STEP3）**

![imageRL6-11-6](https://reinforce.github.io/images/imageRL6-11-6.jpg) 

D先生 ; “学習の過程はいろいろあるのでしょうが、最後の**軌跡がうまく動いています**ね。次は第3段階ですね。大きな手直しがないことを、お祈りしております(笑)。”



## ～　まとめ　～

QEU:FOUNDER ： “久々に、イケメンバトルをやろうか・・・。”

![imageRL6-11-7](https://reinforce.github.io/images/imageRL6-11-7.jpg) 

D部長: “〇〇ハン、よくやっとる・・・。”

<iframe width="560" height="315" src="https://www.youtube.com/embed/oCeLqGzUplc" ti-tle="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; en-crypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

QEU:FOUNDER ： “まずは、**食い物の改善**から始めたほうがいいんじゃない？”

![imageRL6-11-8](https://reinforce.github.io/images/imageRL6-11-8.jpg) 

QEU:FOUNDER ; “恨まれるよ・・・。”

