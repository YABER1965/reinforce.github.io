## QEUR21_2DMZ2:　６つの「ランドマーク」

## ～　より、「現実」にちかく　～

C部長 : “また自動搬送機（AGV）のプロジェクトを始めたと聞きました。”

D先生 ： “まあね。メイズなので、そうだとも言えます。それがなにか？”

C部長 : “実機に展開するとき、もっと精度を上げられませんか？うちにもAGVがあるんですが、動きがギクシャクしていて、いまいちです。たぶん、現在位置の誤差が蓄積するのだと思います。”

D先生 ： “おそらく、ベンチマークがないため起きた問題だと思います。メイズというゲームの**2次元(X,Y)空間は、あくまで仮想空間**ですからね。もし、**ベンチマーク**をつけると、以下の6か所になるのではないでしょうか。”

![imageRL4-3-1](https://reinforce.github.io/images/imageRL4-3-1.jpg)

QEU:FOUNDER  : “距離計を6か所配置してコマの位置を計測するのは、十分意味がありますね。あとは、この情報を入力して、強化学習が収束するか・・・。・・・なにはともあれ、やってみましょう。プログラムをドン！！“

```python
# ----------------
# メイズ(MAZE)ゲームの強化学習システム
# step2 : RTを動的メトリックスとして活用する
# step2_dqn_maze_pyTorch(6_landmark).py
# step2 : DQN-Experience replay
# ---------------- 
# 数値計算のﾗｲﾌﾞﾗﾘ読み込み
import numpy as np
import pandas as pd
import math, copy, random, time
from collections import deque, Counter
from scipy.special import softmax
from IPython.display import clear_output
# ----------------
import torch
import torch.nn.functional
import torch.utils.data
# ---------------- 
import matplotlib.pyplot as plt
#%matplotlib inline

# =================================================
# Game board difinition 
# =================================================
foldername  = "./MAZE_test/"
init_maze = [['#', 'S', '#', '#', '#', '#', '#', '#', '#', '#'], ['#', 0, '#', -1, 0, -1, 0, '#', '#', '#'], ['#', -1, '#', -1, -1, 0, -1, -1, 0, '#'], ['#', 0, -1, 0, 0, -1, 0, 0, '#', '#'], ['#', '#', -1, 0, 0, -1, '#', -1, 0, '#'], ['#', -1, 0, 0, -1, -1, 0, '#', -1, '#'], ['#', 0, 0, 0, -1, '#', -1, '#', -1, '#'], ['#', 0, -1, 0, -1, 0, -1, 0, -1, '#'], ['#', 0, -1, 0, 0, -1, 0, 0, -1, '#'], ['#', '#', '#', '#', '#', '#', '#', 50, '#', '#']]
mx_lm = [[0, 0],[0, 9],[9, 0],[9, 9],[0, 1],[9, 7]]

# --------------------------------------------------
# MAZE盤面をCSVファイルに保存する
def save_csvMAZE(maze): 
    # --------------------------------------------------
    # CSV出力用のデータフレームを作る(2)
    arr_columns = list(range(10))
    # -----
    df_csvout = pd.DataFrame(maze, columns=arr_columns)
    #print(df_csvout)
    
    # --------------------------------------------------
    # CSV ファイル (file_csvout) として出力
    code_csvout = "maze_board.csv"       # file name  
    file_csvout = foldername + code_csvout   # standard(training) file name   
    print("メトリックス保管用CSVファイル ：{0}".format(file_csvout))
    # -----
    df_csvout.to_csv(file_csvout, index=True)

# MAZE盤面をCSVファイルに保存する
#save_csvMAZE(init_maze)


# =================================================
# difinition of function
# =================================================
# 状態の表記を変更(STATE-6_landmarkへ)
def calc_address(a_row, a_col, size):

    state = np.zeros(6)
    for i in range(6):
        pos_lm  = mx_lm[i]
        sqEuc   = (a_row - pos_lm[0])**2 + (a_col - pos_lm[1])**2
        distEuc = np.sqrt(sqEuc)
        state[i] = round(distEuc,5)

    return state

# -------
# Maze Class
class Maze(object):
    def __init__(self, size=10, blocks_rate=0.1):
        self.size = size if size > 3 else 10
        self.blocks = int((size ** 2) * blocks_rate) 
        self.s_list = []
        self.maze_list = []
        self.e_list = []

    def create_mid_lines(self, k):
        if k == 0: self.maze_list.append(self.s_list)
        elif k == self.size - 1: self.maze_list.append(self.e_list)
        else:
            tmp_list = []
            for l in range(0,self.size):
                if l == 0: tmp_list.extend("#")
                elif l == self.size-1: tmp_list.extend("#")
                else:
                    a = random.randint(-1, 0)
                    tmp_list.extend([a])
            self.maze_list.append(tmp_list)

    def insert_blocks(self, k, s_r, e_r):
        b_y = random.randint(1, self.size-2)
        b_x = random.randint(1, self.size-2)
        if [b_y, b_x] == [1, s_r] or [b_y, b_x] == [self.size - 2, e_r]: k = k-1
        else: self.maze_list[b_y][b_x] = "#"
            
    def generate_maze(self): 
        s_r = random.randint(1, int((self.size / 2)) - 1)
        for i in range(0, self.size):
            if i == s_r: self.s_list.extend("S")
            else: self.s_list.extend("#")
        start_point = [0, s_r]

        e_r = random.randint(int((self.size / 2)) + 1, self.size - 2)
        for j in range(0, self.size):
            if j == e_r: self.e_list.extend([50])
            else: self.e_list.extend("#")
        goal_point = [self.size - 1, e_r]

        for k in range(0, self.size):
            self.create_mid_lines(k)
        
        for k in range(self.blocks):
            self.insert_blocks(k, s_r, e_r)

        return self.maze_list, start_point, goal_point

    # --------------------------------------------------
    # 盤面のCSVファイルを読み込み表示する
    def read_boardfile(self):
        # CSVファイルの読み込み
        code_csvout = "maze_board.csv"       # file name
        file_readcsv = foldername + code_csvout   # standard(training) file name  
        df = pd.read_csv(file_readcsv)
        #max_play = len(df)
        # print("データ量 max_play",max_play)
        # print(df)
        # --------------------------------------------------
        # 選択項目の読み込み
        maze = df.loc[:, "0":"9"].values.tolist()
        start_point = [0,0]
        goal_point  = [0,0]
        for i in range(size):
            for j in range(size):
                if maze[i][j] == 'S':
                    start_point = [i, j]
                if maze[i][j] == '50':
                    goal_point  = [i, j]

        self.maze_list   = maze
        self.start_point = start_point
        self.goal_point  = goal_point
        #print("----- mx_maze(read_boardfile) -----")
        #print(mx_maze)

        return self.maze_list, start_point, goal_point

# -------
# Maze functions
class Field(object):
    def __init__(self, maze, start_point, goal_point):
        self.maze = maze
        self.start_point = start_point
        self.goal_point = goal_point
        self.movable_vec = [[1,0],[-1,0],[0,1],[0,-1]]

    def display(self, point=None):
        field_data = copy.deepcopy(self.maze)
        if not point is None:
                y, x = point
                field_data[y][x] = "@@"
        else:
                point = ""
        for line in field_data:
                print ("\t" + "%3s " * len(line) % tuple(line))

    def get_actions(self, state):
        movables = []
        if state == self.start_point:
            y = state[0] + 1
            x = state[1]
            a = [[y, x]]
            return a
        else:
            for v in self.movable_vec:
                y = state[0] + v[0]
                x = state[1] + v[1]
                if not(0 < x < len(self.maze) and
                       0 <= y <= len(self.maze) - 1 and
                       maze[y][x] != "#" and
                       maze[y][x] != "S"):
                    continue
                movables.append([y,x])
            if len(movables) != 0:
                return movables
            else:
                return None

    def get_val(self, state):
        y, x = state
        if state == self.start_point: return 0, False
        else:
            v = float(self.maze[y][x])
            if state == self.goal_point: 
                return v, True
            else: 
                return v, False

# -------
# Generate a maze
size = 10
barriar_rate = 0.1

maze_1 = Maze(size, barriar_rate)
#maze, start_point, goal_point = maze_1.generate_maze()
maze, start_point, goal_point = maze_1.read_boardfile()
maze_field = Field(maze, start_point, goal_point)

maze_field.display()

#=================================================
# Deep Learning Model class            
#=================================================
# PyTorchのDLの定義
class Net(torch.nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = torch.nn.Linear(dim_input, 128)
        self.fc1.weight.data.normal_(0, 0.1)
        self.fc2 = torch.nn.Linear(128, 128)
        self.fc2.weight.data.normal_(0, 0.1)
        self.fc3 = torch.nn.Linear(128, 128)
        self.fc3.weight.data.normal_(0, 0.1)
        self.fc4 = torch.nn.Linear(128, dim_output)
        self.fc4.weight.data.normal_(0, 0.1)
        
    def forward(self, x):
        x = torch.nn.functional.relu(self.fc1(x))
        x = torch.nn.functional.relu(self.fc2(x))
        x = torch.nn.functional.relu(self.fc3(x))
        x = self.fc4(x)
        return x

# =======================================
# =======================================
# Solving the maze in Deep Q-learning
class DQN_Solver:
    def __init__(self, state_size, action_size):
        self.state_size = state_size # list size of state
        self.action_size = action_size # list size of action
        self.memory = deque(maxlen=10000) # memory space
        self.gamma = 0.96 # discount rate
        self.epsilon = 1.0 # randomness of choosing random action or the best one
        self.e_decay = 0.9995 # epsilon decay rate
        self.e_min = 0.01 # minimum rate of epsilon
        self.learning_rate = 0.001 # learning rate of neural network
        # --------------------------------------------------
        # crate instance for input
        self.eval_net, self.target_net = Net(), Net()                           # Netを利用して２つのニューラルネットをつくる
        self.learn_step_counter = 0    
        # --------------------------------------------------
        # Save and load the model via state_dict
        #self.eval_net.load_state_dict(torch.load(file_input_model))
        # --------------------------------------------------
        # set validaiton mode
        self.eval_net.eval()
        # --------------------------------------------------
        # set training parameters
        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=self.learning_rate)
        self.criterion = torch.nn.MSELoss()

    # ----------------
    # PyTorchモデルの保存  
    #def save_models(self):
        #torch.save(self.eval_net.state_dict(), file_output_model)

    # remember state, action, its reward, next state and next possible action. done means boolean for goal
    def remember_memory(self, state, action, reward, next_state, next_movables, done):
        self.memory.append((state, action, reward, next_state, next_movables, done))

    # choosing action depending on epsilon
    def choose_action(self, state, movables):
        acType = "random"
        Qvalue = -0.001
        T_Dist = -0.001
        #print("movables: ", movables)
        if self.epsilon >= random.random():
            # randomly choosing action
            action = random.choice(movables)
            return acType, action, Qvalue, T_Dist
        else:
            # choosing the best action from model.predict()
            acType, action, Qvalue, T_Dist = self.choose_best_action(state, movables)
            return acType, action, Qvalue, T_Dist
        
    # choose the best action to maximize reward expectation
    def choose_best_action(self, state, movables):

        action = [0,0]
        acType = "machine"
        Qvalue = -0.001
        iCnt = 0
        for a in movables:

            # ONEHOT状態(INPUT_STATE)を生成する
            input_state = calc_address(state[0], state[1], size)
            np_action = np.hstack([input_state, a])
            #print("np_action: ",np_action)
            if iCnt == 0:
                mx_input = [np_action]  # 状態(S)行動(A)マトリックス
            else:
                mx_input = np.append(mx_input, [np_action], axis=0)     # 状態(S)マトリックス      
            iCnt = iCnt + 1
        # print("----- mx_input -----")
        # print(mx_input)
        # --------------------------------------------------
        # generate new 'x'
        x_input_tensor = torch.tensor(mx_input).float()
        # predict 'y'
        with torch.no_grad():
            y_pred_tensor = self.eval_net(x_input_tensor)
        # convert tensor to numpy
        y_pred      = y_pred_tensor.data.numpy().flatten()
        Qvalue      = np.max(y_pred)
        temp_order  = np.argmax(y_pred)
        action      = movables[temp_order]

        return acType, action, Qvalue, input_state[4]+input_state[5]


    # this experience replay is going to train the model from memorized states, actions and rewards
    def replay_experience(self, batch_size):

        # ターゲットNetのパラメタを更新する
        if self.learn_step_counter < 5 or self.learn_step_counter % TARGET_REPLACE_ITER == 0:                  # 最初5回は毎回に更新し、あとは定期的に更新する
            self.target_net.load_state_dict(self.eval_net.state_dict())         # 評価NetのパラメタをターゲットNetに引き渡す
        self.learn_step_counter += 1   
        # --------------------------------------------------
        batch_size = min(batch_size, len(self.memory))
        minibatch = random.sample(self.memory, batch_size)
        X, Y = [], []
        # --------------------------------------------------
        for ibat in range(batch_size):
            state, action, reward, next_state, next_movables, done = minibatch[ibat]

            # ONEHOT状態(INPUT_STATE)を生成する
            input_state = calc_address(state[0], state[1], size)
            state_action_curr = np.hstack([input_state, action])
            #print("state_action_curr",state_action_curr)
            # ----------------
            if done:
                target_f = reward
            else:
                next_rewards = []
                iCnt = 0
                for a in next_movables:

                    # ONEHOT状態(INPUT_STATE)を生成する
                    input_next_state = calc_address(next_state[0], next_state[1], size)
                    np_next_s_a = np.hstack([input_next_state, a])
                    
                    if iCnt == 0:
                        mx_input = [np_next_s_a]  # 状態(S)行動(A)マトリックス
                    else:
                        mx_input = np.concatenate([mx_input, [np_next_s_a]], axis=0)     # 状態(S)マトリックス
                    
                    iCnt = iCnt + 1
                # ----------------
                # generate new 'x'
                x_input_tensor = torch.tensor(mx_input).float()
                # predict 'y'
                with torch.no_grad():
                    y_pred_tensor = self.target_net(x_input_tensor)
                # convert tensor to numpy
                y_pred = y_pred_tensor.data.numpy().flatten()
                np_n_r_max = np.amax(y_pred)
                target_f = reward + self.gamma * np_n_r_max
            # ----------------
            if ibat == 0:
                X = np.array([state_action_curr])  # 状態(S)行動(A)マトリックス
            else:
                X = np.concatenate([X, np.array([state_action_curr])], axis=0)  # 状態(S)行動(A)マトリックス
            Y = np.append(Y,target_f)

        # --------------------------------------------------
        # TRAINING
        # convert numpy array to tensor
        state_action_eval = torch.from_numpy(X).float()
        q_target = torch.from_numpy(Y.reshape(-1, 1)).float()
        #print("state_action_eval:",state_action_eval)
        #print("q_target:",q_target)

        # --- building model ---
        q_eval = self.eval_net(state_action_eval)
        # calculate loss
        loss = self.criterion(q_eval, q_target)
        # update weights
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        # Show progress
        #print('learn done -- [epsilon: {0}, loss: {1}]'.format(self.epsilon, loss))
        if self.epsilon > self.e_min:
            self.epsilon *= self.e_decay

        return round(self.epsilon,5) , round(loss.data.item(),5)


# ---------------------------
# パラメタの設定
SLEEP_TIME  = 0.01
TARGET_REPLACE_ITER = 100            # ターゲットNet更新頻度
state_size  = 6
action_size = 2
dim_input   = state_size + action_size
dim_output  = 1
dql_solver  = DQN_Solver(state_size, action_size)
BATCH_SIZE  = 128                    # サンプルサイズ

# number of episodes to run training
episodes = 3000

# number of times to sample the combination of state, action and reward
times    = 5000

# ---------------------------
# 出力用:Pytorchモデルのファイル名
#comment_output_model = "initial"
#code_output_model = "model_cliffwalk_DQNER_{0}.pt".format(comment_output_model)  # モデルの指定
#file_output_model = foldername + code_output_model  # ファイルパス名の生成

#=================================================
# Calculation class(3) : Agent
#=================================================
class Agent():
    # -----
    def __init__(self):

        # ---------------------------
        # 記録用パラメタ類(プレイベース)
        self.arr_iplay     = []  # count game play    プレイ番号
        self.arr_maxturn   = []  # turn game play    ターン数
        self.arr_maxscore  = []  # rl_score game play    報酬の総和
        self.arr_victory   = []  # victory    勝利したか
        self.arr_loss      = []  # DQN-Experience Replay学習
        self.arr_epsilon   = []  # ε-Greedy        
        # ---------------------------
        # Q値の分析用(プレイベース)
        self.arr_numQV     = []  # QV値のN数
        self.arr_maxQV     = []  # QV値の最大値
        self.arr_q25QV     = []  # QV値の4分の1値
        self.arr_q75QV     = []  # QV値の4分の3値
        # ---------------------------
        # 距離の分析用(プレイベース)
        self.arr_maxTD     = []  # Total距離の最大値

        # エピソードの実行
        for iCnt_play in range(episodes):

            maxturn, maxscore, flag_goal = self.get_episode(iCnt_play)
            # ----------
            # DQN-Experience Replay学習  
            val_epsilon, val_loss = dql_solver.replay_experience(BATCH_SIZE)
            # 学習結果の出力
            if iCnt_play % 5 == 0:
                print("iCnt_play:{0}, maxturn:{1}, maxscore:{2}, flag_goal:{3}, epsilon:{4}, loss:{5}".format(iCnt_play, maxturn, maxscore, flag_goal, val_epsilon, val_loss))

            # ----------
            # 記録用パラメタ類(プレイベース)の追加
            self.arr_iplay.append(iCnt_play)        # count game play    プレイ番号
            self.arr_maxturn.append(maxturn)        # max_turn   ゲームのターン数
            self.arr_maxscore.append(maxscore)      # rl_score game play    最終プレイスコア
            self.arr_victory.append(flag_goal)      # victory    勝利したか
            self.arr_loss.append(val_loss)          # DQN-Experience Replay学習
            self.arr_epsilon.append(val_epsilon)    # イプシロン       
            # ----------
            # Q値の保管(プレイベース)
            self.arr_numQV.append(maxturn)      # QV値のN数
            self.arr_maxQV.append(np.max(self.arr_predQV))                 # QV値の最大値
            self.arr_q25QV.append(np.percentile(self.arr_predQV, q=25))    # QV値の4分の1値
            self.arr_q75QV.append(np.percentile(self.arr_predQV, q=75))    # QV値の4分の3値
            # ----------
            # 距離の保管(プレイベース)
            self.arr_maxTD.append(np.max(self.arr_Tdist))              # Total距離の最大値
  
            # ----------
            # しばらくすれば表示が消えます
            if iCnt_play%50 == 0:
                time.sleep(SLEEP_TIME)
                clear_output(wait=True)

        # --------------------------------------------------
        # 学習履歴を出力する
        fig = plt.figure(figsize=(14, 6))
        ax1 = fig.add_subplot(1, 2, 1)
        ax1.set_title('learning transition : epsilon')
        ax1.plot(self.arr_iplay, self.arr_epsilon, label="epsilon", color="blue")
        ax1.set_xlabel('#episode')
        ax1.set_ylabel('epsilon')
        ax1.legend(loc='best')
        # -----
        ax2 = fig.add_subplot(1, 2, 2)
        ax2.set_title('learning transition : Learning loss')
        ax2.set_xlabel('#episode')
        ax2.set_ylabel('Loss amount')
        ax2.plot(self.arr_iplay, self.arr_loss, label="loss", color="blue")
        ax2.legend(loc='best')
        plt.show()

        # --------------------------------------------------
        # グラフを表示する
        self.show_graph()

    # --------------------------------------------------
    # エピソードを運用する
    def get_episode(self, iCnt_play):

        # ---------------------------
        # 記録用パラメタ類(ターンベース)
        self.arr_iturn     = []    # ターン・カウンタリスト
        self.arr_orders    = []    # 指示リスト(アドレス)
        self.arr_acType    = []    # 指示のタイプ
        self.arr_scores    = []    # ゲームスコアリスト
        self.arr_dones     = []    # ゲームオーバーフラグ
        self.arr_predQV    = []    # Q値のリスト
        # ---------------------------
        # 記録用パラメタ類(ターンベース)
        self.arr_Tdist     = []    # Total距離

        # ---------------------------
        # 初期化
        state = start_point
        score = 0
        for time in range(times):
            movables = maze_field.get_actions(state)
            acType, action, Qvalue, T_Dist = dql_solver.choose_action(state, movables)
            reward, done = maze_field.get_val(action)
            score = score + reward
            next_state = action
            next_movables = maze_field.get_actions(next_state)
            dql_solver.remember_memory(state, action, reward, next_state, next_movables, done)
            if done or time == (times - 1):
                break
            state = next_state

            # ---------------------------
            # 記録用リストを追記する
            self.arr_iturn.append(time)    # ターン・カウンタリスト
            self.arr_orders.append(action)      # 指示リスト(アドレス)
            self.arr_acType.append(acType)      # 指示のタイプ
            self.arr_scores.append(reward)      # ゲームスコアリスト   
            self.arr_dones.append(done)         # ゲームオーバーフラグ
            self.arr_predQV.append(Qvalue)      # Q値のリスト
            # ---------------------------
            # 記録用パラメタ類(ターンベース)
            self.arr_Tdist.append(T_Dist)       # Total距離

        # ---------------------------
        # 結果の出力
        maxturn   = time
        maxscore  = score
        flag_goal = done

        return maxturn, maxscore, flag_goal


    # ----------------
    # 学習結果のグラフ化
    def show_graph(self):

        fig2 = plt.figure(figsize=(14, 12))
        # -----
        ax1 = fig2.add_subplot(2, 2, 1)
        ax1.set_title('DQN parameter transition: QValue')
        ax1.set_xlabel('#episode')
        ax1.set_ylabel('QValue')
        ax1.plot(self.arr_iplay, self.arr_maxQV, label="max_QV", color="blue")
        ax1.plot(self.arr_iplay, self.arr_q25QV, label="q25_QV", color="red")
        ax1.plot(self.arr_iplay, self.arr_q75QV, label="q75_QV", color="green")
        ax1.grid(True)
        ax1.legend(loc='best')
        # -----
        # 移動平均を付ける
        y_rolling = pd.Series(self.arr_maxTD).rolling(window=12, center=True).mean()
        ax2 = fig2.add_subplot(2, 2, 2)
        ax2.set_title('learning transition : distance')
        ax2.set_xlabel('#episode')
        ax2.set_ylabel('maxturn')
        ax2.grid(True)
        ax2.plot(self.arr_iplay, self.arr_maxTD, label="original", color="blue")
        ax2.plot(self.arr_iplay, y_rolling, label="moving", color="red")
        ax2.legend(loc='best')
        # -----
        # 移動平均を付ける
        y_rolling = pd.Series(self.arr_maxturn).rolling(window=12, center=True).mean()
        ax3 = fig2.add_subplot(2, 2, 3)
        ax3.set_title('learning transition : maxturn')
        ax3.set_xlabel('#episode')
        ax3.set_ylabel('maxturn')
        ax3.grid(True)
        ax3.plot(self.arr_iplay, self.arr_maxturn, label="original", color="blue")
        ax3.plot(self.arr_iplay, y_rolling, label="moving", color="red")
        ax3.legend(loc='best')
        # -----
        # 移動平均を付ける
        y_rolling = pd.Series(self.arr_maxscore).rolling(window=12, center=True).mean()
        ax4 = fig2.add_subplot(2, 2, 4)
        ax4.set_title('learning transition : maxscore')
        ax4.set_xlabel('#episode')
        ax4.set_ylabel('maxscore')
        ax4.grid(True)
        ax4.plot(self.arr_iplay, self.arr_maxscore, label="original", color="blue")
        ax4.plot(self.arr_iplay, y_rolling, label="moving", color="red")
        ax4.legend(loc='best')
        # -----
        fig2.tight_layout()
        #fig.savefig("./AAC_img.png")
        plt.show()


# ---------------------------
# ゲームを実行する
Agent()

```

QEU:FOUNDER  : “前回らかの改造が、すごく少ないのがいいね・・・。ここで、パフォーマンスとして**トータル距離**をとっていますが、これはスタート距離とゴール距離の合計です。 でも・・・、ちょっとびっくりしない？“

**（学習曲線）**

![imageRL4-3-2](https://reinforce.github.io/images/imageRL4-3-2.jpg)

**（パフォーマンス推移）**

![imageRL4-3-3](https://reinforce.github.io/images/imageRL4-3-3.jpg)

D先生 ： “パフォーマンスが前回の100次元の状態次元の実験と、ほとんど変わりません。驚いた・・・。”

C部長 : “これにRT距離を追加すると、もっと良くなるのかなぁ・・・。”

D先生 ： “これからは、画像認識以外ではRT距離を使わないって・・・。”

QEU;FOUNDER ： “RT距離に近い考え方のメトリックスを新しく導入します。”

## ～　まとめ　～

### ・・・　前回のつづきです　・・・

QEU:FOUNDER ： “あの国はある意味、「究極の自由社会」だからね。悪い方向に循環すると歯止めがかからないんですよ。その意味で**生活安全保障のしくみ**って、経済にとって必要なんですよ。お偉い方は自己責任で不要だとおもっているんでしょうが・・・。“

![imageRL4-3-4](https://reinforce.github.io/images/imageRL4-3-4.jpg)

D先生 : “お偉い方は、いまは大変ですから。こんな具合に・・・（笑）。”

![imageRL4-3-5](https://reinforce.github.io/images/imageRL4-3-5.jpg)

QEU:FOUNDER ： “この大先生を見ると、おもわず**「エズ様」を思い出す**よね。”

D先生 : “なんたって、**出現した時期がほとんど同じ**だから・・・。”

![imageRL4-3-6](https://reinforce.github.io/images/imageRL4-3-6.jpg)

D先生 ： “久々に、このお方の名作の書評を読んでみましょう。”

![imageRL4-3-7](https://reinforce.github.io/images/imageRL4-3-7.jpg)

### Received as a First Read via GoodReads giveaways and St. Martin's Press for an unbiased review.

### GoodReads giveawaysとSt. Martin's Pressにより、公平な書評として評価された。

Ezarti's hypothesis on aging demographics and the place of the world's super powers in the modern global power struggle is just that, a series of interesting considerations, but rife with what can only be upper class ig-norance to realities of the working populations and its poor.
高齢化する人口統計に関するエズさんの仮説と、現代の世界的な権力闘争における世界の超大国の位置は興味深い考察ですが、労働者の貧困の現実に対する上流階級の無知でしかあり得ないものに満ちています。

To be fair, the ideas are based on proven theories, and there are various examples of Ezrati's beliefs. What does Ezrati believe? That in the future our aging demographics will put extreme strain on the economic wel-fare of today's super powers which will cause over working of the younger generation and that the super pow-ers will need to make major changes in production, economic behaviours and political agendas.
公平に言うと、彼のアイデアは実証済みの理論に基づいており、エズさんの信念のさまざまな例があります。エズさんは何を信じているのか？将来、私たちの高齢化は、今日の超大国の経済的福祉に極端な負担をかけることになり、若い世代の働きすぎを引き起こし、超大国は生産、経済、政治的動向に大きな変化をもたらすでしょう。

If the reader is looking for a extremely middle or upper class hypothesis, this book is most definitely for them. The author notes that changing from farm work to office work is a huge leap in skills, but would be possible. Ezrati fails to consider that many of these people do not have the skills or proclivity to change their way of life, and stating that the workforce will be "reabsorbed" in the new job market is extremely narrow minded. Ezrati notes that retail and mall are taking in these displaced citizens, playing on this fact as if it were a positive change for the hard working exurbanite population. 
読者が中流階級または上流階級好みの仮説を探している場合、この本は間違いなく彼らのためのものです。農作業から事務作業への変更はスキルの大きな飛躍であるが、エズさんは可能であると述べています。エズさんは、これらの人々の多くは自分たちの生活様式を変えるスキルや性向を持っていないと考えず、労働力は新しい雇用市場に「再吸収」されると非常に狭い視野で述べています。エズさんは、小売店やショッピングモールがこれらの難民を受け入れており、あたかもそれが勤勉な郊外の団地族にとって前向きな変化であるかのようにこの事実に取り組んでいると述べています。

The author fails to consider that retail jobs are much lower wages, lower standard of living and require a com-pletely different set of skills. Where Ezrati sees a harmless change in venues, I see an increase in human suf-fering, lifestyle changes and morale slumping. Not everyone wants to work minimum wage, indoors, long hours and no sense of progress in their lives. Ezrati has obviously never taken a walk on the lower end of the career spectrum.
著者は、小売業ははるかに低い賃金、低い生活水準であり、まったく異なるスキルのセットを必要とするとは考えていません。エズさんが世の中の変化は無害と見ているのに対し、私は人間の苦しみ、ライフスタイルの変化、士気の落ち込みが増しているのがわかります。誰もが最低賃金、屋内、長時間労働を望んでいるわけではなく、自分たちの生活に進歩感がありません。エズさんは、明らかにキャリアの下限を歩いたことはありません。

If we ignore the obvious privileged background of the author, the book works on sound bases )historical refer-ences, basic economic theories and terms). And the concern that the aging demographics will not be replaced by a younger workforce, in turn developing an employment vacuum and pushing retirement years into our golden years. This book raises legitimate concerns and adeptly explains the repercussions of an aging popula-tion, its just unfortunately written in a way that I fear is not looking at the situation objectively outside current social class assumptions.
著者の明らかな特権的背景を無視すると、本は健全な根拠（歴史的参考文献、基本的な経済理論および用語）に基づいています。そして、高齢化する人口統計が若い労働力に取って代わられることはなく、その結果、雇用の空白が生じ、私たち働き盛りを退職に押しやることになることを懸念しています。この本はこの懸念を提起し、高齢化の影響を適切に説明しています。残念ながら、現在の社会階級の想定外の状況を客観的に見ないように書かれています。

This book does what it set out to do, kudos! I just wish it was more sensitive to the plight of those who arent as well established or economically comfortable prior to the initial changes. (less)
この本は、これからやろうとしていることを書いてあるのです！私は、それらの準備された、経済的に快適なモノに対してだけでなく、放棄された人々の窮状に対してもっと敏感になればいいのにと思います。 

QEU:FOUNDER ： “いつ読んでもすごい本（書評）だね・・・。”

C部長 : “この、エズさんの提案は実現するのでしょうか？”

QEU:FOUNDER ： “それが「正しい」ことであれば、実現するのではないでしょうか・・・。正しいことであるには、立場が変わったとしても、同じく「良いことに見える」必要があります。例えば、ある人がスポーツイベントで「無理なお金の稼ぎ方」をしていたとして、その人がエラい時にはやって良いことで、彼がエラくなくなった時には悪いという理屈はあり得ません。”

