## QEUR21_MZPREP3: A3Cスキームでの開発はここまでか？

## ～　「力（知恵）がおよばす」か・・・　～

### ・・・　こだわってみたい人は、自分で開発を続けてください　・・・

QEU:FOUNDER ; “あまりうまく言っていない**A3C(Asynchronous Advantage Actor Critic)による強化学習**だが、メトリックスを追加して再度リベンジしましょう。。”

D先生 : “メトリックスをたった2件を追加しただけじゃ、なんだか心もとない・・・。”

![imageRL5-4-1](https://reinforce.github.io/images/imageRL5-4-1.jpg) 

QEU:FOUNDER ; “・このメトリックスをフィードバックのロジックに追加するんです。”

D先生 : “いままでの標準（最短）ルートからのminRT距離によるコントロールとはちがうんですか？”

![imageRL5-4-2](https://reinforce.github.io/images/imageRL5-4-2.jpg) 

QEU:FOUNDER ; “センターラインに加えて、**路側帯がついた**と思えばよろしい・・・（笑）。”


D先生 : “なるほど・・・。で・・・、うまく行ったんですか？”

![imageRL5-4-3](https://reinforce.github.io/images/imageRL5-4-3.jpg) 

QEU：FOUNDER ; “先に結論をいえば、**「発散もせず」かつ「収束もせず」**・・・。”

D先生 : “それはいいのか、それとも悪いのか…。”

QEU：FOUNDER ; “それではプログラムをドン！！”

```python
# ----------------
# メイズ(MAZE)ゲームの強化学習システム
# step3D : 3Dメイズの準備段階です(UPDATE)
# step3D : reinforce_3DMazePrep_pyTorch(A3C_sminRT_size27).py
# step3D : A3CとsminRT距離ｘ3を導入する(フィードバック手法をレベルアップしました)
# ---------------- 
# 数値計算のﾗｲﾌﾞﾗﾘ読み込み
import numpy as np
import pandas as pd
import math, copy, random, time
# -----
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable
# -----
import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = (16, 10)

# =================================================
# Game board difinition 
# =================================================
foldername  = "./MAZE_test/"

# =================================================
# difinition of STATE CALCULATION function
# =================================================
# タイムライン用minRTメトリックスを計算する
def timeline_minRT(a_row, a_col, iCnt_turn):
    len_route = len(mx_route)
    if iCnt_turn <= len_route - 1:
        pos_rt  = mx_route[iCnt_turn]
    else:
        pos_rt  = mx_route[len_route - 1]
    # -----
    tlEuc   = (a_row - pos_rt[0])**2 + (a_col - pos_rt[1])**2
    dist_tlminRT = np.sqrt(tlEuc)
        
    return round(dist_tlminRT,5)

# -------
# 空間用minRTメトリックスを計算する
def space_minRT(pos_route, a_row, a_col):
    len_route = len(pos_route)
    arr_sminRT = []
    for i in range(len_route):
        pos_rt  = pos_route[i]
        sqEuc   = (a_row - pos_rt[0])**2 + (a_col - pos_rt[1])**2
        distEuc = np.sqrt(sqEuc)
        arr_sminRT.append(round(distEuc,5))   
    dist_sminRT = np.min(arr_sminRT)
        
    return dist_sminRT

# -------
# 状態の表記を変更(XY座標へ)、空間用minRTメトリックスつき
def calc_address(a_row, a_col, num_bump, mx_lm, iCnt_turn):

    add_state = np.zeros(2)
    for i in range(2):
        pos_lm  = mx_lm[i]
        sqEuc   = (a_row - pos_lm[0])**2 + (a_col - pos_lm[1])**2
        distEuc = np.sqrt(sqEuc)
        add_state[i] = round(distEuc,5)
             
    # 空間用minRTメトリックスを計算する
    # Aルート
    dist_sminRTA = space_minRT(mx_route, a_row, a_col)
    dist_sminRTA = round(dist_sminRTA,5)
    # Bルート(標準との差異分)
    dist_sminRTB = space_minRT(mx_routeB, a_row, a_col)
    dist_sminRTB = round(dist_sminRTB,5) - dist_sminRTA
    # Cルート(標準との差異分)
    dist_sminRTC = space_minRT(mx_routeC, a_row, a_col)
    dist_sminRTC = round(dist_sminRTC,5) - dist_sminRTA
    
    # タイムライン用minRTメトリックスを計算する
    dist_tlminRT = timeline_minRT(a_row, a_col, iCnt_turn)

    # STATEを生成する
    state = np.hstack([[a_row, a_col, num_bump], add_state, [dist_sminRTA, dist_sminRTB, dist_sminRTC]])
    
    return state, dist_tlminRT

# ----------
# Maze Class
class Maze(object):
    def __init__(self, size=27, blocks_rate=0.1):
        self.size = size if size > 3 else 27
        self.blocks = int((size ** 2) * blocks_rate) 
        self.s_list = []
        self.maze_list = []
        self.e_list = []

    def create_mid_lines(self, k):
        if k == 0: self.maze_list.append(self.s_list)
        elif k == self.size - 1: self.maze_list.append(self.e_list)
        else:
            tmp_list = []
            for l in range(0,self.size):
                if l == 0: tmp_list.extend("#")
                elif l == self.size-1: tmp_list.extend("#")
                else:
                    a = random.randint(-1, 0)
                    tmp_list.extend([a])
            self.maze_list.append(tmp_list)

    def insert_blocks(self, k, s_r, e_r):
        b_y = random.randint(1, self.size-2)
        b_x = random.randint(1, self.size-2)
        if [b_y, b_x] == [1, s_r] or [b_y, b_x] == [self.size - 2, e_r]: k = k-1
        else: self.maze_list[b_y][b_x] = "#"
            
    def generate_maze(self): 
        s_r = random.randint(1, int((self.size / 2)) - 1)
        for i in range(0, self.size):
            if i == s_r: self.s_list.extend("S")
            else: self.s_list.extend("#")
        start_point = [0, s_r]

        e_r = random.randint(int((self.size / 2)) + 1, self.size - 2)
        for j in range(0, self.size):
            if j == e_r: self.e_list.extend([50])
            else: self.e_list.extend("#")
        goal_point = [self.size - 1, e_r]

        for k in range(0, self.size):
            self.create_mid_lines(k)
        
        for k in range(self.blocks):
            self.insert_blocks(k, s_r, e_r)

        return self.maze_list, start_point, goal_point

    # --------------------------------------------------
    # 盤面のCSVファイルを読み込み表示する
    def read_boardfile(self):
        # CSVファイルの読み込み
        code_csvout = "maze_board27.csv"       # file name
        file_readcsv = foldername + code_csvout   # standard(training) file name  
        df = pd.read_csv(file_readcsv)
        #max_play = len(df)
        # print("データ量 max_play",max_play)
        # print(df)
        # --------------------------------------------------
        # 選択項目の読み込み
        maze = df.loc[:, "0":"26"].values.tolist()
        start_point = [0,0]
        goal_point  = [0,0]
        for i in range(size):
            for j in range(size):
                if maze[i][j] == 'S':
                    start_point = [i, j]
                if maze[i][j] == '5000':
                    goal_point  = [i, j]

        self.maze_list   = maze
        self.start_point = start_point
        self.goal_point  = goal_point
        print("goal_point", goal_point)
        print("----- mx_maze(read_boardfile) -----")
        print(maze)

        return self.maze_list, start_point, goal_point

    # --------------------------------------------------
    # コマの最短ルートのCSVファイルを読み込み表示する（標準Aルート）
    def read_chessfile(self):
        # CSVファイルの読み込み
        code_csvout = "chess_route27.csv"       # file name
        file_readcsv = foldername + code_csvout   # standard(training) file name  
        df = pd.read_csv(file_readcsv)
        #max_play = len(df)
        # print("データ量 max_play",max_play)
        # print(df)
        # --------------------------------------------------
        # 選択項目の読み込み
        mx_route    = df.loc[:, "row":"col"].values.tolist()
        ref_action  = df.loc[:, "action"].values.tolist()
        print("ref_action: ", ref_action)
        #print("----- route -----")
        #print(route)

        return mx_route, ref_action

    # --------------------------------------------------
    # コマの最短ルートのCSVファイルを読み込み表示する（限界BCルート）
    def read_chessfileBC(self, code_csvout):

        # CSVファイルの読み込み
        file_readcsv = foldername + code_csvout   # standard(training) file name  
        df = pd.read_csv(file_readcsv)
        #max_play = len(df)
        # print("データ量 max_play",max_play)
        # print(df)
        # --------------------------------------------------
        # 選択項目の読み込み
        mx_route    = df.loc[:, "row":"col"].values.tolist()
        #print("----- route -----")
        #print(route)

        return mx_route

# ----------
# Maze functions
class Field(object):
    def __init__(self, maze, start_point, goal_point):
        self.maze = maze
        self.start_point = start_point
        self.goal_point  = goal_point
        self.movable_vec = [[-1,0],[1,0],[0,-1],[0,1]]

    def display(self, point=None):
        field_data = copy.deepcopy(self.maze)
        if not point is None:
                y, x = point
                field_data[y][x] = "@@"
        else:
                point = ""
        print("================")
        for line in field_data:
                print ("\t" + "%3s " * len(line) % tuple(line))
        print("================")

    def all_display(self, mx_route):
        field_data = copy.deepcopy(self.maze)
        for i in range(len(mx_route)):
            point = mx_route[i]
            y, x = point[0], point[1]
            field_data[y][x] = "@@"
        print("================")
        for line in field_data:
                print ("\t" + "%3s " * len(line) % tuple(line))
        print("================")

    def get_actions(self, state):
        movables = []
        if state == self.start_point:
            y = state[0] + 1
            x = state[1]
            a = [[y, x]]
            return a
        else:
            for v in self.movable_vec:
                y = state[0] + v[0]
                x = state[1] + v[1]
                if not(0 < x < len(self.maze) and
                       0 <= y <= len(self.maze) - 1 and
                       self.maze[y][x] != "#" and
                       self.maze[y][x] != "S"):
                    continue
                movables.append([y,x])
            if len(movables) != 0:
                return movables
            else:
                return None

    # --------------------------------------------------
    # 次のコマの「決定」位置と報酬、ゲームフラグを決める
    # 新概念：マスク「#」位置に着くと、スタート地点に戻る
    def get_val(self, action_chess, state_chess, movables):
        yPos = action_chess[0]
        xPos = action_chess[1]
        # ----------------
        if action_chess == self.start_point:
            nxt_state = action_chess
            return nxt_state, 0, False, 0
        elif self.maze[yPos][xPos] == "#":
            #print("movables: ", movables)
            if len(movables) > 0:
                nxt_state = random.choice(movables)
            else:
                nxt_state = self.start_point
            v = -0.001        # -10
            return nxt_state, v, False, 1
        else:
            nxt_state = action_chess
            v = float(self.maze[yPos][xPos])
            if action_chess == self.goal_point: 
                return nxt_state, v, True, 0
            else: 
                return nxt_state, v, False, 0

    # --------------------------------------------------
    # 次のコマの位置を決める
    def nxtPosition(self, state_chess, action):
        # ----------------
        # 次の行動(ACTION)に進む
        # action number / row / col / 方向 / onehot
        # 0 / -1 / 0  / 上(up) / 1000
        # 1 / +1 / 0  / 下(down) / 0100
        # 2 / 0  / -1 / 左(left) / 0010
        # 3 / 0  / +1 / 右(right) / 0001
        # ----------------
        # ACTION_CHESS
        if action == 0:
            diff_row, diff_col = -1, 0
        if action == 1:
            diff_row, diff_col = 1, 0 
        if action == 2:
            diff_row, diff_col = 0, -1 
        if action == 3:
            diff_row, diff_col = 0, 1 
        # ----------------
        nxtPos    = [0,0]
        nxtPos[0] = state_chess[0] + diff_row   # y方向
        nxtPos[1] = state_chess[1] + diff_col   # x方向

        return nxtPos

# ----------
# Initialize and Generate a maze
size = 27
barriar_rate = 0.1
maze_1 = Maze(size, barriar_rate)
# init_maze : カリキュラム学習用に改造
init_maze, start_point, goal_point = maze_1.read_boardfile()
print("start_point: {}, goal_point: {}".format(start_point, goal_point))
# インスタンスの生成
maze_field = Field(init_maze, start_point, goal_point)
# ボード表示(初期表示)
maze_field.display()
# ----------
# 最短ルートを読み込み(Aルート)
mx_route, ref_action = maze_1.read_chessfile()
# ルートを表示する
maze_field.all_display(mx_route)
# ----------
# コマの最短ルートのCSVファイルを読み込み表示する（限界BCルート）
# Bルート
mx_routeB = maze_1.read_chessfileBC("chess_route27B.csv")
# Cルート
mx_routeC = maze_1.read_chessfileBC("chess_route27C.csv")
#print("--- mx_routeC ---")
#print(mx_routeC)

#=================================================
# Calculation class(1) : Actor-Network
#=================================================
class ActorNetwork(nn.Module):

    def __init__(self,input_size,hidden_size,action_size):
        super(ActorNetwork, self).__init__()
        self.fc1 = nn.Linear(input_size,hidden_size)
        self.fc2 = nn.Linear(hidden_size,hidden_size)
        self.fc3 = nn.Linear(hidden_size,hidden_size)
        self.fc4 = nn.Linear(hidden_size,action_size)

    def forward(self,x):
        out = F.relu(self.fc1(x))
        out = F.relu(self.fc2(out))
        out = F.relu(self.fc3(out))
        out = F.log_softmax(self.fc4(out), dim = 1)
        return out.squeeze(-1)

class ValueNetwork(nn.Module):

    def __init__(self,input_size,hidden_size,output_size):
        super(ValueNetwork, self).__init__()
        self.fc1 = nn.Linear(input_size,hidden_size)
        self.fc2 = nn.Linear(hidden_size,hidden_size)
        self.fc3 = nn.Linear(hidden_size,hidden_size)
        self.fc4 = nn.Linear(hidden_size,output_size)

    def forward(self,x):
        out = F.relu(self.fc1(x))
        out = F.relu(self.fc2(out))
        out = F.relu(self.fc3(out))
        out = self.fc4(out)
        return out.squeeze(-1)

#=================================================
# Calculation class(2) : Agent
#=================================================
class Agent():
    def __init__(self):

        # init value network
        self.value_network = ValueNetwork(input_size = STATE_DIM, hidden_size = 128, output_size = 1)
        self.value_network_optim = torch.optim.Adam(self.value_network.parameters(), lr=0.001)
        #value_scheduler = torch.optim.lr_scheduler.StepLR(self.value_network_optim, step_size=20, gamma=0.5)

        # init actor network
        self.actor_network = ActorNetwork(input_size = STATE_DIM, hidden_size = 128, action_size = ACTION_DIM)
        self.actor_network_optim = torch.optim.Adam(self.actor_network.parameters(), lr = 0.001)
        #actor_scheduler = torch.optim.lr_scheduler.StepLR(self.actor_network_optim, step_size=50, gamma=0.3)

        # ---------------------------
        # 盤面データを読む
        self.mx_lm = [[0, 2],[26, 25]]
        self.maze_field     = maze_field
        self.start_point    = start_point
        self.goal_point     = goal_point
        self.stage_info     = "default"
        self.epsilon_play   = 0.3
        # ----------------
        # リストの初期化
        self.arr_iplay      = []
        self.arr_maxscore   = []
        self.arr_maxturn    = []
        self.arr_actorloss  = []
        self.arr_criticloss = []
        # 距離の分析用累積リスト
        self.arr_maxTD      = []  # Total距離
        self.arr_sminRTD    = []  # 空間用minRT距離(A)
        self.arr_tlminRTD   = []  # タイムラインminRT距離
        self.arr_sminRTBD   = []  # 空間用minRT距離(B)
        self.arr_sminRTCD   = []  # 空間用minRT距離(C)
        # ----------------
        # 学習する
        for iCnt_play in range(EPISODE_STEP):
            maxturn, maxscore, value_loss, actor_loss = self.train(iCnt_play)
            # ----------
            self.arr_iplay.append(iCnt_play)
            self.arr_maxscore.append(maxscore)
            self.arr_maxturn.append(maxturn)
            self.arr_actorloss.append(actor_loss)
            self.arr_criticloss.append(value_loss)
            # 距離の保管(プレイベース)
            self.arr_maxTD.append(np.mean(self.arr_Tdist))        # Total距離の平均値
            self.arr_sminRTD.append(np.mean(self.arr_sminRT))       # 空間用minRT距離(A)の平均値
            self.arr_tlminRTD.append(np.mean(self.arr_tlminRT))      # タイムラインminRT距離の平均値
            self.arr_sminRTBD.append(np.mean(self.arr_sminRTB))       # 空間用minRT距離(B)の平均値
            self.arr_sminRTCD.append(np.mean(self.arr_sminRTC))       # 空間用minRT距離(C)の平均値

        # ---------------------------
        # 学習履歴の出力
        fig = plt.figure(figsize=(12, 9))
        ax1 = fig.add_subplot(2, 2, 1)
        ax1.plot(self.arr_iplay, self.arr_actorloss)
        ax1.set_xlabel('#episode')
        ax1.set_ylabel('actor_loss')
        # -----
        ax2 = fig.add_subplot(2, 2, 2)
        ax2.plot(self.arr_iplay, self.arr_criticloss)
        ax2.set_xlabel('#episode')
        ax2.set_ylabel('critic_loss')
        # -----
        ax3 = fig.add_subplot(2, 2, 3)
        ax3.plot(self.arr_iplay, self.arr_maxscore)
        ax3.set_xlabel('#episode')
        ax3.set_ylabel('score')
        #ax3.set_ylim([-10000, 10000])
        # -----
        ax4 = fig.add_subplot(2, 2, 4)
        ax4.plot(self.arr_iplay, self.arr_maxturn)
        ax4.set_xlabel('#episode')
        ax4.set_ylabel('turn')
        #ax4.set_ylim([0, 1000])
        # -----
        fig.tight_layout()
        #fig.savefig("./REINFORCE_img.png")
        plt.show()
        
        # ---------------------------
        # パフォーマンス推移図のグラフ化
        self.show_graph()


    # ----------------
    # 学習結果のグラフ化
    def show_graph(self):

        # ------
        # 学習履歴の出力
        fig2 = plt.figure(figsize=(12, 9))
        # -----
        # 移動平均を付ける
        y_rolling = pd.Series(self.arr_tlminRTD).rolling(window=12, center=True).mean()
        ax1 = fig2.add_subplot(2, 2, 1)
        ax1.set_title('learning transition : tlminRTD distance')
        ax1.set_xlabel('#episode')
        ax1.set_ylabel('distance')
        ax1.grid(True)
        ax1.plot(self.arr_iplay, self.arr_tlminRTD, label="tlminRTD", color="blue")
        ax1.plot(self.arr_iplay, y_rolling, label="moving", color="red")
        ax1.legend(loc='best')
        # -----
        ax2 = fig2.add_subplot(2, 2, 2)
        ax2.set_title('learning transition : maxTD, sminRTD distance')
        ax2.set_xlabel('#episode')
        ax2.set_ylabel('distance')
        ax2.set_ylim([0.0, 15.0])
        ax2.grid(True)
        ax2.scatter(self.arr_iplay, self.arr_maxTD, label="maxTD", color="blue")
        ax2.scatter(self.arr_iplay, self.arr_sminRTD, label="sminRTD", color="red")
        ax2.legend(loc='best')
        # -----
        ax3 = fig2.add_subplot(2, 2, 3)
        # 移動平均を付ける
        y_rolling = pd.Series(self.arr_sminRTBD).rolling(window=12, center=True).mean()
        ax3.set_title('learning transition : SminRTD distance[limit-B]')
        ax3.set_xlabel('#episode')
        ax3.set_ylabel('distance')
        ax3.grid(True)
        ax3.plot(self.arr_iplay, self.arr_sminRTBD, label="limitB", color="blue")
        ax3.plot(self.arr_iplay, y_rolling, label="moving", color="red")
        ax3.legend(loc='best')
        # -----
        ax4 = fig2.add_subplot(2, 2, 4)
        # 移動平均を付ける
        y_rolling = pd.Series(self.arr_sminRTCD).rolling(window=12, center=True).mean()
        ax4.set_title('learning transition : SminRTD distance[limit-C]')
        ax4.set_xlabel('#episode')
        ax4.set_ylabel('distance')
        ax4.grid(True)
        ax4.plot(self.arr_iplay, self.arr_sminRTCD, label="limitC", color="blue")
        ax4.plot(self.arr_iplay, y_rolling, label="moving", color="red")
        ax4.legend(loc='best')
        # -----
        fig2.tight_layout()
        #fig2.savefig("./REINFORCE_img.png")
        plt.show()

    # ----------------
    # DRL学習する
    def train(self, iCnt_play):

        # ----------------
        # カリキュラム学習[1]
        # ボードを調整する
        # ========================================
        # stage 1
        if iCnt_play == 0:
            # メイズ面を調整する
            self.stage_info  = "stage1"
            new_maze = copy.deepcopy(init_maze)
            # -----
            # 盤面の調整
            epsilon_board = 0.8
            for i in range(1, size-1):
                for j in range(1, size-1):
                    if new_maze[i][j] == '#' and random.random() < epsilon_board:
                        new_maze[i][j] = -500
            # -----
            # インスタンスを生成する
            self.maze_field = Field(new_maze, start_point, goal_point)
            # ボード表示(調整後表示)
            self.maze_field.display()
        # ========================================
        # stage 2
        if iCnt_play == 300:
            # メイズ面を調整する
            self.stage_info  = "stage2"
            new_maze = copy.deepcopy(init_maze)
            # -----
            # 盤面の調整
            epsilon_board = 0.7
            for i in range(1, size-1):
                for j in range(1, size-1):
                    if new_maze[i][j] == '#' and random.random() < epsilon_board:
                        new_maze[i][j] = -500
            # -----
            # インスタンスを生成する
            self.maze_field = Field(new_maze, start_point, goal_point)
            # ボード表示(調整後表示)
            self.maze_field.display()
        # ========================================
        # stage 3
        if iCnt_play == 900:
            # メイズ面を調整する
            self.stage_info  = "stage3"
            new_maze = copy.deepcopy(init_maze)
            # -----
            # 盤面の調整
            epsilon_board = 0.6
            for i in range(1, size-1):
                for j in range(1, size-1):
                    if new_maze[i][j] == '#' and random.random() < epsilon_board:
                        new_maze[i][j] = -500
            # -----
            # インスタンスを生成する
            self.maze_field = Field(new_maze, start_point, goal_point)
            # ボード表示(調整後表示)
            self.maze_field.display()
        # ========================================
        # stage 4
        if iCnt_play == 1500:
            # メイズ面を調整する
            self.stage_info  = "stage4"
            new_maze = copy.deepcopy(init_maze)
            # -----
            # 盤面の調整
            epsilon_board = 0.5
            for i in range(1, size-1):
                for j in range(1, size-1):
                    if new_maze[i][j] == '#' and random.random() < epsilon_board:
                        new_maze[i][j] = -500
            # -----
            # インスタンスを生成する
            self.maze_field = Field(new_maze, start_point, goal_point)
            # ボード表示(調整後表示)
            self.maze_field.display()
        # ========================================
        # カリキュラム学習[2A]
        # 強制的に最適ルートを入力する
        # epsilon for play
        if iCnt_play < 300:
            self.epsilon_play   = 0.15
        elif iCnt_play >= 300 and iCnt_play < 900:
            self.epsilon_play   = 0.15
        elif iCnt_play >= 900 and iCnt_play < 1500:
            self.epsilon_play   = 0.20
        else:
            self.epsilon_play   = 0.15
        # -----
        # ROLL_OUT(ゲームのバッチ実行)
        states, actions, rewards, final_r, maxturn, maxscore, state_chess = self.roll_out(iCnt_play)
        actions_var = Variable(torch.Tensor(actions))
        states_var = Variable(torch.Tensor(states).view(-1,STATE_DIM))

        # train actor network
        self.actor_network_optim.zero_grad()
        log_softmax_actions = self.actor_network(states_var)
        vs = self.value_network(states_var).detach()
        # calculate qs
        qs = Variable(torch.Tensor(self.discount_reward(rewards,0.97,final_r)))

        advantages = qs - vs
        self.actor_network_loss = - torch.mean(torch.sum(log_softmax_actions*actions_var,1)*advantages)
        self.actor_network_loss.backward()
        self.actor_network_optim.step()

        # train value network
        self.value_network_optim.zero_grad()
        target_values = qs
        values = self.value_network(states_var)
        criterion = nn.MSELoss()
        self.value_network_loss = criterion(values,target_values)
        self.value_network_loss.backward()
        self.value_network_optim.step()
        
        # loss amount
        value_loss = round(self.value_network_loss.item(), 3)
        actor_loss = round(self.actor_network_loss.item(), 3)
        
        # ----------
        # 学習結果の出力(2)
        if iCnt_play%5 ==0:
            print("iCnt_play:{}, stage:{}, state_chess:{}, value_loss:{}, actor_loss:{}, maxturn:{}, maxscore:{}".format(iCnt_play, self.stage_info, state_chess, value_loss, actor_loss, maxturn, maxscore))       

        return maxturn, maxscore, value_loss, actor_loss
    
        
    # ゲームを実行する
    def roll_out(self, iCnt_play):

        states  = []
        actions = []
        rewards = []
        # ----------
        # リストの初期化
        self.arr_Tdist     = []    # Total距離
        self.arr_sminRT    = []    # 空間用minRT距離(A)
        self.arr_tlminRT   = []    # タイムラインminRT距離
        self.arr_sminRTB   = []    # 空間用minRT距離(B)
        self.arr_sminRTC   = []    # 空間用minRT距離(C)
        # ----------
        # ゲームの初期化
        state_chess = self.start_point
        num_bump    = 0
        state, dist_tlminRT = calc_address(state_chess[0], state_chess[1], num_bump, self.mx_lm, 0)
        # 移動稼働なオプション
        movables    = self.maze_field.get_actions(state_chess)
        # ----------
        sum_bump = 0
        score    = 0
        is_done  = False
        final_r  = 0
        for iCnt_turn in range(SAMPLE_NUMS):
            states.append(state)
            log_softmax_action = self.actor_network(Variable(torch.Tensor([state])))
            softmax_action = torch.exp(log_softmax_action)
            action = np.random.choice(ACTION_DIM, p=softmax_action.data.numpy()[0])
            # ----------------
            # カリキュラム学習[2B]
            # 強制的に最適ルートを入力する
            # stage 1
            if iCnt_play == 0 or iCnt_play == 2 or iCnt_play == 5 or iCnt_play == 8 or random.random() < self.epsilon_play:
                if iCnt_turn <= len(mx_route) - 1:
                    action = ref_action[iCnt_turn]
            # stage 2
            if iCnt_play == 300 or iCnt_play == 302 or iCnt_play == 305 or iCnt_play == 308  or iCnt_play == 312 or random.random() < self.epsilon_play:
                if iCnt_turn <= len(mx_route) - 1:
                    action = ref_action[iCnt_turn]  
            # stage 3
            if iCnt_play == 900 or iCnt_play == 902 or iCnt_play == 905 or iCnt_play == 908 or iCnt_play == 912 or random.random() < self.epsilon_play:
                if iCnt_turn <= len(mx_route) - 1:
                    action = ref_action[iCnt_turn]  
            # stage 4
            if iCnt_play == 1500 or iCnt_play == 1502 or iCnt_play == 1505 or iCnt_play == 1508 or iCnt_play == 1512 or random.random() < self.epsilon_play:
                if iCnt_turn <= len(mx_route) - 1:
                    action = ref_action[iCnt_turn]  
            # ----------
            # ONTHOT化
            one_hot_action = [int(k == action) for k in range(ACTION_DIM)]
            action_chess = self.maze_field.nxtPosition(state_chess, action)
            # 次の報酬を得る（#に当たったら、STUCKPOINTを生成する）
            next_state_chess, temp_reward, done, num_bump = self.maze_field.get_val(action_chess, state_chess, movables)
            # 状態の表記を変更(ONEHOTプラス距離)
            next_state, dist_tlminRT = calc_address(next_state_chess[0], next_state_chess[1], num_bump, self.mx_lm, iCnt_turn)
            # ----------------
            # ゴールに到達した
            if next_state_chess[0] >= 25 and next_state_chess[1] >= 25:
                done = True
                reward = 10000 - 5*iCnt_turn - 100*sum_bump
                nominal_reward = 10000
            # ----------
            # 移動稼働なオプション
            next_movables = self.maze_field.get_actions(next_state_chess)
            # ----------
            # 距離の計算
            T_Dist  = state[3] + state[4]
            dist_sminRT  = state[5]        # 空間用minRT距離(A)
            dist_sminRTB = state[6]        # 空間用minRT距離(B)
            dist_sminRTC = state[7]        # 空間用minRT距離(C)
            # ----------
            self.arr_Tdist.append(T_Dist-8.0)          # Total距離
            self.arr_sminRT.append(dist_sminRT)        # 空間用minRT距離(A)
            self.arr_sminRTB.append(dist_sminRTB)      # 空間用minRT距離(B)
            self.arr_sminRTC.append(dist_sminRTC)      # 空間用minRT距離(C)
            self.arr_tlminRT.append(dist_tlminRT)      # タイムラインminRT距離
            # ----------
            # [重要]フィードバック制御あり
            if done == False:
                diff_sminRTB = dist_sminRTB
                diff_sminRTC = dist_sminRTC
                if diff_sminRTB > 0:
                    diff_sminRTB = 0
                if diff_sminRTC > 0:
                    diff_sminRTC = 0
                reward = temp_reward + 30 * diff_sminRTB + 30 * diff_sminRTC - 20 * dist_sminRT
                nominal_reward = temp_reward
            # ----------
            # リストへの追加
            actions.append(one_hot_action)
            rewards.append(reward)
            final_state = next_state
            state_chess = next_state_chess
            state = next_state
            movables = next_movables
            score += nominal_reward
            sum_bump += num_bump
            # ----------
            # 学習結果の出力(1)
            #if iCnt_turn%10 == 0:
            #    print("iCnt_play:{}, iCnt_turn:{}, state_chess:{}, sminRT:{}, sminRTB:{}, sminRTC:{}".format(iCnt_play, iCnt_turn, state_chess, dist_sminRT, dist_sminRTB, dist_sminRTC))       
            if done == True:
                is_done = True
                break
        # ----------
        if is_done == False:
            final_r = self.value_network(Variable(torch.Tensor([final_state]))).data.numpy()
        # ----------
        # 学習統計
        maxturn  = int(iCnt_turn)
        maxscore = int(score)

        return states, actions, rewards, final_r, maxturn, maxscore, state_chess

    def discount_reward(self, r, gamma, final_r):
        discounted_r = np.zeros_like(r)
        running_add = final_r
        for t in reversed(range(0, len(r))):
            running_add = running_add * gamma + r[t]
            discounted_r[t] = running_add
        return discounted_r

if __name__ == '__main__':

    # ----------
    # Hyper Parameters
    STATE_DIM       = 8
    ACTION_DIM      = 4
    EPISODE_STEP    = 1000
    SAMPLE_NUMS     = 5000

    # ----------
    # 強化学習を実行する
    Agent()

```

D先生: “カリキュラム学習も変えたんですか？”

**(STAGE1)**

![imageRL5-4-4](https://reinforce.github.io/images/imageRL5-4-4.jpg) 

**(STAGE2)**

![imageRL5-4-5](https://reinforce.github.io/images/imageRL5-4-5.jpg) 

QEU：FOUNDER ; “カリキュラムのレベルに合わせて侵入不可の記号「」を大きなマイナス価値に変換しています。マイナス価値の場合には、強化学習が不安定になることはないですからね。それでは、学習結果をドン！！”

**（学習曲線）**

![imageRL5-4-6](https://reinforce.github.io/images/imageRL5-4-6.jpg) 

**（パフォーマンス推移）**

![imageRL5-4-7](https://reinforce.github.io/images/imageRL5-4-7.jpg) 

D先生: “いやぁ・・・、これは・・・。A3Cはもう無理かな・・・。”

QEU：FOUNDER ; “A3Cには収束させる力がなさそうだね。”

D先生 : “じゃあ、**ε-Greedy系列**に行きましょうか・・・。”

QEU：FOUNDER ; “それが妥当な判断だね・・・。もし、もっと良いアイデアがある人はA3Cを続けてトライしてください。”


## ～　まとめ　～

C部長 ： “知らせを聞いて驚きました。まさか本当に来るとはね・・・。この動画（↓）の長さも異例だったし・・・。”

<iframe width="560" height="315" src="https://www.youtube.com/embed/0hvBY1OPF04" ti-tle="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; en-crypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

QEU：FOUNDER ; “C部長様のご意見はいかがですか？”

C部長 ： “最近、つくづく思ってるんです、「自分も変わったな」って・・・。なんかね・・・。先日の**U国とR国の戦争とか比較しながら**・・・、今回のA国の行動が本当に問題の解決になるのだろうかと・・・。”

<iframe width="560" height="315" src="https://www.youtube.com/embed/FgqoXoZLT4o" ti-tle="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; en-crypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

QEU:FOUNDER ; “さては、大先生のノート(↓)をみたな・・・。”

![imageRL5-4-8](https://reinforce.github.io/images/imageRL5-4-8.jpg) 

D部長: “いや・・・。この大先生(↓)の意見を参考にしたんじゃないですか？”

![imageRL5-4-9](https://reinforce.github.io/images/imageRL5-4-9.jpg) 

C部長 ; “小生はノンポリなので、「こういう考え方もあるな」としか・・・(笑)。”

