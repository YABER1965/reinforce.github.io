## QEUR21_MZDYNQ0: DYNAQから始めましょう (２，３次元メイズ-やりなおし)

## ～　3DMAZEは、2Dに逆戻り・・・！　～

### ・・・　前回のつづきです　・・・

QEU：FOUNDER ; “そして第4段階のパフォーマンスはこうなりました。”

![imageRL7-1-1](https://reinforce.github.io/images/imageRL7-1-1.jpg) 

D先生 : “あ～あ・・・。まえよりパフォーマンスが悪くなりました。”

![imageRL7-1-2](https://reinforce.github.io/images/imageRL7-1-2.jpg) 

QEU：FOUNDER ; “我々が提唱する「転写学習」は教師あり学習と強化学習の中間だと説明しました。多分、そのバランスを見直すべきなんでしょうね。これなんかはどう？”

![imageRL7-1-3](https://reinforce.github.io/images/imageRL7-1-3.jpg) 

D先生 : “**Dyna-Q?なんですか**それ？・・・というか、J国語の参考文献はありませんか？”

![imageRL7-1-4](https://reinforce.github.io/images/imageRL7-1-4.jpg) 

QEU:FOUNDER ： “**モデルベースに属する方法**ですよ。ちなみに、J国には期待しない方がいいです。現状のロボティックスの実力をを見ればわかる・・・（笑）。個人的には、C国の参考資料のほうが好きです。Cliff-Walkingを使っているからね。”

**（参考資料）**

![imageRL7-1-5](https://reinforce.github.io/images/imageRL7-1-5.jpg) 

**（パフォーマンス推移）**

![imageRL7-1-6](https://reinforce.github.io/images/imageRL7-1-6.jpg) 

QEU:FOUNDER ： “Dyna-Qのオリジナルパラメタである**「planning step」**を使えば、学習収束がはやくなります。”

D先生 ： “よっしゃ！すぐやろ！！・・・でもね、そもそも**「モデル(model)」**ってなんだろう・・・。”

![imageRL7-1-7](https://reinforce.github.io/images/imageRL7-1-7.jpg) 

QEU:FOUNDER ： “それがポイントです・・・。モデルとは**Q値のデータ**という意味です。もともと**Q値テーブルがあるんだったら学習する必要がない**ですよね。基本・・・、model-basedは矛盾していますよね。今回は「新シリーズ」として、Dyna-Qから始めましょう。”



## ～　まとめ　～

D先生 : “この方（↓）をご存じですか？”

![imageRL7-1-8](https://reinforce.github.io/images/imageRL7-1-8.jpg) 

QEU:FOUNDER ： “知る人ぞ知る・・・。”

<iframe width="560" height="315" src="https://www.youtube.com/embed/0AGor-oruPo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

D先生 : “すげえ・・・。”

QEU:FOUNDER ： “今回は敢えて取り上げてくださった海外メディアを誉めたいよね・・・。”


