## QEUR21_2DMZ4:　タイムラインを加味すると・・・

## ～　使えないけど・・・ ～

### ・・・ 使えます（笑） ・・・

D先生 ： “強化学習に空間minRT距離の情報を追加すると、Y軸（学習Loss）のスケールが半分になっています。このグラフを見れば、学習加速の効果は一目瞭然です。でも・・・、どうして「空間（minRT）」なのですか？”

**（学習曲線）**

![imageRL4-5-1](https://reinforce.github.io/images/imageRL4-5-1.jpg) 


**（パフォーマンス推移）**

![imageRL4-5-2](https://reinforce.github.io/images/imageRL4-5-2.jpg)

QEU:FOUNDER : “実はminRT法には「時間(タイムライン)」もあるから・・・（笑）。考え方はこんな感じね・・・。”

![imageRL4-5-3](https://reinforce.github.io/images/imageRL4-5-3.jpg)

QEU:FOUNDER : “空間minRT法のように、すべての単位空間と計測点間の距離をソートするのではなく、時間minRTでは単位空間の始点はタイムラインで動きます。

D先生 ： “つまり、「やり直しの効かない」、厳しい評価方法ですね。

QEU:FOUNDER : “今回は、このメトリックスを学習情報に「追加」します。”

D先生 ： “えっ？比較せずに、追加するの・・・？”

QEU;FOUNDER ： “パフォーマンスが悪くなると、当該メトリックスには学習のための効用がないことがわかりますからね。”

D先生 ： “それでは、プログラムをドン！！”

```python
# ----------------
# メイズ(MAZE)ゲームの強化学習システム
# step4 : RTを動的メトリックスとして活用する
# step4_dqn_maze_pyTorch(timeline_minRT).py
# step4 : DQN-Experience replay
# ---------------- 
# 数値計算のﾗｲﾌﾞﾗﾘ読み込み
import numpy as np
import pandas as pd
import math, copy, random, time
from collections import deque, Counter
from scipy.special import softmax
from IPython.display import clear_output
# ----------------
import torch
import torch.nn.functional
import torch.utils.data
# ---------------- 
import matplotlib.pyplot as plt
#%matplotlib inline

# =================================================
# Game board difinition 
# =================================================
foldername  = "./MAZE_test/"
init_maze = [['#', 'S', '#', '#', '#', '#', '#', '#', '#', '#'], ['#', 0, '#', -1, 0, -1, 0, '#', '#', '#'], ['#', -1, '#', -1, -1, 0, -1, -1, 0, '#'], ['#', 0, -1, 0, 0, -1, 0, 0, '#', '#'], ['#', '#', -1, 0, 0, -1, '#', -1, 0, '#'], ['#', -1, 0, 0, -1, -1, 0, '#', -1, '#'], ['#', 0, 0, 0, -1, '#', -1, '#', -1, '#'], ['#', 0, -1, 0, -1, 0, -1, 0, -1, '#'], ['#', 0, -1, 0, 0, -1, 0, 0, -1, '#'], ['#', '#', '#', '#', '#', '#', '#', 50, '#', '#']]
mx_lm = [[0, 0],[0, 9],[9, 0],[9, 9],[0, 1],[9, 7]]

# --------------------------------------------------
# MAZE盤面をCSVファイルに保存する
def save_csvMAZE(maze): 
    # --------------------------------------------------
    # CSV出力用のデータフレームを作る(2)
    arr_columns = list(range(10))
    # -----
    df_csvout = pd.DataFrame(maze, columns=arr_columns)
    #print(df_csvout)
    
    # --------------------------------------------------
    # CSV ファイル (file_csvout) として出力
    code_csvout = "maze_board.csv"       # file name  
    file_csvout = foldername + code_csvout   # standard(training) file name   
    print("メトリックス保管用CSVファイル ：{0}".format(file_csvout))
    # -----
    df_csvout.to_csv(file_csvout, index=True)

# MAZE盤面をCSVファイルに保存する
#save_csvMAZE(init_maze)

# =================================================
# difinition of STATE CALCULATION function
# =================================================
# タイムライン用minRTメトリックスを計算する
def timeline_minRT(a_row, a_col, iCnt_turn):
    len_route = len(mx_route)
    if iCnt_turn <= len_route - 1:
        pos_rt  = mx_route[iCnt_turn]
    else:
        pos_rt  = mx_route[len_route - 1]
    # -----
    sqEuc   = (a_row - pos_rt[0])**2 + (a_col - pos_rt[1])**2
    dist_tlminRT = np.sqrt(sqEuc)
        
    return dist_tlminRT

# -------
# 空間用minRTメトリックスを計算する
def space_minRT(a_row, a_col):
    len_route = len(mx_route)
    arr_sminRT = []
    for i in range(len_route):
        pos_rt  = mx_route[i]
        sqEuc   = (a_row - pos_rt[0])**2 + (a_col - pos_rt[1])**2
        distEuc = np.sqrt(sqEuc)
        arr_sminRT.append(round(distEuc,5))   
    dist_sminRT = np.min(arr_sminRT)
        
    return dist_sminRT

# -------
# 状態の表記を変更(STATE-6_landmarkへ)、タイムライン用minRTメトリックスつき
def calc_address(a_row, a_col, iCnt_turn):

    state = np.zeros(8)
    for i in range(6):
        # 一般的な距離を計算する(0-5)
        pos_lm  = mx_lm[i]
        sqEuc   = (a_row - pos_lm[0])**2 + (a_col - pos_lm[1])**2
        distEuc = np.sqrt(sqEuc)
        state[i] = round(distEuc,5)
        
    # 空間用minRTメトリックスを計算する(6)
    dist_sminRT  = space_minRT(a_row, a_col)
    state[6]    = dist_sminRT

    # タイムライン用minRTメトリックスを計算する(6)
    dist_tlminRT  = timeline_minRT(a_row, a_col, iCnt_turn)
    state[7]    = dist_tlminRT

    return state

# -------
# Maze Class
class Maze(object):
    def __init__(self, size=10, blocks_rate=0.1):
        self.size = size if size > 3 else 10
        self.blocks = int((size ** 2) * blocks_rate) 
        self.s_list = []
        self.maze_list = []
        self.e_list = []

    def create_mid_lines(self, k):
        if k == 0: self.maze_list.append(self.s_list)
        elif k == self.size - 1: self.maze_list.append(self.e_list)
        else:
            tmp_list = []
            for l in range(0,self.size):
                if l == 0: tmp_list.extend("#")
                elif l == self.size-1: tmp_list.extend("#")
                else:
                    a = random.randint(-1, 0)
                    tmp_list.extend([a])
            self.maze_list.append(tmp_list)

    def insert_blocks(self, k, s_r, e_r):
        b_y = random.randint(1, self.size-2)
        b_x = random.randint(1, self.size-2)
        if [b_y, b_x] == [1, s_r] or [b_y, b_x] == [self.size - 2, e_r]: k = k-1
        else: self.maze_list[b_y][b_x] = "#"
            
    def generate_maze(self): 
        s_r = random.randint(1, int((self.size / 2)) - 1)
        for i in range(0, self.size):
            if i == s_r: self.s_list.extend("S")
            else: self.s_list.extend("#")
        start_point = [0, s_r]

        e_r = random.randint(int((self.size / 2)) + 1, self.size - 2)
        for j in range(0, self.size):
            if j == e_r: self.e_list.extend([50])
            else: self.e_list.extend("#")
        goal_point = [self.size - 1, e_r]

        for k in range(0, self.size):
            self.create_mid_lines(k)
        
        for k in range(self.blocks):
            self.insert_blocks(k, s_r, e_r)

        return self.maze_list, start_point, goal_point

    # --------------------------------------------------
    # 盤面のCSVファイルを読み込み表示する
    def read_boardfile(self):
        # CSVファイルの読み込み
        code_csvout = "maze_board.csv"       # file name
        file_readcsv = foldername + code_csvout   # standard(training) file name  
        df = pd.read_csv(file_readcsv)
        #max_play = len(df)
        # print("データ量 max_play",max_play)
        # print(df)
        # --------------------------------------------------
        # 選択項目の読み込み
        maze = df.loc[:, "0":"9"].values.tolist()
        start_point = [0,0]
        goal_point  = [0,0]
        for i in range(size):
            for j in range(size):
                if maze[i][j] == 'S':
                    start_point = [i, j]
                if maze[i][j] == '50':
                    goal_point  = [i, j]

        self.maze_list   = maze
        self.start_point = start_point
        self.goal_point  = goal_point
        #print("----- mx_maze(read_boardfile) -----")
        #print(mx_maze)

        return self.maze_list, start_point, goal_point

    # --------------------------------------------------
    # コマの最短ルートのCSVファイルを読み込み表示する
    def read_chessfile(self):
        # CSVファイルの読み込み
        code_csvout = "chess_route.csv"       # file name
        file_readcsv = foldername + code_csvout   # standard(training) file name  
        df = pd.read_csv(file_readcsv)
        #max_play = len(df)
        # print("データ量 max_play",max_play)
        # print(df)
        # --------------------------------------------------
        # 選択項目の読み込み
        mx_route = df.loc[:, "row":"col"].values.tolist()
        #print("----- route -----")
        #print(route)

        return mx_route

# -------
# Maze functions
class Field(object):
    def __init__(self, maze, start_point, goal_point):
        self.maze = maze
        self.start_point = start_point
        self.goal_point = goal_point
        self.movable_vec = [[1,0],[-1,0],[0,1],[0,-1]]

    def display(self, point=None):
        field_data = copy.deepcopy(self.maze)
        if not point is None:
                y, x = point
                field_data[y][x] = "@@"
        else:
                point = ""
        print("================")
        for line in field_data:
                print ("\t" + "%3s " * len(line) % tuple(line))
        print("================")

    def all_display(self, mx_route):
        field_data = copy.deepcopy(self.maze)
        for i in range(len(mx_route)):
            point = mx_route[i]
            y, x = point[0], point[1]
            field_data[y][x] = "@@"
        print("================")
        for line in field_data:
                print ("\t" + "%3s " * len(line) % tuple(line))
        print("================")

    def get_actions(self, state):
        movables = []
        if state == self.start_point:
            y = state[0] + 1
            x = state[1]
            a = [[y, x]]
            return a
        else:
            for v in self.movable_vec:
                y = state[0] + v[0]
                x = state[1] + v[1]
                if not(0 < x < len(self.maze) and
                       0 <= y <= len(self.maze) - 1 and
                       maze[y][x] != "#" and
                       maze[y][x] != "S"):
                    continue
                movables.append([y,x])
            if len(movables) != 0:
                return movables
            else:
                return None

    def get_val(self, state):
        y, x = state
        if state == self.start_point: return 0, False
        else:
            v = float(self.maze[y][x])
            if state == self.goal_point: 
                return v, True
            else: 
                return v, False

# -------
# Generate a maze
size = 10
barriar_rate = 0.1

maze_1 = Maze(size, barriar_rate)
#maze, start_point, goal_point = maze_1.generate_maze()
maze, start_point, goal_point = maze_1.read_boardfile()
maze_field = Field(maze, start_point, goal_point)
# 普通の表示
maze_field.display()

# 最短ルートを読み込み
mx_route = maze_1.read_chessfile()
# ルートの表示(個別表示)
#for i in range(len(mx_route)):
#    maze_field.display(mx_route[i])
# ルートの表示(全表示)
maze_field.all_display(mx_route)


#=================================================
# Deep Learning Model class            
#=================================================
# PyTorchのDLの定義
class Net(torch.nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = torch.nn.Linear(dim_input, 128)
        self.fc1.weight.data.normal_(0, 0.1)
        self.fc2 = torch.nn.Linear(128, 128)
        self.fc2.weight.data.normal_(0, 0.1)
        self.fc3 = torch.nn.Linear(128, 128)
        self.fc3.weight.data.normal_(0, 0.1)
        self.fc4 = torch.nn.Linear(128, dim_output)
        self.fc4.weight.data.normal_(0, 0.1)
        
    def forward(self, x):
        x = torch.nn.functional.relu(self.fc1(x))
        x = torch.nn.functional.relu(self.fc2(x))
        x = torch.nn.functional.relu(self.fc3(x))
        x = self.fc4(x)
        return x

# =======================================
# =======================================
# Solving the maze in Deep Q-learning
class DQN_Solver:
    def __init__(self, state_size, action_size):
        self.state_size = state_size # list size of state
        self.action_size = action_size # list size of action
        self.memory = deque(maxlen=10000) # memory space
        self.gamma = 0.96 # discount rate
        self.epsilon = 1.0 # randomness of choosing random action or the best one
        self.e_decay = 0.9995 # epsilon decay rate
        self.e_min = 0.01 # minimum rate of epsilon
        self.learning_rate = 0.001 # learning rate of neural network
        # --------------------------------------------------
        # crate instance for input
        self.eval_net, self.target_net = Net(), Net()                           # Netを利用して２つのニューラルネットをつくる
        self.learn_step_counter = 0    
        # --------------------------------------------------
        # Save and load the model via state_dict
        #self.eval_net.load_state_dict(torch.load(file_input_model))
        # --------------------------------------------------
        # set validaiton mode
        self.eval_net.eval()
        # --------------------------------------------------
        # set training parameters
        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=self.learning_rate)
        self.criterion = torch.nn.MSELoss()

    # ----------------
    # PyTorchモデルの保存  
    #def save_models(self):
        #torch.save(self.eval_net.state_dict(), file_output_model)

    # remember state, action, its reward, next state and next possible action. done means boolean for goal
    def remember_memory(self, state, action, reward, next_state, next_movables, iCnt_turn, done):
        self.memory.append((state, action, reward, next_state, next_movables, iCnt_turn, done))

    # choosing action depending on epsilon
    def choose_action(self, state, movables, iCnt_turn):
        acType = "random"
        Qvalue = -0.001
        T_Dist = -0.001
        dist_sminRT = -0.001
        dist_tlminRT = -0.001
        #print("movables: ", movables)
        if self.epsilon >= random.random():
            # randomly choosing action
            action = random.choice(movables)
            return acType, action, Qvalue, T_Dist, dist_sminRT, dist_tlminRT
        else:
            # choosing the best action from model.predict()
            acType, action, Qvalue, T_Dist, dist_sminRT, dist_tlminRT = self.choose_best_action(state, movables, iCnt_turn)
            return acType, action, Qvalue, T_Dist, dist_sminRT, dist_tlminRT
        
    # choose the best action to maximize reward expectation
    def choose_best_action(self, state, movables, iCnt_turn):

        action = [0,0]
        acType = "machine"
        Qvalue = -0.001
        iCnt = 0
        for a in movables:

            # ONEHOT状態(INPUT_STATE)を生成する
            input_state = calc_address(state[0], state[1], iCnt_turn)
            np_action = np.hstack([input_state, a])
            #print("np_action: ",np_action)
            if iCnt == 0:
                mx_input = [np_action]  # 状態(S)行動(A)マトリックス
            else:
                mx_input = np.append(mx_input, [np_action], axis=0)     # 状態(S)マトリックス      
            iCnt = iCnt + 1
        # print("----- mx_input -----")
        # print(mx_input)
        # --------------------------------------------------
        # generate new 'x'
        x_input_tensor = torch.tensor(mx_input).float()
        # predict 'y'
        with torch.no_grad():
            y_pred_tensor = self.eval_net(x_input_tensor)
        # convert tensor to numpy
        y_pred      = y_pred_tensor.data.numpy().flatten()
        Qvalue      = np.max(y_pred)
        temp_order  = np.argmax(y_pred)
        action      = movables[temp_order]

        return acType, action, Qvalue, input_state[4]+input_state[5], input_state[6], input_state[7]


    # this experience replay is going to train the model from memorized states, actions and rewards
    def replay_experience(self, batch_size):

        # ターゲットNetのパラメタを更新する
        if self.learn_step_counter < 5 or self.learn_step_counter % TARGET_REPLACE_ITER == 0:                  # 最初5回は毎回に更新し、あとは定期的に更新する
            self.target_net.load_state_dict(self.eval_net.state_dict())         # 評価NetのパラメタをターゲットNetに引き渡す
        self.learn_step_counter += 1   
        # --------------------------------------------------
        batch_size = min(batch_size, len(self.memory))
        minibatch = random.sample(self.memory, batch_size)
        X, Y = [], []
        # --------------------------------------------------
        for ibat in range(batch_size):
            state, action, reward, next_state, next_movables, iCnt_turn, done = minibatch[ibat]

            # ONEHOT状態(INPUT_STATE)を生成する
            input_state = calc_address(state[0], state[1], iCnt_turn)
            state_action_curr = np.hstack([input_state, action])
            #print("state_action_curr",state_action_curr)
            # ----------------
            if done:
                target_f = reward
            else:
                next_rewards = []
                iCnt = 0
                for a in next_movables:

                    # ONEHOT状態(INPUT_STATE)を生成する
                    input_next_state = calc_address(next_state[0], next_state[1], iCnt_turn)
                    np_next_s_a = np.hstack([input_next_state, a])
                    
                    if iCnt == 0:
                        mx_input = [np_next_s_a]  # 状態(S)行動(A)マトリックス
                    else:
                        mx_input = np.concatenate([mx_input, [np_next_s_a]], axis=0)     # 状態(S)マトリックス
                    
                    iCnt = iCnt + 1
                # ----------------
                # generate new 'x'
                x_input_tensor = torch.tensor(mx_input).float()
                # predict 'y'
                with torch.no_grad():
                    y_pred_tensor = self.target_net(x_input_tensor)
                # convert tensor to numpy
                y_pred = y_pred_tensor.data.numpy().flatten()
                np_n_r_max = np.amax(y_pred)
                target_f = reward + self.gamma * np_n_r_max
            # ----------------
            if ibat == 0:
                X = np.array([state_action_curr])  # 状態(S)行動(A)マトリックス
            else:
                X = np.concatenate([X, np.array([state_action_curr])], axis=0)  # 状態(S)行動(A)マトリックス
            Y = np.append(Y,target_f)

        # --------------------------------------------------
        # TRAINING
        # convert numpy array to tensor
        state_action_eval = torch.from_numpy(X).float()
        q_target = torch.from_numpy(Y.reshape(-1, 1)).float()
        #print("state_action_eval:",state_action_eval)
        #print("q_target:",q_target)

        # --- building model ---
        q_eval = self.eval_net(state_action_eval)
        # calculate loss
        loss = self.criterion(q_eval, q_target)
        # update weights
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        # Show progress
        #print('learn done -- [epsilon: {0}, loss: {1}]'.format(self.epsilon, loss))
        if self.epsilon > self.e_min:
            self.epsilon *= self.e_decay

        return round(self.epsilon,5) , round(loss.data.item(),5)

# ---------------------------
# パラメタの設定
SLEEP_TIME  = 0.01
TARGET_REPLACE_ITER = 100            # ターゲットNet更新頻度
state_size  = 8
action_size = 2
dim_input   = state_size + action_size
dim_output  = 1
dql_solver  = DQN_Solver(state_size, action_size)
BATCH_SIZE  = 128                    # サンプルサイズ

# number of episodes to run training
episodes = 3000

# number of times to sample the combination of state, action and reward
times    = 5000

# ---------------------------
# 出力用:Pytorchモデルのファイル名
#comment_output_model = "initial"
#code_output_model = "model_cliffwalk_DQNER_{0}.pt".format(comment_output_model)  # モデルの指定
#file_output_model = foldername + code_output_model  # ファイルパス名の生成

#=================================================
# Calculation class(3) : Agent
#=================================================
class Agent():
    # -----
    def __init__(self):

        # ---------------------------
        # 記録用パラメタ類(プレイベース)
        self.arr_iplay     = []  # count game play    プレイ番号
        self.arr_maxturn   = []  # turn game play    ターン数
        self.arr_maxscore  = []  # rl_score game play    報酬の総和
        self.arr_victory   = []  # victory    勝利したか
        self.arr_loss      = []  # DQN-Experience Replay学習
        self.arr_epsilon   = []  # ε-Greedy        
        # ---------------------------
        # Q値の分析用(プレイベース)
        self.arr_numQV     = []  # QV値のN数
        self.arr_maxQV     = []  # QV値の最大値
        self.arr_q25QV     = []  # QV値の4分の1値
        self.arr_q75QV     = []  # QV値の4分の3値
        # ---------------------------
        # 距離の分析用(プレイベース)
        self.arr_maxTD     = []  # Total距離の最大値
        self.arr_sminRTD   = []  # 空間minRT距離の最大値
        self.arr_tlminRTD  = []  # タイムラインminRT距離の最大値
        
        # エピソードの実行
        for iCnt_play in range(episodes):

            maxturn, maxscore, flag_goal = self.get_episode(iCnt_play)
            # ----------
            # DQN-Experience Replay学習  
            val_epsilon, val_loss = dql_solver.replay_experience(BATCH_SIZE)
            # 学習結果の出力
            if iCnt_play % 5 == 0:
                print("iCnt_play:{0}, maxturn:{1}, maxscore:{2}, flag_goal:{3}, epsilon:{4}, loss:{5}".format(iCnt_play, maxturn, maxscore, flag_goal, val_epsilon, val_loss))

            # ----------
            # 記録用パラメタ類(プレイベース)の追加
            self.arr_iplay.append(iCnt_play)        # count game play    プレイ番号
            self.arr_maxturn.append(maxturn)        # max_turn   ゲームのターン数
            self.arr_maxscore.append(maxscore)      # rl_score game play    最終プレイスコア
            self.arr_victory.append(flag_goal)      # victory    勝利したか
            self.arr_loss.append(val_loss)          # DQN-Experience Replay学習
            self.arr_epsilon.append(val_epsilon)    # イプシロン       
            # ----------
            # Q値の保管(プレイベース)
            self.arr_numQV.append(maxturn)      # QV値のN数
            self.arr_maxQV.append(np.max(self.arr_predQV))                 # QV値の最大値
            self.arr_q25QV.append(np.percentile(self.arr_predQV, q=25))    # QV値の4分の1値
            self.arr_q75QV.append(np.percentile(self.arr_predQV, q=75))    # QV値の4分の3値
            # ----------
            # 距離の保管(プレイベース)
            self.arr_maxTD.append(np.max(self.arr_Tdist))           # Total距離の最大値
            self.arr_sminRTD.append(np.max(self.arr_sminRT))        # 空間minRT距離の最大値
            self.arr_tlminRTD.append(np.max(self.arr_tlminRT))      # タイムラインminRT距離の最大値
            # ----------
            # しばらくすれば表示が消えます
            if iCnt_play%50 == 0:
                time.sleep(SLEEP_TIME)
                clear_output(wait=True)

        # --------------------------------------------------
        # 学習履歴を出力する
        fig = plt.figure(figsize=(14, 6))
        ax1 = fig.add_subplot(1, 2, 1)
        ax1.set_title('learning transition : epsilon')
        ax1.plot(self.arr_iplay, self.arr_epsilon, label="epsilon", color="blue")
        ax1.set_xlabel('#episode')
        ax1.set_ylabel('epsilon')
        ax1.legend(loc='best')
        # -----
        ax2 = fig.add_subplot(1, 2, 2)
        ax2.set_title('learning transition : Learning loss')
        ax2.set_xlabel('#episode')
        ax2.set_ylabel('Loss amount')
        ax2.plot(self.arr_iplay, self.arr_loss, label="loss", color="blue")
        ax2.legend(loc='best')
        plt.show()

        # --------------------------------------------------
        # グラフを表示する
        self.show_graph()

    # --------------------------------------------------
    # エピソードを運用する
    def get_episode(self, iCnt_play):

        # ---------------------------
        # 記録用パラメタ類(ターンベース)
        self.arr_iturn     = []    # ターン・カウンタリスト
        self.arr_orders    = []    # 指示リスト(アドレス)
        self.arr_acType    = []    # 指示のタイプ
        self.arr_scores    = []    # ゲームスコアリスト
        self.arr_dones     = []    # ゲームオーバーフラグ
        self.arr_predQV    = []    # Q値のリスト
        # ---------------------------
        # 記録用パラメタ類(ターンベース)
        self.arr_Tdist     = []    # Total距離
        self.arr_sminRT    = []    # 空間minRT距離
        self.arr_tlminRT   = []    # タイムラインminRT距離
        
        # ---------------------------
        # 初期化
        state = start_point
        score = 0
        for time in range(times):
            movables = maze_field.get_actions(state)
            acType, action, Qvalue, T_Dist, dist_sminRT, dist_tlminRT = dql_solver.choose_action(state, movables, time)
            reward, done = maze_field.get_val(action)
            score = score + reward
            next_state = action
            next_movables = maze_field.get_actions(next_state)
            dql_solver.remember_memory(state, action, reward, next_state, next_movables, time, done)
            if done or time == (times - 1):
                break
            state = next_state

            # ---------------------------
            # 記録用リストを追記する
            self.arr_iturn.append(time)    # ターン・カウンタリスト
            self.arr_orders.append(action)      # 指示リスト(アドレス)
            self.arr_acType.append(acType)      # 指示のタイプ
            self.arr_scores.append(reward)      # ゲームスコアリスト   
            self.arr_dones.append(done)         # ゲームオーバーフラグ
            self.arr_predQV.append(Qvalue)      # Q値のリスト
            # ---------------------------
            # 記録用パラメタ類(ターンベース)
            self.arr_Tdist.append(T_Dist)       # Total距離
            self.arr_sminRT.append(dist_sminRT)    # 空間minRT距離
            self.arr_tlminRT.append(dist_tlminRT)    # タイムラインminRT距離
            
        # ---------------------------
        # 結果の出力
        maxturn   = time
        maxscore  = score
        flag_goal = done

        return maxturn, maxscore, flag_goal

    # ----------------
    # 学習結果のグラフ化
    def show_graph(self):

        fig2 = plt.figure(figsize=(14, 12))
        # -----
        ax1 = fig2.add_subplot(2, 2, 1)
        ax1.set_title('DQN parameter transition: QValue')
        ax1.set_xlabel('#episode')
        ax1.set_ylabel('QValue')
        ax1.plot(self.arr_iplay, self.arr_maxQV, label="max_QV", color="blue")
        ax1.plot(self.arr_iplay, self.arr_q25QV, label="q25_QV", color="red")
        ax1.plot(self.arr_iplay, self.arr_q75QV, label="q75_QV", color="green")
        ax1.grid(True)
        ax1.legend(loc='best')
        # -----
        ax2 = fig2.add_subplot(2, 2, 2)
        ax2.set_title('learning transition : distance')
        ax2.set_xlabel('#episode')
        ax2.set_ylabel('maxturn')
        ax2.grid(True)
        ax2.scatter(self.arr_iplay, self.arr_maxTD, label="maxTD", color="blue")
        ax2.scatter(self.arr_iplay, self.arr_sminRTD, label="sminRTD", color="red")
        ax2.scatter(self.arr_iplay, self.arr_tlminRTD, label="tlminRTD", color="green")
        ax2.legend(loc='best')
        # -----
        # 移動平均を付ける
        y_rolling = pd.Series(self.arr_maxturn).rolling(window=12, center=True).mean()
        ax3 = fig2.add_subplot(2, 2, 3)
        ax3.set_title('learning transition : maxturn')
        ax3.set_xlabel('#episode')
        ax3.set_ylabel('maxturn')
        ax3.grid(True)
        ax3.plot(self.arr_iplay, self.arr_maxturn, label="original", color="blue")
        ax3.plot(self.arr_iplay, y_rolling, label="moving", color="red")
        ax3.legend(loc='best')
        # -----
        # 移動平均を付ける
        y_rolling = pd.Series(self.arr_maxscore).rolling(window=12, center=True).mean()
        ax4 = fig2.add_subplot(2, 2, 4)
        ax4.set_title('learning transition : maxscore')
        ax4.set_xlabel('#episode')
        ax4.set_ylabel('maxscore')
        ax4.grid(True)
        ax4.plot(self.arr_iplay, self.arr_maxscore, label="original", color="blue")
        ax4.plot(self.arr_iplay, y_rolling, label="moving", color="red")
        ax4.legend(loc='best')
        # -----
        fig2.tight_layout()
        #fig.savefig("./AAC_img.png")
        plt.show()

# ---------------------------
# ゲームを実行する
Agent()

```

QEU:FOUNDER : “それでは、結果をドン！！D先生、良くなりましたか？“

**（学習曲線）**

![imageRL4-5-4](https://reinforce.github.io/images/imageRL4-5-4.jpg)

**（パフォーマンス推移）**

![imageRL4-5-5](https://reinforce.github.io/images/imageRL4-5-5.jpg)

D先生 ： “あんまり、良くなったようには見えないですね。

QEU:FOUNDER : “右上のグラフに緑色のプロットが追加されました。これがタイムラインRTですが、パフォーマンスがわかりやすくなったでしょ？”

D先生 ： “すごくわかりやすくなりました。まだ学習が完了していないことがわかりますね。”

![imageRL4-5-6](https://reinforce.github.io/images/imageRL4-5-6.jpg)

QEU:FOUNDER : “ライムラインRTはゲームの評価に使いましょう。つぎは、いよいよフィードバック制御です。フィードバックあるなしでのパフォーマンスの差異がわかりやすくなります。それえは、次回に・・・。”

## ～ まとめ ～

QEU:FOUNDER ： “このタイムスというトコロは深いね。こんなこともあるのか・・・。”

![imageRL4-5-7](https://reinforce.github.io/images/imageRL4-5-7.jpg)

D先生 : “うわぁ・・・、大物ぞろいですね。そういえば、FOUNDERはパンフ下の「かっこいいおにいさん（〇馬プロジェクト）」が好きなんですね。”

QEU:FOUNDER ： “ずっと昔、Youtubeで若手の歴史学者と組んで近現代史のシリーズをやってました。あれは面白かったですよ。学校では近現代史は勉強できないからね。”

D先生 : “FOUNDERは、またもやショック？”

QEU:FOUNDER ： “ワタシ、心のソコから「カレを信じて」います。”

D先生 : “今はいろんな情報が氾濫して、真偽がわかりません。ソコって、ドコなんですか？”

![imageRL4-5-8](https://reinforce.github.io/images/imageRL4-5-8.jpg)

QEU:FOUNDER ： “ソコだよ・・・。”

