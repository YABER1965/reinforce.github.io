## QEUR21_MZPREP5: 行動(ACTION)を多様化する

### ～　次は、やっと本番3DMAZEへ・・・　～

QEU:FOUNDER ; “今回は、「3DMAZE準備編」の最後になります。思ったよりも、すんなり収束してくれました。もともと**「変則ONEHOT」**を使っていたから・・・。”

![imageRL5-6-1](https://reinforce.github.io/images/imageRL5-6-1.jpg) 

D先生 : “**行動が2次元ベクトル[0,1]型式で表現されていれば、8方向への拡張はカンタンですから**ね・・・。”

QEU：FOUNDER ; “あとは簡単にPythonコードをドン！！”

```python
# ----------------
# メイズ(MAZE)ゲームの強化学習システム
# step6A : 3Dメイズの準備段階です(DQNに方針変更、8direction)
# step6A : Reinforce_3DMazePrep_pyTorch(DQN_size27).py
# step6A : DQN-ERとsminRT距離ｘ3、さらにBoltzmann行動選択を導入する
# ---------------- 
# 数値計算のﾗｲﾌﾞﾗﾘ読み込み
import numpy as np
import pandas as pd
import math, copy, random, time
from collections import deque, Counter
from scipy.special import softmax
from IPython.display import clear_output
# ----------------
import torch
import torch.nn.functional
import torch.utils.data
# ---------------- 
import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = (16, 10)

# =================================================
# Game board difinition 
# =================================================
foldername  = "./MAZE_test/"

# =================================================
# difinition of STATE CALCULATION function
# =================================================
# タイムライン用minRTメトリックスを計算する
def timeline_minRT(a_row, a_col, iCnt_turn):
    len_route = len(mx_route)
    if iCnt_turn <= len_route - 1:
        pos_rt  = mx_route[iCnt_turn]
    else:
        pos_rt  = mx_route[len_route - 1]
    # -----
    tlEuc   = (a_row - pos_rt[0])**2 + (a_col - pos_rt[1])**2
    dist_tlminRT = np.sqrt(tlEuc)
        
    return round(dist_tlminRT,5)

# -------
# 空間用minRTメトリックスを計算する
def space_minRT(pos_route, a_row, a_col):
    len_route = len(pos_route)
    arr_sminRT = []
    for i in range(len_route):
        pos_rt  = pos_route[i]
        sqEuc   = (a_row - pos_rt[0])**2 + (a_col - pos_rt[1])**2
        distEuc = np.sqrt(sqEuc)
        arr_sminRT.append(round(distEuc,5))   
    dist_sminRT = np.min(arr_sminRT)
        
    return dist_sminRT

# -------
# 状態の表記を変更(XY座標へ)、空間用minRTメトリックスつき
def calc_address(a_row, a_col, mx_lm, iCnt_turn):

    add_state = np.zeros(2)
    for i in range(2):
        pos_lm  = mx_lm[i]
        sqEuc   = (a_row - pos_lm[0])**2 + (a_col - pos_lm[1])**2
        distEuc = np.sqrt(sqEuc)
        add_state[i] = round(distEuc,5)
             
    # 空間用minRTメトリックスを計算する
    # Aルート(標準)
    dist_sminRTA = space_minRT(mx_route, a_row, a_col)
    dist_sminRTA = round(dist_sminRTA,5)
    # Bルート(標準との差異分)
    dist_sminRTB = space_minRT(mx_routeB, a_row, a_col)
    dist_sminRTB = round(dist_sminRTB,5) - dist_sminRTA
    # Cルート(標準との差異分)
    dist_sminRTC = space_minRT(mx_routeC, a_row, a_col)
    dist_sminRTC = round(dist_sminRTC,5) - dist_sminRTA
    # タイムライン用minRTメトリックスを計算する
    dist_tlminRT = timeline_minRT(a_row, a_col, iCnt_turn)

    # --------------------------------------------------
    # 距離とりまとめリストを生成する
    arr_RTDists = np.zeros(5)
    arr_RTDists[0] = add_state[0] + add_state[1]     # Total距離
    arr_RTDists[1] = dist_sminRTA    # 空間minRT距離
    arr_RTDists[2] = dist_tlminRT    # タイムラインminRT距離
    arr_RTDists[3] = dist_sminRTB    # 空間用minRT距離(B)
    arr_RTDists[4] = dist_sminRTC    # 空間用minRT距離(C)

    # STATEを生成する
    state = np.hstack([[a_row, a_col], add_state, [dist_sminRTA, dist_sminRTB, dist_sminRTC]])
    
    return state, arr_RTDists

# ----------
# Maze Class
class Maze(object):
    def __init__(self, size=27, blocks_rate=0.1):
        self.size = size if size > 3 else 27
        self.blocks = int((size ** 2) * blocks_rate) 
        self.s_list = []
        self.maze_list = []
        self.e_list = []

    def create_mid_lines(self, k):
        if k == 0: self.maze_list.append(self.s_list)
        elif k == self.size - 1: self.maze_list.append(self.e_list)
        else:
            tmp_list = []
            for l in range(0,self.size):
                if l == 0: tmp_list.extend("#")
                elif l == self.size-1: tmp_list.extend("#")
                else:
                    a = random.randint(-1, 0)
                    tmp_list.extend([a])
            self.maze_list.append(tmp_list)

    def insert_blocks(self, k, s_r, e_r):
        b_y = random.randint(1, self.size-2)
        b_x = random.randint(1, self.size-2)
        if [b_y, b_x] == [1, s_r] or [b_y, b_x] == [self.size - 2, e_r]: k = k-1
        else: self.maze_list[b_y][b_x] = "#"
            
    def generate_maze(self): 
        s_r = random.randint(1, int((self.size / 2)) - 1)
        for i in range(0, self.size):
            if i == s_r: self.s_list.extend("S")
            else: self.s_list.extend("#")
        start_point = [0, s_r]

        e_r = random.randint(int((self.size / 2)) + 1, self.size - 2)
        for j in range(0, self.size):
            if j == e_r: self.e_list.extend([50])
            else: self.e_list.extend("#")
        goal_point = [self.size - 1, e_r]

        for k in range(0, self.size):
            self.create_mid_lines(k)
        
        for k in range(self.blocks):
            self.insert_blocks(k, s_r, e_r)

        return self.maze_list, start_point, goal_point

    # --------------------------------------------------
    # 盤面のCSVファイルを読み込み表示する
    def read_boardfile(self):
        # CSVファイルの読み込み
        code_csvout = "maze_board27.csv"       # file name
        file_readcsv = foldername + code_csvout   # standard(training) file name  
        df = pd.read_csv(file_readcsv)
        #max_play = len(df)
        # print("データ量 max_play",max_play)
        # print(df)
        # --------------------------------------------------
        # 選択項目の読み込み
        maze = df.loc[:, "0":"26"].values.tolist()
        start_point = [0,0]
        goal_point  = [0,0]
        for i in range(size):
            for j in range(size):
                if maze[i][j] == 'S':
                    start_point = [i, j]
                if maze[i][j] == '5000':
                    goal_point  = [i, j]

        self.maze_list   = maze
        self.start_point = start_point
        self.goal_point  = goal_point
        print("goal_point", goal_point)
        print("----- mx_maze(read_boardfile) -----")
        print(maze)

        return self.maze_list, start_point, goal_point

    # --------------------------------------------------
    # コマの最短ルートのCSVファイルを読み込み表示する（標準Aルート）
    def read_chessfile(self):
        # CSVファイルの読み込み
        code_csvout = "chess_route27.csv"       # file name
        file_readcsv = foldername + code_csvout   # standard(training) file name  
        df = pd.read_csv(file_readcsv)
        #max_play = len(df)
        # print("データ量 max_play",max_play)
        # print(df)
        # --------------------------------------------------
        # 選択項目の読み込み
        mx_route    = df.loc[:, "row":"col"].values.tolist()
        ref_action  = df.loc[:, "action"].values.tolist()
        print("ref_action: ", ref_action)
        #print("----- route -----")
        #print(route)

        return mx_route, ref_action

    # --------------------------------------------------
    # コマの最短ルートのCSVファイルを読み込み表示する（限界BCルート）
    def read_chessfileBC(self, code_csvout):

        # CSVファイルの読み込み
        file_readcsv = foldername + code_csvout   # standard(training) file name  
        df = pd.read_csv(file_readcsv)
        #max_play = len(df)
        # print("データ量 max_play",max_play)
        # print(df)
        # --------------------------------------------------
        # 選択項目の読み込み
        mx_route    = df.loc[:, "row":"col"].values.tolist()
        #print("----- route -----")
        #print(route)

        return mx_route

# ----------
# Maze functions
class Field(object):
    def __init__(self, maze, start_point, goal_point):
        # ----------------
        # 次の行動(ACTION-８方向)に進む
        # action number / row / col / 方向/ ベクトル(変則ONEHOT)
        # 0 / -1 / 0  / 上上(up-up) / [-1, 0]
        # 1 / -1 / -1  / 上左(up-left) / [-1, -1]
        # 2 / 0 / -1  / 左左(left-left) / [0, -1]
        # 3 / +1 / -1  / 左下(down-left) / [+1, -1]
        # 4 / +1 / 0  / 下下(down-down) / [+1, 0]
        # 5 / +1 / +1  / 下右(down-right) / [+1, +1]
        # 6 / 0  / +1 / 右右(right-right) / [0, +1]
        # 7 / -1  / +1 / 上右(up-right) / [-1, +1]
        # ----------------
        self.maze = maze
        self.start_point = start_point
        self.goal_point  = goal_point
        self.movable_vec = [[-1, 0],[-1, -1],[0, -1],[1, -1],[1, 0],[1, 1],[0, 1],[-1, 1]]

    def display(self, point=None):
        field_data = copy.deepcopy(self.maze)
        if not point is None:
                y, x = point
                field_data[y][x] = "@@"
        else:
                point = ""
        print("================")
        for line in field_data:
                print ("\t" + "%3s " * len(line) % tuple(line))
        print("================")

    def all_display(self, mx_route):
        field_data = copy.deepcopy(self.maze)
        for i in range(len(mx_route)):
            point = mx_route[i]
            y, x = point[0], point[1]
            field_data[y][x] = "@@"
        print("================")
        for line in field_data:
                print ("\t" + "%3s " * len(line) % tuple(line))
        print("================")

    def get_actions(self, state):
        movables = []
        if state == self.start_point:
            y = state[0] + 1
            x = state[1]
            a = [[y, x]]
            return a
        else:
            for v in self.movable_vec:
                y = state[0] + v[0]
                x = state[1] + v[1]
                if not(0 < x < len(self.maze) and
                       0 <= y <= len(self.maze) - 1 and
                       self.maze[y][x] != "#" and
                       self.maze[y][x] != "S"):
                    continue
                movables.append([y,x])
            if len(movables) != 0:
                return movables
            else:
                return None

    # --------------------------------------------------
    # 次のコマの「決定」位置と報酬、ゲームフラグを決める
    # 新概念：マスク「#」位置に着くと、スタート地点に戻る
    def get_val(self, action_chess, state_chess, movables):
        yPos = action_chess[0]
        xPos = action_chess[1]
        # ----------------
        if action_chess == self.start_point:
            nxt_state = action_chess
            return nxt_state, 0, False, 0
        elif self.maze[yPos][xPos] == "#":
            #print("movables: ", movables)
            if len(movables) > 0:
                nxt_state = random.choice(movables)
            else:
                nxt_state = self.start_point
            v = -0.1        # -10
            return nxt_state, v, False, 1
        else:
            nxt_state = action_chess
            v = float(self.maze[yPos][xPos])
            if action_chess == self.goal_point: 
                return nxt_state, v, True, 0
            else: 
                return nxt_state, v, False, 0

# ----------
# Initialize and Generate a maze
size = 27
barriar_rate = 0.1
maze_1 = Maze(size, barriar_rate)
# init_maze : カリキュラム学習用に改造
init_maze, start_point, goal_point = maze_1.read_boardfile()
print("start_point: {}, goal_point: {}".format(start_point, goal_point))
# インスタンスの生成
maze_field = Field(init_maze, start_point, goal_point)
# ボード表示(初期表示)
maze_field.display()
# ----------
# 最短ルートを読み込み(Aルート)
mx_route, ref_action = maze_1.read_chessfile()
# ルートを表示する
maze_field.all_display(mx_route)
# ----------
# コマの最短ルートのCSVファイルを読み込み表示する（限界BCルート）
# Bルート
mx_routeB = maze_1.read_chessfileBC("chess_route27B.csv")
# Cルート
mx_routeC = maze_1.read_chessfileBC("chess_route27C.csv")
#print("--- mx_routeC ---")
#print(mx_routeC)



#=================================================
# BOLTZMAN FUNCTION           
#=================================================
# ボルツマン選択による出現確率の調整
def calc_Boltzman(movables):

    iCnt, temp  = 0, 300.0
    mx_lm   = [[0, 2],[26, 25]]
    pos_lm  = mx_lm[1]
    arr_action, arr_Gdists, arr_sminRTA = [], [], []
    for a_order in movables:
        # ゴール距離
        sqGoal   = (a_order[0] - pos_lm[0])**2 + (a_order[1] - pos_lm[1])**2
        distGoal = np.sqrt(sqGoal)
        # 空間用minRTメトリックスを計算する
        # Aルート(標準)
        dist_sminRTA = space_minRT(mx_route, a_order[0], a_order[1])
        # リストに追加する
        arr_action.append(iCnt)
        arr_Gdists.append(round(distGoal,5))
        arr_sminRTA.append(round(dist_sminRTA,5))
        iCnt = iCnt + 1

    arr_probs = softmax(temp/(np.array(arr_Gdists)+np.array(arr_sminRTA)+10.0))
    #print("movables:{}, arr_probs:{}".format(movables, arr_probs))

    return arr_action, arr_probs

#=================================================
# Deep Learning Model class            
#=================================================
# PyTorchのDLの定義
class Net(torch.nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = torch.nn.Linear(dim_input, 128)
        self.fc1.weight.data.normal_(0, 0.1)
        self.fc2 = torch.nn.Linear(128, 128)
        self.fc2.weight.data.normal_(0, 0.1)
        self.fc3 = torch.nn.Linear(128, 128)
        self.fc3.weight.data.normal_(0, 0.1)
        self.fc4 = torch.nn.Linear(128, dim_output)
        self.fc4.weight.data.normal_(0, 0.1)
        
    def forward(self, x):
        x = torch.nn.functional.relu(self.fc1(x))
        x = torch.nn.functional.relu(self.fc2(x))
        x = torch.nn.functional.relu(self.fc3(x))
        x = self.fc4(x)
        return x

# =======================================
# =======================================
# Solving the maze in Deep Q-learning
class DQN_Solver:
    def __init__(self, state_size, action_size):
        self.state_size = state_size # list size of state
        self.action_size = action_size # list size of action
        self.memory = deque(maxlen=5000) # memory space
        self.gamma  = 0.96 # discount rate
        self.epsilon = 0.99 # randomness of choosing random action or the best one
        self.e_decay = 0.9992 # epsilon decay rate
        self.e_min  = 0.01 # minimum rate of epsilon
        self.learning_rate = 0.002 # learning rate of neural network
        # --------------------------------------------------
        # crate instance for input
        self.eval_net, self.target_net = Net(), Net()                           # Netを利用して２つのニューラルネットをつくる
        self.learn_step_counter = 0    
        # --------------------------------------------------
        # Save and load the model via state_dict
        #self.eval_net.load_state_dict(torch.load(file_input_model))
        # --------------------------------------------------
        # set validaiton mode
        self.eval_net.eval()
        # --------------------------------------------------
        # set training parameters
        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=self.learning_rate)
        self.criterion = torch.nn.MSELoss()

    # ----------------
    # PyTorchモデルの保存  
    #def save_models(self):
        #torch.save(self.eval_net.state_dict(), file_output_model)

    # remember state, action, its reward, next state and next possible action. done means boolean for goal
    def remember_memory(self, state, action, reward, next_state, next_movables, done):
        self.memory.append((state, action, reward, next_state, next_movables, done))

    # choosing action depending on epsilon
    def choose_action(self, state, movables):
        acType = "random"
        Qvalue = -0.001
        #print("movables: ", movables)
        if self.epsilon >= random.random():
            # ボルツマン選択による出現確率の調整
            arr_action, arr_probs = calc_Boltzman(movables)
            # randomly choosing action
            temp_action  = np.random.choice(arr_action, p=arr_probs)
            action_chess = movables[temp_action]
            return acType, action_chess, Qvalue
        else:
            # choosing the best action from model.predict()
            acType, action_chess, Qvalue = self.choose_best_action(state, movables)
            return acType, action_chess, Qvalue
        
    # choose the best action to maximize reward expectation
    def choose_best_action(self, state, movables):

        # ----------------
        # 次の行動(ACTION-８方向)に進む
        # action number / row / col / 方向/ ベクトル
        # 0 / -1 / 0  / 上上(up-up) / [-1, 0]
        # 1 / -1 / -1  / 上左(up-left) / [-1, -1]
        # 2 / 0 / -1  / 左左(left-left) / [0, -1]
        # 3 / +1 / -1  / 左下(down-left) / [+1, -1]
        # 4 / +1 / 0  / 下下(down-down) / [+1, 0]
        # 5 / +1 / +1  / 下右(down-right) / [+1, +1]
        # 6 / 0  / +1 / 右右(right-right) / [0, +1]
        # 7 / -1  / +1 / 上右(up-right) / [-1, +1]
        # ----------------
        action_chess = [0,0]
        acType = "machine"
        Qvalue = -0.001
        iCnt = 0
        for a_order in movables:

            # ONE-HOT化
            diff_row = a_order[0] - state[0]
            diff_col = a_order[1] - state[1]
            # ----------------
            # 状態(STATE)行動(ACTION)ベクトルを生成する
            np_action = np.hstack([state, [diff_row, diff_col]])
            #print("np_action: ",np_action)
            if iCnt == 0:
                mx_input   = [np_action]  # 状態(S)行動(A)マトリックス
            else:
                mx_input   = np.concatenate([mx_input, [np_action]], axis=0)     # 状態(S)行動(A)マトリックス
            iCnt = iCnt + 1
        # print("----- mx_input -----")
        # print(mx_input)
        # --------------------------------------------------
        # generate new 'x'
        x_input_tensor = torch.tensor(mx_input).float()
        # predict 'y'
        with torch.no_grad():
            y_pred_tensor = self.eval_net(x_input_tensor)
        # convert tensor to numpy
        y_pred       = y_pred_tensor.data.numpy().flatten()
        Qvalue       = np.max(y_pred)
        temp_order   = np.argmax(y_pred)
        action_chess = movables[temp_order]
        #print("state:{0}, y_pred:{1}, temp_order:{2}, action_chess:{3}".format(state, y_pred, temp_order, action_chess))

        return acType, action_chess, Qvalue


    # this experience replay is going to train the model from memorized states, actions and rewards
    def replay_experience(self, batch_size):

        # ターゲットNetのパラメタを更新する
        if self.learn_step_counter < 5 or self.learn_step_counter % TARGET_REPLACE_ITER == 0:                  # 最初5回は毎回に更新し、あとは定期的に更新する
            self.target_net.load_state_dict(self.eval_net.state_dict())         # 評価NetのパラメタをターゲットNetに引き渡す
        self.learn_step_counter += 1   
        # --------------------------------------------------
        batch_size = min(batch_size, len(self.memory))
        minibatch = random.sample(self.memory, batch_size)
        X, Y = [], []
        # --------------------------------------------------
        for ibat in range(batch_size):
            state, action_chess, reward, next_state, next_movables, done = minibatch[ibat]

            # ----------------
            # 次の行動(ACTION-８方向)に進む
            # action number / row / col / 方向/ ベクトル
            # 0 / -1 / 0  / 上上(up-up) / [-1, 0]
            # 1 / -1 / -1  / 上左(up-left) / [-1, -1]
            # 2 / 0 / -1  / 左左(left-left) / [0, -1]
            # 3 / +1 / -1  / 左下(down-left) / [+1, -1]
            # 4 / +1 / 0  / 下下(down-down) / [+1, 0]
            # 5 / +1 / +1  / 下右(down-right) / [+1, +1]
            # 6 / 0  / +1 / 右右(right-right) / [0, +1]
            # 7 / -1  / +1 / 上右(up-right) / [-1, +1]
            # ----------------
            # ONE-HOT化
            diff_row = action_chess[0] - state[0]
            diff_col = action_chess[1] - state[1]
            # ----------------
            # 状態(STATE)行動(ACTION)ベクトルを生成する
            state_action_curr = np.hstack([state, [diff_row, diff_col]])
            #print("state_action_curr",state_action_curr)
            # ----------------
            if done:
                target_f = reward
            else:
                next_rewards = []
                iCnt = 0
                for a_order in next_movables:

                    # ONE-HOT化
                    diff_row = a_order[0] - next_state[0]
                    diff_col = a_order[1] - next_state[1]
                    # ----------------
                    # 状態(STATE)行動(ACTION)ベクトルを生成する
                    np_next_s_a = np.hstack([next_state, [diff_row, diff_col]])
                    if iCnt == 0:
                        mx_input = [np_next_s_a]  # 状態(S)行動(A)マトリックス
                    else:
                        mx_input = np.concatenate([mx_input, [np_next_s_a]], axis=0)     # 状態(S)行動(A)マトリックス
                    iCnt = iCnt + 1
                # ----------------
                # generate new 'x'
                x_input_tensor = torch.tensor(mx_input).float()
                # predict 'y'
                with torch.no_grad():
                    y_pred_tensor = self.target_net(x_input_tensor)
                # convert tensor to numpy
                y_pred = y_pred_tensor.data.numpy().flatten()
                np_n_r_max = np.amax(y_pred)
                target_f = reward + self.gamma * np_n_r_max
            # ----------------
            if ibat == 0:
                X = np.array([state_action_curr])  # 状態(S)行動(A)マトリックス
            else:
                X = np.concatenate([X, np.array([state_action_curr])], axis=0)  # 状態(S)行動(A)マトリックス
            Y = np.append(Y,target_f)
        # --------------------------------------------------
        # TRAINING
        # convert numpy array to tensor
        state_action_eval = torch.from_numpy(X).float()
        q_target = torch.from_numpy(Y.reshape(-1, 1)).float()
        #print("state_action_eval:",state_action_eval)
        #print("q_target:",q_target)
        # --- building model ---
        q_eval = self.eval_net(state_action_eval)
        # calculate loss
        loss = self.criterion(q_eval, q_target)
        # update weights
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        # Show progress
        #print('learn done -- [epsilon: {0}, loss: {1}]'.format(self.epsilon, loss))
        if self.epsilon > self.e_min:
            self.epsilon *= self.e_decay

        return round(self.epsilon,5) , round(loss.data.item(),5)


#=================================================
# Calculation class(4) : Agent
#=================================================
class Agent():
    # -----
    def __init__(self):

        # ---------------------------
        # 盤面データを読む
        self.mx_lm         = [[0, 2],[26, 25]]
        self.maze_field    = maze_field
        self.start_point   = start_point
        self.goal_point    = goal_point
        self.stage_info    = "stage1"
        # ---------------------------
        # 記録用パラメタ類(プレイベース)
        self.arr_iplay     = []  # count game play    プレイ番号
        self.arr_maxturn   = []  # turn game play    ターン数
        self.arr_maxscore  = []  # rl_score game play    報酬の総和
        self.arr_victory   = []  # victory    勝利したか
        self.arr_loss      = []  # DQN-Experience Replay学習
        self.arr_epsilon   = []  # ε-Greedy        
        # ---------------------------
        # Q値の分析用(プレイベース)
        self.arr_maxQV     = []  # QV値の最大値
        self.arr_q25QV     = []  # QV値の4分の1値
        self.arr_q75QV     = []  # QV値の4分の3値
        # ---------------------------
        # 距離の分析用累積リスト
        self.arr_maxTD     = []  # Total距離
        self.arr_sminRTD   = []  # 空間用minRT距離(A)
        self.arr_tlminRTD  = []  # タイムラインminRT距離
        self.arr_sminRTBD  = []  # 空間用minRT距離(B)
        self.arr_sminRTCD  = []  # 空間用minRT距離(C)

        # エピソードの実行
        for iCnt_play in range(NUMS_EPISODES):

            # エピソードを実行する
            maxturn, maxscore, flag_goal = self.get_episode(iCnt_play)
            # ========================================
            # DQN-Experience Replay学習  
            val_epsilon, val_loss = dql_solver.replay_experience(BATCH_SIZE)
            # QValueリストを生成する
            val_maxQV = round(np.max(self.arr_predQV), 5)                 # QV値の最大値
            val_q25QV = round(np.percentile(self.arr_predQV, q=25), 5)    # QV値の4分の1値
            val_q75QV = round(np.percentile(self.arr_predQV, q=75), 5)    # QV値の4分の3値
            list_QVs  = [val_maxQV, val_q25QV, val_q75QV]
            # 学習結果の出力
            if iCnt_play % 2 == 0:
                print("iplay:{0}, stage:{1}, maxturn:{2}, maxscore:{3}, FGoal:{4}, Eps:{5}, loss:{6}, QVs:{7}".format(iCnt_play, self.stage_info, maxturn, maxscore, flag_goal, val_epsilon, val_loss, list_QVs))

            # ----------
            # 記録用パラメタ類(プレイベース)の追加
            self.arr_iplay.append(iCnt_play)        # count game play    プレイ番号
            self.arr_maxturn.append(maxturn)        # max_turn   ゲームのターン数
            self.arr_maxscore.append(maxscore)      # rl_score game play    最終プレイスコア
            self.arr_victory.append(flag_goal)      # victory    勝利したか
            self.arr_loss.append(val_loss)          # DQN-Experience Replay学習
            self.arr_epsilon.append(val_epsilon)    # イプシロン       
            # ----------
            # Q値の保管(プレイベース)
            self.arr_maxQV.append(val_maxQV)        # QV値の最大値
            self.arr_q25QV.append(val_q25QV)        # QV値の4分の1値
            self.arr_q75QV.append(val_q75QV)        # QV値の4分の3値
            # ----------
            # 距離の保管(プレイベース)
            self.arr_maxTD.append(np.mean(self.arr_Tdist))        # Total距離の平均値
            self.arr_sminRTD.append(np.mean(self.arr_sminRT))       # 空間用minRT距離(A)の平均値
            self.arr_tlminRTD.append(np.mean(self.arr_tlminRT))      # タイムラインminRT距離の平均値
            self.arr_sminRTBD.append(np.mean(self.arr_sminRTB))       # 空間用minRT距離(B)の平均値
            self.arr_sminRTCD.append(np.mean(self.arr_sminRTC))       # 空間用minRT距離(C)の平均値
            # ----------
            # しばらくすれば表示が消えます
            if iCnt_play%50 == 0:
                time.sleep(SLEEP_TIME)
                clear_output(wait=True)

        # --------------------------------------------------
        # 学習履歴を出力する
        fig = plt.figure(figsize=(14, 9))
        ax1 = fig.add_subplot(2, 2, 1)
        ax1.set_title('learning transition : epsilon')
        ax1.plot(self.arr_iplay, self.arr_epsilon, label="epsilon", color="blue")
        ax1.set_xlabel('#episode')
        ax1.set_ylabel('epsilon')
        ax1.legend(loc='best')
        # -----
        ax2 = fig.add_subplot(2, 2, 2)
        ax2.set_title('learning transition : Learning loss')
        ax2.set_xlabel('#episode')
        ax2.set_ylabel('Loss amount')
        ax2.plot(self.arr_iplay, self.arr_loss, label="loss", color="blue")
        ax2.legend(loc='best')
        # -----
        # 移動平均を付ける
        y_rolling = pd.Series(self.arr_maxturn).rolling(window=12, center=True).mean()
        ax3 = fig.add_subplot(2, 2, 3)
        ax3.set_title('learning transition : maxturn')
        ax3.set_xlabel('#episode')
        ax3.set_ylabel('maxturn')
        ax3.grid(True)
        ax3.plot(self.arr_iplay, self.arr_maxturn, label="original", color="blue")
        ax3.plot(self.arr_iplay, y_rolling, label="moving", color="red")
        ax3.legend(loc='best')
        # -----
        # 移動平均を付ける
        y_rolling = pd.Series(self.arr_maxscore).rolling(window=12, center=True).mean()
        ax4 = fig.add_subplot(2, 2, 4)
        ax4.set_title('learning transition : maxscore')
        ax4.set_xlabel('#episode')
        ax4.set_ylabel('maxscore')
        ax4.grid(True)
        ax4.plot(self.arr_iplay, self.arr_maxscore, label="original", color="blue")
        ax4.plot(self.arr_iplay, y_rolling, label="moving", color="red")
        ax4.legend(loc='best')
        # -----
        fig.tight_layout()
        plt.show()
        
        # --------------------------------------------------
        # グラフを表示する
        self.show_graph()


    # --------------------------------------------------
    # エピソードを運用する
    def get_episode(self, iCnt_play):

        # 記録用パラメタ類(ターンベース)
        self.arr_iturn     = []    # ターン・カウンタリスト
        self.arr_orders    = []    # 指示リスト(アドレス)
        self.arr_acType    = []    # 指示のタイプ
        self.arr_scores    = []    # ゲームスコアリスト
        self.arr_dones     = []    # ゲームオーバーフラグ
        self.arr_predQV    = []    # Q値のリスト
        # ---------------------------
        # 記録用パラメタ類(ターンベース)
        self.arr_Tdist     = []    # Total距離
        self.arr_sminRT    = []    # 空間minRT距離
        self.arr_tlminRT   = []    # タイムラインminRT距離
        self.arr_sminRTB   = []    # 空間用minRT距離(B)
        self.arr_sminRTC   = []    # 空間用minRT距離(C)
        # ---------------------------
        # ゲームのリセット
        state_chess = start_point
        score , iCnt_turn, done = 0, 0, False
        # 状態(STATE)を生成する
        state, arr_RTDists = calc_address(state_chess[0], state_chess[1], self.mx_lm, iCnt_turn) 
        movables = maze_field.get_actions(state_chess)
        # ---------------------------
        for iCnt_turn in range(NUMS_TURNS - 1):

            # 次の行動(ACTION)を発行する
            acType, action_chess, Qvalue = dql_solver.choose_action(state, movables)
            temp_action_chess, temp_reward, done, num_bump = maze_field.get_val(action_chess, state_chess, movables)
            next_state_chess  = action_chess
            next_movables     = maze_field.get_actions(next_state_chess)
            #print("num_bump:{}, action_chess:{}, state_chess:{}, movables:{}".format(num_bump, action_chess, state_chess, movables))
            # ----------------
            T_Dist        = arr_RTDists[0]    # Total距離
            dist_sminRT   = arr_RTDists[1]    # 空間minRT距離
            dist_tlminRT  = arr_RTDists[2]    # タイムラインminRT距離
            dist_sminRTB  = arr_RTDists[3]    # 空間用minRT距離(B)
            dist_sminRTC  = arr_RTDists[4]    # 空間用minRT距離(C)
            # ----------------
            # ゴールに到達した
            if iCnt_play == 500:
                self.stage_info    = "stage2"
                dql_solver.epsilon = 0.99
            elif iCnt_play == 1000:
                self.stage_info    = "stage3"
                dql_solver.epsilon = 0.99
            # ----------------
            # ゴールに到達した
            if iCnt_play < 500 and next_state_chess[0] >= 15 and next_state_chess[1] >= 15:
                done = True
                reward = 10000 - 5*iCnt_turn
                nominal_reward = 10000
            elif iCnt_play >= 500 and iCnt_play < 1000 and next_state_chess[0] >= 20 and next_state_chess[1] >= 20:
                done = True
                reward = 10000 - 5*iCnt_turn
                nominal_reward = 10000
            elif iCnt_play >= 1000 and next_state_chess[0] >= 25 and next_state_chess[1] >= 25:
                done = True
                reward = 10000 - 5*iCnt_turn
                nominal_reward = 10000
            else:
                # [重要]フィードバック制御あり
                diff_sminRTB = dist_sminRTB
                diff_sminRTC = dist_sminRTC
                if diff_sminRTB > 0:
                    diff_sminRTB = 0
                if diff_sminRTC > 0:
                    diff_sminRTC = 0
                reward = temp_reward + 30 * diff_sminRTB + 30 * diff_sminRTC - 20 * dist_sminRT
                nominal_reward = temp_reward
            # ----------------
            # 状態(STATE)を生成する
            next_state, next_arr_RTDists = calc_address(next_state_chess[0], next_state_chess[1], self.mx_lm, iCnt_turn + 1)  
            dql_solver.remember_memory(state, action_chess, reward, next_state, next_movables, done)
            # ---------------------------
            # 記録用リストを追記する
            self.arr_iturn.append(time)    # ターン・カウンタリスト
            self.arr_orders.append(action_chess)      # 指示リスト(アドレス)
            self.arr_acType.append(acType)      # 指示のタイプ
            self.arr_scores.append(reward)      # ゲームスコアリスト   
            self.arr_dones.append(done)         # ゲームオーバーフラグ
            self.arr_predQV.append(Qvalue)      # Q値のリスト
            # ---------------------------
            # 記録用パラメタ類(ターンベース)
            self.arr_Tdist.append(T_Dist)       # Total距離
            self.arr_sminRT.append(dist_sminRT)     # 空間minRT距離
            self.arr_tlminRT.append(dist_tlminRT)    # タイムラインminRT距離
            self.arr_sminRTB.append(dist_sminRTB)    # 空間用minRT距離(B)
            self.arr_sminRTC.append(dist_sminRTC)    # 空間用minRT距離(C)
            # ---------------------------
            if done:
                break
            state       = next_state
            state_chess = next_state_chess
            arr_RTDists = next_arr_RTDists
            movables    = next_movables
            score += nominal_reward
        # ---------------------------
        # エピソードの実行結果を出力する
        maxturn   = iCnt_turn
        maxscore  = int(score)
        flag_goal = done

        return maxturn, maxscore, flag_goal


    # ----------------
    # 学習結果のグラフ化
    def show_graph(self):

        fig2 = plt.figure(figsize=(14, 12))
        # -----
        ax1 = fig2.add_subplot(2, 2, 1)
        ax1.set_title('DQN parameter transition: QValue')
        ax1.set_xlabel('#episode')
        ax1.set_ylabel('QValue')
        ax1.plot(self.arr_iplay, self.arr_maxQV, label="max_QV", color="blue")
        ax1.plot(self.arr_iplay, self.arr_q25QV, label="q25_QV", color="red")
        ax1.plot(self.arr_iplay, self.arr_q75QV, label="q75_QV", color="green")
        ax1.grid(True)
        ax1.legend(loc='best')
        # -----
        ax2 = fig2.add_subplot(2, 2, 2)
        ax2.set_title('learning transition : distance')
        ax2.set_xlabel('#episode')
        ax2.set_ylabel('Distance')
        ax2.grid(True)
        ax2.scatter(self.arr_iplay, self.arr_maxTD, label="maxTD", color="blue")
        ax2.scatter(self.arr_iplay, self.arr_sminRTD, label="sminRTD", color="red")
        ax2.scatter(self.arr_iplay, self.arr_tlminRTD, label="tlminRTD", color="green")
        ax2.legend(loc='best')
        # -----
        ax3 = fig2.add_subplot(2, 2, 3)
        # 移動平均を付ける
        y_rolling = pd.Series(self.arr_sminRTBD).rolling(window=12, center=True).mean()
        ax3.set_title('learning transition : SminRTD distance[limit-B]')
        ax3.set_xlabel('#episode')
        ax3.set_ylabel('distance')
        ax3.grid(True)
        ax3.plot(self.arr_iplay, self.arr_sminRTBD, label="limitB", color="blue")
        ax3.plot(self.arr_iplay, y_rolling, label="moving", color="red")
        ax3.legend(loc='best')
        # -----
        ax4 = fig2.add_subplot(2, 2, 4)
        # 移動平均を付ける
        y_rolling = pd.Series(self.arr_sminRTCD).rolling(window=12, center=True).mean()
        ax4.set_title('learning transition : SminRTD distance[limit-C]')
        ax4.set_xlabel('#episode')
        ax4.set_ylabel('distance')
        ax4.grid(True)
        ax4.plot(self.arr_iplay, self.arr_sminRTCD, label="limitC", color="blue")
        ax4.plot(self.arr_iplay, y_rolling, label="moving", color="red")
        ax4.legend(loc='best')
        # -----
        fig2.tight_layout()
        #fig.savefig("./AAC_img.png")
        plt.show()


if __name__ == '__main__':

    # ----------
    # Hyper Parameters
    SLEEP_TIME  = 0.01
    TARGET_REPLACE_ITER = 50           # ターゲットNet更新頻度
    state_size  = 7
    action_size = 2
    dim_input   = state_size + action_size
    dim_output  = 1
    dql_solver  = DQN_Solver(state_size, action_size)
    BATCH_SIZE  = 256                  # サンプルサイズ

    # ----------
    # Game Parameters
    NUMS_EPISODES   = 3000
    NUMS_TURNS      = 8000

    # ----------
    # 出力用:Pytorchモデルのファイル名
    #comment_output_model = "initial"
    #code_output_model = "model_cliffwalk_DQNER_{0}.pt".format(comment_output_model)  # モデルの指定
    #file_output_model = foldername + code_output_model  # ファイルパス名の生成

    # ----------
    # 強化学習を実行する
    Agent()

```

QEU：FOUNDER ; “ああ・・・、うまく行ってよかったわぁ・・・。”

**（学習曲線）**

![imageRL5-6-2](https://reinforce.github.io/images/imageRL5-6-2.jpg) 

**（パフォーマンス推移）**

![imageRL5-6-3](https://reinforce.github.io/images/imageRL5-6-3.jpg) 

D先生 : “う～ん、理想的な動きだ・・・。”

![imageRL5-6-4](https://reinforce.github.io/images/imageRL5-6-4.jpg) 

QEU：FOUNDER ; “B距離（限界）とC距離（限界）のグラフに注目してください。この値は、A距離（標準）その差を取っているんで、**両方の値が同時にプラスになるということは「トラック（道路）の上」を走っているということ**なんです。”

![imageRL5-6-5](https://reinforce.github.io/images/imageRL5-6-5.jpg) 

D先生 ; “いよいよ次に、行きましょ(笑)。イラストの出自は「また一歩野望に近づいた」でググッてください。”


## ～　まとめ　～

QEU:FOUNDER ： “ちょっと視点を変えようか・・・。動画をドン！！”

## <ミ〇ネ屋と比べて腰が引けてるって言ってるんです>

<iframe width="560" height="315" src="https://www.youtube.com/embed/zSFassZb3YQ" ti-tle="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; en-crypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

D部長: “あれ？これは、前回の動画の切り抜きじゃないですか・・・。”

QEU:FOUNDER ： “いや、あまりにも**見事な切り抜き**で・・・。もう、拍手喝采・・・(笑)。”

![imageRL5-6-6](https://reinforce.github.io/images/imageRL5-6-6.jpg) 

QEU:FOUNDER ; “政治系ジャンルにおける、切り抜き動画の重要性を再認識したわ・・・。”

