## QEUR21_MZDYNQ5: QEUの考えるModel-Basedとは(DQN版その１)

## ～　今回は準備運動なので、ゆるく・・・　～

D先生 : “さて、今回から2D-MAZEの強化学習のDQNをやります。今回は、はじめなのでQ関数の初期値を作成しましょう。ちなみに、今回提案する強化学習は、以下の「モデル化の条件」が満たせたので可能になりました。”

**(今回のプロジェクトにおける「モデル」とは)**
- 同じ問題を解くための、Qテーブル解法とDQN解法がある
- Goal距離とminRT距離で、XY座標の代替が可能である
- Goal距離とminRT距離を使えば、最短ルート（移動命令群）の計算が可能である

QEU：FOUNDER ; “・・・思うんだけど、強化学習ってことばは合わないと思います。「fine-tuning」ってどう？何はともあれ、プログラムをドン！！基本は、いままでのプログラムをチョコっといじればできますよ。”

```python
# ----------------
# メイズ(MAZE)ゲームの強化学習システム
# step5A : 3Dメイズの再準備段階です
# step5A : Q2DMaze_DynaQ_Keras_fitting(DQN_size27).py
# step5A : DQNのためのQ関数の初期値を準備する
# ---------------- 
# 数値計算のﾗｲﾌﾞﾗﾘ読み込み
import numpy as np
import pandas as pd
import math, copy, random, time
from collections import defaultdict, deque, Counter
from IPython.display import clear_output
# ---------------- 
# Kerasをインポートする
from tensorflow.python.keras.models import Sequential
from tensorflow.python.keras.layers import Dense
from tensorflow.python.keras.models import load_model
# ---------------- 
#import seaborn as sns
import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = (16, 10)

# =================================================
# Game board difinition 
# =================================================
foldername  = "./MAZE_test/"

# =================================================
# difinition of STATE CALCULATION function
# =================================================
# 空間用minRTメトリックスを計算する
def space_minRT(pos_route, a_row, a_col):
    len_route = len(pos_route)
    arr_sminRT = []
    for i in range(len_route):
        pos_rt  = pos_route[i]
        sqEuc   = (a_row - pos_rt[0])**2 + (a_col - pos_rt[1])**2
        distEuc = np.sqrt(sqEuc)
        arr_sminRT.append(round(distEuc,5))   
    dist_sminRT = np.min(arr_sminRT)
        
    return dist_sminRT

# 状態の表記を変更(XY座標へ)、空間用minRTメトリックスつき
def calc_address(a_row, a_col, mx_lm):

    add_state = np.zeros(4)
    for i in range(len(mx_lm)):
        pos_lm  = mx_lm[i]
        sqEuc   = (a_row - pos_lm[0])**2 + (a_col - pos_lm[1])**2
        distEuc = np.sqrt(sqEuc)
        add_state[i] = round(distEuc,5)
    # --------------------------------------------------
    # 空間用minRTメトリックスを計算する
    # Aルート(標準)
    dist_sminRTA = space_minRT(mx_route, a_row, a_col)
    dist_sminRTA = round(dist_sminRTA,5)
    # Bルート(標準との差異分)
    dist_sminRTB = space_minRT(mx_routeB, a_row, a_col)
    dist_sminRTB = round(dist_sminRTB,5) - dist_sminRTA
    # Cルート(標準との差異分)
    dist_sminRTC = space_minRT(mx_routeC, a_row, a_col)
    dist_sminRTC = round(dist_sminRTC,5) - dist_sminRTA
    # Aルート値の符合化
    if add_state[2] > add_state[3]:
        dist_sminRTA = -1 * dist_sminRTA
    # --------------------------------------------------
    # 距離とりまとめリストを生成する
    arr_RTDists = np.zeros(4)
    arr_RTDists[0] = add_state[1]    # Goal距離
    arr_RTDists[1] = dist_sminRTA    # 空間minRT距離
    arr_RTDists[2] = dist_sminRTB    # 空間用minRT距離(B)
    arr_RTDists[3] = dist_sminRTC    # 空間用minRT距離(C)
    # --------------------------------------------------
    # STATEを生成する
    state   = np.array([add_state[1], dist_sminRTA, dist_sminRTB, dist_sminRTC])
    
    return state, arr_RTDists
    
# Qテーブル用アクションを決める
def calc_QTaction(i, j, movables, mx_lm):

    movable_vec = [[-1,0],[1,0],[0,-1],[0,1]]
    arr_action  = []
    for a_order in movables:
        # action number / row / col / 方向 / onehot
        # 0 / -1 / 0  / 上(up) / 1000
        # 1 / +1 / 0  / 下(down) / 0100
        # 2 / 0  / -1 / 左(left) / 0010
        # 3 / 0  / +1 / 右(right) / 0001
        # ----------
        yPos, xPos  = i, j 
        pos_movable = movable_vec[a_order]
        yPos, xPos  = i + pos_movable[0], j + pos_movable[1]
        # 状態の表記を変更(XY座標へ)、空間用minRTメトリックスつき
        state, arr_RTDists = calc_address(yPos, xPos, mx_lm)
        # ゴール距離
        goal_Dist   = round(arr_RTDists[0],5)
        # 空間用minRTメトリックス(Aルート)を計算する
        minRTA_Dist = round(arr_RTDists[1],5)
        # 評価指数を計算する
        val_action  = 100/((0.4*goal_Dist/10)**2+minRTA_Dist**2+1.0)
        # リストに追加する
        arr_action.append(round(val_action,5))
    # -----
    temp_action = np.argmax(arr_action)
    action      = movables[temp_action]

    return action

# ----------
# Maze Class
class Maze(object):
    def __init__(self, size=27, blocks_rate=0.1):
        self.size = size if size > 3 else 27
        self.blocks = int((size ** 2) * blocks_rate) 
        self.s_list = []
        self.maze_list = []
        self.e_list = []

    def create_mid_lines(self, k):
        if k == 0: self.maze_list.append(self.s_list)
        elif k == self.size - 1: self.maze_list.append(self.e_list)
        else:
            tmp_list = []
            for l in range(0,self.size):
                if l == 0: tmp_list.extend("#")
                elif l == self.size-1: tmp_list.extend("#")
                else:
                    a = random.randint(-1, 0)
                    tmp_list.extend([a])
            self.maze_list.append(tmp_list)

    def insert_blocks(self, k, s_r, e_r):
        b_y = random.randint(1, self.size-2)
        b_x = random.randint(1, self.size-2)
        if [b_y, b_x] == [1, s_r] or [b_y, b_x] == [self.size - 2, e_r]: k = k-1
        else: self.maze_list[b_y][b_x] = "#"
            
    def generate_maze(self): 
        s_r = random.randint(1, int((self.size / 2)) - 1)
        for i in range(0, self.size):
            if i == s_r: self.s_list.extend("S")
            else: self.s_list.extend("#")
        start_point = [0, s_r]

        e_r = random.randint(int((self.size / 2)) + 1, self.size - 2)
        for j in range(0, self.size):
            if j == e_r: self.e_list.extend([50])
            else: self.e_list.extend("#")
        goal_point = [self.size - 1, e_r]

        for k in range(0, self.size):
            self.create_mid_lines(k)
        
        for k in range(self.blocks):
            self.insert_blocks(k, s_r, e_r)

        return self.maze_list, start_point, goal_point

    # --------------------------------------------------
    # 盤面のCSVファイルを読み込み表示する
    def read_boardfile(self):
        # CSVファイルの読み込み
        code_csvout = "maze_board27.csv"       # file name
        file_readcsv = foldername + code_csvout   # standard(training) file name  
        df = pd.read_csv(file_readcsv)
        #max_play = len(df)
        # print("データ量 max_play",max_play)
        # print(df)
        # --------------------------------------------------
        # 選択項目の読み込み
        maze = df.loc[:, "0":"26"].values.tolist()
        start_point = ( 0, 0 )
        goal_point  = ( 0, 0 )
        for i in range(size):
            for j in range(size):
                if maze[i][j] == 'S':
                    start_point = ( i, j )
                if maze[i][j] == '5000':
                    goal_point  = ( i, j )

        self.maze_list   = maze
        self.start_point = start_point
        self.goal_point  = goal_point
        print("goal_point", goal_point)
        print("----- mx_maze(read_boardfile) -----")
        print(maze)

        return self.maze_list, start_point, goal_point

    # --------------------------------------------------
    # コマの最短ルートのCSVファイルを読み込み表示する（標準Aルート）
    def read_chessfile(self):
        # CSVファイルの読み込み
        code_csvout = "chess_route27.csv"       # file name
        file_readcsv = foldername + code_csvout   # standard(training) file name  
        df = pd.read_csv(file_readcsv)
        # --------------------------------------------------
        # 選択項目の読み込み
        mx_route    = df.loc[:, "row":"col"].values.tolist()
        ref_action  = df.loc[:, "action"].values.tolist()
        print("ref_action: ", ref_action)
        #print("----- route -----")
        #print(route)

        return mx_route, ref_action

    # --------------------------------------------------
    # コマの最短ルートのCSVファイルを読み込み表示する（限界BCルート）
    def read_chessfileBC(self, code_csvout):

        # CSVファイルの読み込み
        file_readcsv = foldername + code_csvout   # standard(training) file name  
        df = pd.read_csv(file_readcsv)
        # --------------------------------------------------
        # 選択項目の読み込み
        mx_route    = df.loc[:, "row":"col"].values.tolist()
        #print("----- route -----")
        #print(route)

        return mx_route

# ----------
# Maze functions
class Field(object):
    def __init__(self, maze, start_point, goal_point):

        # ----------------
        # 次の行動(ACTION)に進む
        # action number / row / col / 方向 / onehot
        # 0 / -1 / 0  / 上(up) / 1000
        # 1 / +1 / 0  / 下(down) / 0100
        # 2 / 0  / -1 / 左(left) / 0010
        # 3 / 0  / +1 / 右(right) / 0001
        # ----------------
        self.maze = maze
        self.start_point = start_point
        self.goal_point  = goal_point
        self.movable_vec = [[-1,0],[1,0],[0,-1],[0,1]]

    def display(self, point=None):
        field_data = copy.deepcopy(self.maze)
        if not point is None:
                y, x = point
                field_data[y][x] = "@@"
        else:
                point = ""
        print("================")
        for line in field_data:
                print ("\t" + "%3s " * len(line) % tuple(line))
        print("================")

    def all_display(self, mx_route):
        field_data = copy.deepcopy(self.maze)
        for i in range(len(mx_route)):
            point = mx_route[i]
            y, x = point[0], point[1]
            field_data[y][x] = "@@"
        print("================")
        for line in field_data:
                print ("\t" + "%3s " * len(line) % tuple(line))
        print("================")

    # --------------------------------------------------
    # 次のアクション(movable)を生成する
    def get_actions(self, state):
    
        movables = []
        if state == self.start_point:
            action = 1
            return [action]
        else:
            for i in range(4):
                pos_movable = self.movable_vec[i]
                y = state[0] + pos_movable[0]
                x = state[1] + pos_movable[1]
                if 0 < x < len(self.maze) and 0 < y < len(self.maze) and self.maze[y][x] != "#" and self.maze[y][x] != "S":
                    movables.append(i)
            if len(movables) != 0:
                return movables
            else:
                return None

    # --------------------------------------------------
    # 次のコマの「決定」位置と報酬、ゲームフラグを決める
    # 新概念：マスク「#」位置に着くと、スタート地点に戻る
    def get_val(self, action, state_chess):
        pos_movable = self.movable_vec[action]
        yPos = state_chess[0] + pos_movable[0]
        xPos = state_chess[1] + pos_movable[1]
        next_state = ( yPos, xPos )
        # ----------------
        if state_chess == self.start_point:
            return next_state, 0, False
        else:
            v = float(self.maze[yPos][xPos])
            if next_state == self.goal_point: 
                return next_state, v, True
            else: 
                return next_state, v, False

# ----------
# Initialize and Generate a maze
size    = 27
barriar_rate = 0.1
env     = Maze(size, barriar_rate)
# init_maze : カリキュラム学習用に改造
init_maze, start_point, goal_point = env.read_boardfile()
print("start_point: {}, goal_point: {}".format(start_point, goal_point))
# インスタンスの生成
maze_field = Field(init_maze, start_point, goal_point)
# ボード表示(初期表示)
maze_field.display()
# ----------
# 最短ルートを読み込み(Aルート)
mx_route, ref_action = env.read_chessfile()
# ルートを表示する
maze_field.all_display(mx_route)
# ----------
# コマの最短ルートのCSVファイルを読み込み表示する（限界BCルート）
# Bルート
mx_routeB = env.read_chessfileBC("chess_route27B.csv")
# Cルート
mx_routeC = env.read_chessfileBC("chess_route27C.csv")
#print("--- mx_routeC ---")
#print(mx_routeC)

# ---------------------------
# 各種パラメタ
mx_lm   = [(0, 2),(26, 25),(26, 0),(0, 26)]
action_space = 4
gamma   = 0.95
e_epsilon = 0.30    # randomness of choosing random action or the best one
e_decay = 0.992     # epsilon decay rate
e_min   = 0.01  # minimum rate of epsilon
alpha   = 0.85  # learning rate

# =======================================
# Q-Table based reinforcement learning
# =======================================
# Solving the maze in Q-learning
class Agent:
    def __init__(self, init_maze, model):

        # Q-Tableの初期化
        self.Q_table  = defaultdict(lambda: np.zeros(action_space))
        # Q値の分布を人工的に生成する
        for i in range(size):
            for j in range(size):
                if init_maze[i][j] != 'S' and init_maze[i][j] != '#': 
                
                    # 移動可能な状態(movables)を生成する
                    movables = maze_field.get_actions([i, j])
                    # Qテーブル用アクションを決める
                    action   = calc_QTaction(i, j, movables, mx_lm)
                    # --------
                    # リストに追加する
                    # 状態の表記を変更(XY座標へ)、空間用minRTメトリックスつき
                    state, arr_RTDists = calc_address(i, j, mx_lm)
                    Xnew = np.array([arr_RTDists])
                    ynew = model.predict(Xnew)
                    print("(i,j): ({},{}), action: {}, X: {}, Predicted: {}, ".format(i, j, action, Xnew[0], ynew[0][0]))
                    # --------
                    # Qテーブルに追加する
                    if action == 0:
                        self.Q_table[(i,j)] = np.array([ynew[0][0], -10, -10, -10])
                    elif action == 1:
                        self.Q_table[(i,j)] = np.array([-10, ynew[0][0], -10, -10])
                    elif action == 2:
                        self.Q_table[(i,j)] = np.array([-10, -10, ynew[0][0], -10])
                    elif action == 3:
                        self.Q_table[(i,j)] = np.array([-10, -10, -10, ynew[0][0]])
        print("--- self.Q_table ---") 
        print(self.Q_table)            
        # --------
        self.arr_iplay  = [] 
        self.QL_reward, self.QL_maxturn, self.QL_epsilon = [], [], []
        self.DQ_reward, self.DQ_maxturn, self.DQ_epsilon = [], [], []
        # -----
        # Q-Learning
        self.arr_iplay, self.QL_reward, self.QL_maxturn, self.QL_epsilon = self.run_q_learning(num_episodes, num_turns)
        self.QQL_table  = self.Q_table
   
        # ---------------------------
        # 画像出力用関数
        self.show_graph()
 
    def choose_action(self, state, movables, epsilon):
        if random.random() < epsilon:
            return np.random.choice(movables)
        else:
            values  = []
            for a_order in movables:
                values.append(self.Q_table[state][a_order])
            qmax_idx  = np.argmax(values)
            action    = movables[qmax_idx]
            #print("movables: {}, values: {}".format(movables, values))
            return action

    def q_update_value(self, state, action, reward, next_state, next_movables, done):
        if done:
            self.Q_table[state][action] += alpha * (reward - self.Q_table[state][action])
        else:
            values  = []
            for a_order in next_movables:
                values.append(self.Q_table[next_state][a_order])
            self.Q_table[state][action] += alpha * (reward + gamma * np.max(values) - self.Q_table[state][action])
        return self.Q_table
            
    def run_q_learning(self, num_episodes, num_turns):

        # 初期化
        epsilon = e_epsilon
        model   = {}
        arr_iplay, arr_reward, arr_maxturn, arr_epsilon = [], [], [], []
        # ------
        for i in range(num_episodes):
            # 状態(STATE)を生成する
            state   = start_point
            rewards, done   = 0, False
            iCnt_turn = 0
            # 移動可能な状態(movables)を生成する
            movables = maze_field.get_actions(state)
            # ------
            while True:
                action = self.choose_action(state, movables, epsilon)
                next_state, reward, done = maze_field.get_val(action, state)
                next_movables = maze_field.get_actions(next_state)
                # ------
                # 都合上ゴールをちょっとずらす
                if next_state[0]>=25 and next_state[1]>=25:
                    done   = True
                    reward = 25000 - iCnt_turn
                    print("episode:{}, turn:{}, state:{}, action:{}, reward:{}".format(i, iCnt_turn, state, action, reward))
                # ------
                rewards += reward
                self.q_update_value(state, action, reward, next_state, next_movables, done)
                model[(state, action)] = (reward, next_state, next_movables, done)
                # ------
                if iCnt_turn%1000 == 0:
                    print("episode:{}, turn:{}, state:{}, action:{}, reward:{}".format(i, iCnt_turn, state, action, reward))
                # ------
                if done == True:
                    break
                elif iCnt_turn > num_turns:
                    break
                state    = next_state
                movables = next_movables
                iCnt_turn = iCnt_turn + 1
            # ----------
            # リストに要素を追加します
            arr_iplay.append(i+1)
            arr_reward.append(rewards)
            arr_maxturn.append(iCnt_turn)
            arr_epsilon.append(epsilon)
            # ----------
            # しばらくすれば表示が消えます
            if i%50 == 0:
                #time.sleep(SLEEP_TIME)
                clear_output(wait=True)
            # ----------
            # イプシロンを更新します
            if epsilon > e_min:
                epsilon = round(epsilon*e_decay, 5)
            print(f'episode: {i + 1}/{num_episodes}, epsilon: {epsilon}, rewards: {rewards}, maxturn: {iCnt_turn}')

        return arr_iplay, arr_reward, arr_maxturn, arr_epsilon
        

    # ---------------------------
    # 画像出力用関数
    def show_graph(self):

        fig2 = plt.figure(figsize=(14, 12))
        # -----
        ax1 = fig2.add_subplot(2, 2, 1)
        ax1.set_title('REWARD, e_epsilon :{}'.format(e_epsilon))
        ax1.set_xlabel('#episode')
        ax1.set_ylabel('reward')
        ax1.plot(self.arr_iplay, self.QL_reward, label="QL", color="blue")
        ax1.grid(True)
        ax1.legend(loc='best')
        # -----
        ax2 = fig2.add_subplot(2, 2, 2)
        ax2.set_title('MAX_TURN, e_epsilon :{}'.format(e_epsilon))
        ax2.set_xlabel('#episode')
        ax2.set_ylabel('maxturn')
        ax2.grid(True)
        ax2.plot(self.arr_iplay, self.QL_maxturn, label="QL", color="blue")
        ax2.legend(loc='best')
        # -----
        ax3 = fig2.add_subplot(2, 2, 3)
        ax3.set_title('EPSILON, e_epsilon :{}'.format(e_epsilon))
        ax3.set_xlabel('#episode')
        ax3.set_ylabel('epsilon')
        ax3.grid(True)
        ax3.plot(self.arr_iplay, self.QL_epsilon, label="QL", color="blue")
        ax3.legend(loc='best')
        # -----
        fig2.tight_layout()
        #fig.savefig("./AAC_img.png")
        plt.show()

# ---------------------------
# パラメタ(その３)
num_episodes    = 100
num_turns       = 25000

# ---------------------------
# モデルの読み込み
code_modelinp = "DynaQ_Testing4A.h5"       # file name
file_modelinp = foldername + code_modelinp   # standard(training) file name  
model = load_model(file_modelinp)
model.summary()

# ---------------- 
# Qテーブルを呼び出す
agent = Agent(init_maze, model)
#print("--- agent.QQL_table ---")
#print(agent.QQL_table)

# ---------------- 
# Initialize and Generate a maze
Q_mean = np.zeros([size, size])
Q_max  = np.zeros([size, size])
for i in range(size):
    for j in range(size):
        arr_QV = agent.QQL_table[(i,j)]
        Q_mean[i,j] = round(np.mean(arr_QV),3)
        Q_max[i,j]  = round(np.max(arr_QV),3)    
#print("--- Q_mean ---")
#print(Q_mean) 

# ---------------- 
# 座標変換をして、Q値の分布を把握してみる
arr_iRow, arr_jCol, arr_Goal    = [], [], []
arr_minRT, arr_Ymean, arr_Ymax  = [], [], []
# ------
for i in range(size):
    for j in range(size):
        # 状態の表記を変更(XY座標へ)、空間用minRTメトリックスつき
        state, arr_RTDists = calc_address(i, j, mx_lm)
        goal_Dist   = arr_RTDists[0]
        minRT_Dist  = arr_RTDists[1]
        y_mean     = Q_mean[i,j]
        y_max      = Q_max[i,j]
        # -----
        # リストに追加する
        arr_iRow.append(i)
        arr_jCol.append(j)
        arr_Goal.append(goal_Dist)
        arr_minRT.append(minRT_Dist)
        arr_Ymean.append(y_mean)
        arr_Ymax.append(y_max)
# ------
# Goal-minRT座標系
fig4 = plt.figure(figsize=(14, 9))
ax4 = fig4.add_subplot(projection='3d')
ax4.set_title('Goal-minRT Coodinate, e_epsilon :{}'.format(e_epsilon), size = 20)
ax4.set_xlabel("Goal", size = 14)
ax4.set_ylabel("minRT", size = 14)
ax4.set_zlabel("QV", size = 14)
ax4.scatter(arr_Goal, arr_minRT, arr_Ymean, color="blue")
ax4.scatter(arr_Goal, arr_minRT, arr_Ymax, color="red")
plt.show()

# ---------------- 
# ディープラーニング：　関数のあてはめ
movable_vec = [[-1,0],[1,0],[0,-1],[0,1]]
# Q値の分布を人工的に生成する
iCnt, Y_train = 0, []
# -----
for i in range(size):
    for j in range(size):
        if init_maze[i][j] != 'S' and init_maze[i][j] != '#':        
            # 状態の表記を変更(XY座標へ)、空間用minRTメトリックスつき
            state, arr_RTDists = calc_address(i, j, mx_lm)
            # リストに追加する
            arr_QV = agent.QQL_table[(i,j)]
            for k in range(len(arr_QV)):
                # Qテーブルに追加する
                temp_state   = np.hstack([state, movable_vec[k]])
                if iCnt == 0:
                    Xs_train = [temp_state]
                else:
                    Xs_train = np.concatenate([Xs_train, [temp_state]])
                # -----
                Y_train.append(arr_QV[k])
                iCnt = iCnt + 1
#print("--- Xs_train ---") 
#print(Xs_train)            
#print("--- Y_train ---") 
#print(Y_train)  
# --------
mx_Xs       = np.array(Xs_train)
temp_Ymean  = np.array(Y_train)
arr_Ymean   = np.reshape(temp_Ymean, (-1,1))
# --------
model2 = Sequential()
model2.add(Dense(600, input_dim=6, kernel_initializer='normal', activation='relu'))
model2.add(Dense(600, activation='relu'))
model2.add(Dense(300, activation='relu'))
model2.add(Dense(200, activation='relu'))
model2.add(Dense(1, activation='linear'))
model2.summary()
model2.compile(loss='mse', optimizer='adam', metrics=['mse','mae'])
history = model2.fit(mx_Xs, arr_Ymean, epochs=3000, batch_size=80,  verbose=1, validation_split=0.2)
# --------
print(history.history.keys())
# "Loss"
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

# ---------------- 
Xnew = mx_Xs
ynew = model2.predict(Xnew)
#for i in range(len(Xnew)):
#    print("X=: {}, Raw=: {}, Predicted=: {}".format(Xnew[i], arr_Ymean[i][0], ynew[i][0]))
# --------
# torch can only train on Variable, so convert them to Variable
fig2 = plt.figure(figsize=(10, 7))
ax2 = fig2.add_subplot(projection='3d')
x1 = Xnew[:,0]
x2 = Xnew[:,1]
y_mean = arr_Ymean[:,0]
y_pred = ynew[:,0]
ax2.scatter(x1, x2, y_mean, color="blue")
ax2.scatter(x1, x2, y_pred, color="red")
plt.show()

# ---------------- 
# モデルを保存する
code_modelout = "DynaQ_MazeLearning_initial.h5"       # file name
file_modelout = foldername + code_modelout   # standard(training) file name  
model2.save(file_modelout)

```

QEU:FOUNDER ; “今回は、簡単にいこう・・・。**状態行動価値関数Q(s,a)**の当てはめ結果は以下のようになりました。”

**（損失の推移）**

![imageRL7-6-1](https://reinforce.github.io/images/imageRL7-6-1.jpg)

**（当てはめ状況）**

![imageRL7-6-2](https://reinforce.github.io/images/imageRL7-6-2.jpg)

D先生 : “おや？思ったよりもうまく行った・・・。”

QEU：FOUNDER ; “じゃあ、これを使って**「fine-tuning」**にいきましょう！！”

D先生 : “「fine-tuning」っていいのかなぁ・・・。DQN-2DMazeの結果を見て、私の立場を示す(笑)。”


## ～　まとめ　～

QEU：FOUNDER ; “今回から、イケメンバトルは「プリティさんは自由自在」に改名しました・・・。”

<iframe width="560" height="315" src="https://www.youtube.com/embed/pS5HvrCgGjE" ti-tle="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; en-crypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

D先生: “また、テキトーなことを・・・。”

QEU:FOUNDER ： “まあ、はっきりいって・・・。少なくとも今回の場合、保守を名乗るのであれば、保守（笑）に投票したらいかんでしょう。”

<iframe width="560" height="315" src="https://www.youtube.com/embed/oc7KK0RVRs4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

D先生: “それでも、投票する人はいるでしょう。”

QEU:FOUNDER ; “保守って、あくまで**「（オレが）メシ食うための立場」**のためなんだし、みんな、ゆる～く行こうよ・・・。”

![imageRL7-6-3](https://reinforce.github.io/images/imageRL7-6-3.jpg)

D先生： “「保守が生きづらい時代」になったものだ。某Youtubeチャンネルで、憂国の士がまたまたお嘆きになることでしょう。”

