## QEUR21_MZDYNQ1: 簡単なDYNQで遊ぶ

## ～　準備運動です　～

D先生 : “それでは、我々なりに**Dyna-Q**をやってみましょう・・・。”

![imageRL7-2-1](https://reinforce.github.io/images/imageRL7-2-1.jpg) 

QEU:FOUNDER ： “では、我々のテーマである**(27 x 27)のメイズ**でやってみましょう。今回は「やってみただけ」なので、これ以上の説明はありません。プログラムをドン！！”

```python
# ----------------
# メイズ(MAZE)ゲームの強化学習システム
# step1A : 3Dメイズの再度準備段階です
# step1A : Table_3DMazeQynaQ_pyTorch(DQN_size27).py
# step1A : TABLEでDyna-Qをやってみます（予備実験）
# ---------------- 
# 数値計算のﾗｲﾌﾞﾗﾘ読み込み
import numpy as np
import pandas as pd
import math, copy, random, time
from collections import defaultdict, deque, Counter
from scipy.special import softmax
from IPython.display import clear_output
# ---------------- 
import seaborn as sns
import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = (16, 10)

# =================================================
# Game board difinition 
# =================================================
foldername  = "./MAZE_test/"

# =================================================
# difinition of STATE CALCULATION function
# =================================================
# タイムライン用minRTメトリックスを計算する
def timeline_minRT(a_row, a_col, iCnt_turn):
    len_route = len(mx_route)
    if iCnt_turn <= len_route - 1:
        pos_rt  = mx_route[iCnt_turn]
    else:
        pos_rt  = mx_route[len_route - 1]
    # -----
    tlEuc   = (a_row - pos_rt[0])**2 + (a_col - pos_rt[1])**2
    dist_tlminRT = np.sqrt(tlEuc)
        
    return round(dist_tlminRT,5)

# -------
# 空間用minRTメトリックスを計算する
def space_minRT(pos_route, a_row, a_col):
    len_route = len(pos_route)
    arr_sminRT = []
    for i in range(len_route):
        pos_rt  = pos_route[i]
        sqEuc   = (a_row - pos_rt[0])**2 + (a_col - pos_rt[1])**2
        distEuc = np.sqrt(sqEuc)
        arr_sminRT.append(round(distEuc,5))   
    dist_sminRT = np.min(arr_sminRT)
        
    return dist_sminRT

# -------
# 状態の表記を変更(XY座標へ)、空間用minRTメトリックスつき
def calc_address(a_row, a_col, mx_lm, iCnt_turn):

    add_state = np.zeros(4)
    for i in range(len(mx_lm)):
        pos_lm  = mx_lm[i]
        sqEuc   = (a_row - pos_lm[0])**2 + (a_col - pos_lm[1])**2
        distEuc = np.sqrt(sqEuc)
        add_state[i] = round(distEuc,5)
    # --------------------------------------------------
    # 空間用minRTメトリックスを計算する
    # Aルート(標準)
    dist_sminRTA = space_minRT(mx_route, a_row, a_col)
    dist_sminRTA = round(dist_sminRTA,5)
    # Bルート(標準との差異分)
    dist_sminRTB = space_minRT(mx_routeB, a_row, a_col)
    dist_sminRTB = round(dist_sminRTB,5) - dist_sminRTA
    # Cルート(標準との差異分)
    dist_sminRTC = space_minRT(mx_routeC, a_row, a_col)
    dist_sminRTC = round(dist_sminRTC,5) - dist_sminRTA
    # タイムライン用minRTメトリックスを計算する
    dist_tlminRT = timeline_minRT(a_row, a_col, iCnt_turn)
    # Aルート修正
    if add_state[2] > add_state[3]:
        dist_sminRTA = -1 * dist_sminRTA
    # --------------------------------------------------
    # 距離とりまとめリストを生成する
    arr_RTDists = np.zeros(5)
    arr_RTDists[0] = add_state[0] + add_state[1]     # Total距離
    arr_RTDists[1] = dist_sminRTA    # 空間minRT距離
    arr_RTDists[2] = dist_tlminRT    # タイムラインminRT距離
    arr_RTDists[3] = dist_sminRTB    # 空間用minRT距離(B)
    arr_RTDists[4] = dist_sminRTC    # 空間用minRT距離(C)
    # --------------------------------------------------
    # STATEを生成する
    state = np.hstack([[a_row, a_col], [add_state[0], add_state[1]], [dist_sminRTA, dist_sminRTB, dist_sminRTC]])
    
    return state, arr_RTDists

# ----------
# Maze Class
class Maze(object):
    def __init__(self, size=27, blocks_rate=0.1):
        self.size = size if size > 3 else 27
        self.blocks = int((size ** 2) * blocks_rate) 
        self.s_list = []
        self.maze_list = []
        self.e_list = []

    def create_mid_lines(self, k):
        if k == 0: self.maze_list.append(self.s_list)
        elif k == self.size - 1: self.maze_list.append(self.e_list)
        else:
            tmp_list = []
            for l in range(0,self.size):
                if l == 0: tmp_list.extend("#")
                elif l == self.size-1: tmp_list.extend("#")
                else:
                    a = random.randint(-1, 0)
                    tmp_list.extend([a])
            self.maze_list.append(tmp_list)

    def insert_blocks(self, k, s_r, e_r):
        b_y = random.randint(1, self.size-2)
        b_x = random.randint(1, self.size-2)
        if [b_y, b_x] == [1, s_r] or [b_y, b_x] == [self.size - 2, e_r]: k = k-1
        else: self.maze_list[b_y][b_x] = "#"
            
    def generate_maze(self): 
        s_r = random.randint(1, int((self.size / 2)) - 1)
        for i in range(0, self.size):
            if i == s_r: self.s_list.extend("S")
            else: self.s_list.extend("#")
        start_point = [0, s_r]

        e_r = random.randint(int((self.size / 2)) + 1, self.size - 2)
        for j in range(0, self.size):
            if j == e_r: self.e_list.extend([50])
            else: self.e_list.extend("#")
        goal_point = [self.size - 1, e_r]

        for k in range(0, self.size):
            self.create_mid_lines(k)
        
        for k in range(self.blocks):
            self.insert_blocks(k, s_r, e_r)

        return self.maze_list, start_point, goal_point

    # --------------------------------------------------
    # 盤面のCSVファイルを読み込み表示する
    def read_boardfile(self):
        # CSVファイルの読み込み
        code_csvout = "maze_board27.csv"       # file name
        file_readcsv = foldername + code_csvout   # standard(training) file name  
        df = pd.read_csv(file_readcsv)
        #max_play = len(df)
        # print("データ量 max_play",max_play)
        # print(df)
        # --------------------------------------------------
        # 選択項目の読み込み
        maze = df.loc[:, "0":"26"].values.tolist()
        start_point = ( 0, 0 )
        goal_point  = ( 0, 0 )
        for i in range(size):
            for j in range(size):
                if maze[i][j] == 'S':
                    start_point = ( i, j )
                if maze[i][j] == '5000':
                    goal_point  = ( i, j )

        self.maze_list   = maze
        self.start_point = start_point
        self.goal_point  = goal_point
        print("goal_point", goal_point)
        print("----- mx_maze(read_boardfile) -----")
        print(maze)

        return self.maze_list, start_point, goal_point

    # --------------------------------------------------
    # コマの最短ルートのCSVファイルを読み込み表示する（標準Aルート）
    def read_chessfile(self):
        # CSVファイルの読み込み
        code_csvout = "chess_route27.csv"       # file name
        file_readcsv = foldername + code_csvout   # standard(training) file name  
        df = pd.read_csv(file_readcsv)
        #max_play = len(df)
        # print("データ量 max_play",max_play)
        # print(df)
        # --------------------------------------------------
        # 選択項目の読み込み
        mx_route    = df.loc[:, "row":"col"].values.tolist()
        ref_action  = df.loc[:, "action"].values.tolist()
        print("ref_action: ", ref_action)
        #print("----- route -----")
        #print(route)

        return mx_route, ref_action

    # --------------------------------------------------
    # コマの最短ルートのCSVファイルを読み込み表示する（限界BCルート）
    def read_chessfileBC(self, code_csvout):

        # CSVファイルの読み込み
        file_readcsv = foldername + code_csvout   # standard(training) file name  
        df = pd.read_csv(file_readcsv)
        #max_play = len(df)
        # print("データ量 max_play",max_play)
        # print(df)
        # --------------------------------------------------
        # 選択項目の読み込み
        mx_route    = df.loc[:, "row":"col"].values.tolist()
        #print("----- route -----")
        #print(route)

        return mx_route

# ----------
# Maze functions
class Field(object):
    def __init__(self, maze, start_point, goal_point):

        # ----------------
        # 次の行動(ACTION)に進む
        # action number / row / col / 方向 / onehot
        # 0 / -1 / 0  / 上(up) / 1000
        # 1 / +1 / 0  / 下(down) / 0100
        # 2 / 0  / -1 / 左(left) / 0010
        # 3 / 0  / +1 / 右(right) / 0001
        # ----------------
        self.maze = maze
        self.start_point = start_point
        self.goal_point  = goal_point
        self.movable_vec = [[-1,0],[1,0],[0,-1],[0,1]]

    def display(self, point=None):
        field_data = copy.deepcopy(self.maze)
        if not point is None:
                y, x = point
                field_data[y][x] = "@@"
        else:
                point = ""
        print("================")
        for line in field_data:
                print ("\t" + "%3s " * len(line) % tuple(line))
        print("================")

    def all_display(self, mx_route):
        field_data = copy.deepcopy(self.maze)
        for i in range(len(mx_route)):
            point = mx_route[i]
            y, x = point[0], point[1]
            field_data[y][x] = "@@"
        print("================")
        for line in field_data:
                print ("\t" + "%3s " * len(line) % tuple(line))
        print("================")

    # --------------------------------------------------
    # 次のアクション(movable)を生成する
    def get_actions(self, state):
        # ----------------
        # 次の行動(ACTION)に進む
        # action number / row / col / 方向 / onehot
        # 0 / -1 / 0  / 上(up) / 1000
        # 1 / +1 / 0  / 下(down) / 0100
        # 2 / 0  / -1 / 左(left) / 0010
        # 3 / 0  / +1 / 右(right) / 0001
        # ----------------
        movables = []
        if state == self.start_point:
            action = 1
            return [action]
        else:
            for i in range(4):
                pos_movable = self.movable_vec[i]
                y = state[0] + pos_movable[0]
                x = state[1] + pos_movable[1]
                if 0 < x < len(self.maze) and 0 < y < len(self.maze) and self.maze[y][x] != "#" and self.maze[y][x] != "S":
                    movables.append(i)
            if len(movables) != 0:
                return movables
            else:
                return None

    # --------------------------------------------------
    # 次のコマの「決定」位置と報酬、ゲームフラグを決める
    # 新概念：マスク「#」位置に着くと、スタート地点に戻る
    def get_val(self, action, state_chess):
        pos_movable = self.movable_vec[action]
        yPos = state_chess[0] + pos_movable[0]
        xPos = state_chess[1] + pos_movable[1]
        next_state = ( yPos, xPos )
        # ----------------
        if state_chess == self.start_point:
            return next_state, 0, False
        else:
            v = float(self.maze[yPos][xPos])
            if next_state == self.goal_point: 
                return next_state, v, True
            else: 
                return next_state, v, False

# ----------
# Initialize and Generate a maze
size    = 27
barriar_rate = 0.1
env     = Maze(size, barriar_rate)
# init_maze : カリキュラム学習用に改造
init_maze, start_point, goal_point = env.read_boardfile()
print("start_point: {}, goal_point: {}".format(start_point, goal_point))
# インスタンスの生成
maze_field = Field(init_maze, start_point, goal_point)
# ボード表示(初期表示)
maze_field.display()
# ----------
# 最短ルートを読み込み(Aルート)
mx_route, ref_action = env.read_chessfile()
# ルートを表示する
maze_field.all_display(mx_route)
# ----------
# コマの最短ルートのCSVファイルを読み込み表示する（限界BCルート）
# Bルート
mx_routeB = env.read_chessfileBC("chess_route27B.csv")
# Cルート
mx_routeC = env.read_chessfileBC("chess_route27C.csv")
#print("--- mx_routeC ---")
#print(mx_routeC)



# ---------------------------
# パラメタ
mx_lm   = [(0, 2),(26, 25),(26, 0),(0, 26)]
action_space = 4
gamma   = 0.95
e_epsilon = 0.97    # randomness of choosing random action or the best one
e_decay = 0.992     # epsilon decay rate
e_min   = 0.01  # minimum rate of epsilon
alpha   = 0.85  # learning rate

# =======================================
# =======================================
# Solving the maze in Q-learning
class Agent:
    def __init__(self):

        self.arr_iplay  = [] 
        self.QL_reward  = [] 
        self.QL_maxturn = []
        self.QL_epsilon = []
        self.DQ_reward  = [] 
        self.DQ_maxturn = []
        self.DQ_epsilon = []
        # -----
        # Q-Learning
        self.Q_table  = defaultdict(lambda: np.zeros(action_space))
        flg_planning  = False
        self.arr_iplay, self.QL_reward, self.QL_maxturn, self.QL_epsilon = self.run_q_learning(num_episodes, num_turns, n_planning, flg_planning)
        self.QQL_table  = self.Q_table
        
        # -----
        # DynaQ
        self.Q_table  = defaultdict(lambda: np.zeros(action_space))
        flg_planning  = True
        _, self.DQ_reward, self.DQ_maxturn, self.DQ_epsilon = self.run_q_learning(num_episodes, num_turns, n_planning, flg_planning)
        self.QDQ_table  = self.Q_table

        # ---------------------------
        # 画像出力用関数
        self.show_graph()
        
    def choose_action(self, state, movables, epsilon):
        if random.random() < epsilon:
            return np.random.choice(movables)
        else:
            values  = []
            for a_order in movables:
                values.append(self.Q_table[state][a_order])
            qmax_idx  = np.argmax(values)
            action    = movables[qmax_idx]
            #print("movables: {}, values: {}".format(movables, values))
            return action

    def q_update_value(self, state, action, reward, next_state, next_movables, done):
        if done:
            self.Q_table[state][action] += alpha * (reward - self.Q_table[state][action])
        else:
            values  = []
            for a_order in next_movables:
                values.append(self.Q_table[next_state][a_order])
            self.Q_table[state][action] += alpha * (reward + gamma * np.max(values) - self.Q_table[state][action])
        return self.Q_table
            
    def run_q_learning(self, num_episodes, num_turns, n_planning, flg_planning):

        # 初期化
        epsilon = e_epsilon
        model = {}
        # ------
        arr_iplay   = []
        arr_reward  = []
        arr_maxturn = []
        arr_epsilon = []
        # ------
        for i in range(num_episodes):
            # 状態(STATE)を生成する
            state   = start_point
            rewards = 0 
            done    = False
            iCnt_turn = 0
            # 移動可能な状態(movables)を生成する
            movables = maze_field.get_actions(state)
            # ------
            while True:
                action = self.choose_action(state, movables, epsilon)
                next_state, reward, done = maze_field.get_val(action, state)
                next_movables = maze_field.get_actions(next_state)
                # ------
                # 都合上ゴールをちょっとずらす
                if next_state[0]>=25 and next_state[1]>=25:
                    done   = True
                    reward = 25000 - iCnt_turn
                    print("episode:{}, turn:{}, state:{}, action:{}, reward:{}".format(i, iCnt_turn, state, action, reward))
                # ------
                rewards += reward
                self.q_update_value(state, action, reward, next_state, next_movables, done)
                model[(state, action)] = (reward, next_state, next_movables, done)
                if i > 10 and flg_planning == True:
                    for _ in range(n_planning):
                        (S, A), (R, S_, M_, done_) = random.choice(list(model.items()))
                        self.q_update_value(S, A, R, S_, M_, done_)
                # ------
                if iCnt_turn%1000 == 0:
                    print("episode:{}, turn:{}, state:{}, action:{}, reward:{}".format(i, iCnt_turn, state, action, reward))
                # ------
                if done == True:
                    break
                elif iCnt_turn > num_turns:
                    break
                state    = next_state
                movables = next_movables
                iCnt_turn = iCnt_turn + 1
            # ----------
            # リストに要素を追加します
            arr_iplay.append(i+1)
            arr_reward.append(rewards)
            arr_maxturn.append(iCnt_turn)
            arr_epsilon.append(epsilon)
            # ----------
            # しばらくすれば表示が消えます
            if i%50 == 0:
                #time.sleep(SLEEP_TIME)
                clear_output(wait=True)
            # ----------
            # イプシロンを更新します
            if epsilon > e_min:
                epsilon = round(epsilon*e_decay, 5)
            print(f'episode: {i + 1}/{num_episodes}, epsilon: {epsilon}, rewards: {rewards}, maxturn: {iCnt_turn}')

        return arr_iplay, arr_reward, arr_maxturn, arr_epsilon

    # ---------------------------
    # 画像出力用関数
    def show_graph(self):

        fig2 = plt.figure(figsize=(14, 12))
        # -----
        ax1 = fig2.add_subplot(2, 2, 1)
        ax1.set_title('REWARD')
        ax1.set_xlabel('#episode')
        ax1.set_ylabel('reward')
        ax1.plot(self.arr_iplay, self.QL_reward, label="QL", color="blue")
        ax1.plot(self.arr_iplay, self.DQ_reward, label="DQ", color="red")
        ax1.grid(True)
        ax1.legend(loc='best')
        # -----
        # 移動平均を付ける
        ax2 = fig2.add_subplot(2, 2, 2)
        ax2.set_title('MAX_TURN')
        ax2.set_xlabel('#episode')
        ax2.set_ylabel('maxturn')
        ax2.grid(True)
        ax2.plot(self.arr_iplay, self.QL_maxturn, label="QL", color="blue")
        ax2.plot(self.arr_iplay, self.DQ_maxturn, label="DQ", color="red")
        ax2.legend(loc='best')
        # -----
        ax3 = fig2.add_subplot(2, 2, 3)
        ax3.set_title('EPSILON')
        ax3.set_xlabel('#episode')
        ax3.set_ylabel('epsilon')
        ax3.grid(True)
        ax3.plot(self.arr_iplay, self.QL_epsilon, label="QL", color="blue")
        #ax3.plot(self.arr_iplay, self.DQ_epsilon, label="DQ", color="red")
        ax3.legend(loc='best')
        # -----
        fig2.tight_layout()
        #fig.savefig("./AAC_img.png")
        plt.show()

# ---------------------------
# パラメタ(その３)
num_episodes    = 300
num_turns       = 25000
n_planning      = 10
# -----
# 実行
agent = Agent()
# Qテーブルを呼び出す
#print("--- agent.QQL_table ---")
#print(agent.QQL_table)

# Initialize and Generate a maze
#size    = 27
Q_mean = np.zeros([size, size])
Q_max  = np.zeros([size, size])
for i in range(size):
    for j in range(size):
        arr_QV = agent.QQL_table[(i,j)]
        Q_mean[i,j] = round(np.mean(arr_QV),3)
        Q_max[i,j]  = round(np.max(arr_QV),3)    

# HEAT MAPを呼び出す
# 平均
plt.figure(figsize=(17, 10))
plt.title('Q_Value(MEAN)')
sns.heatmap(Q_mean, annot=True, cbar=False, cmap='Blues', fmt='.1f')
plt.show()

# 最大
plt.figure(figsize=(17, 10))
plt.title('Q_Value(MAX)')
sns.heatmap(Q_max, annot=True, cbar=False, cmap='hot', fmt='.1f')
plt.show()

```

QEU:FOUNDER ; “やっぱり、小生がやってもDyna-Qの方が普通のQ-learningよりも性能がいいですね・・・。”

![imageRL7-2-2](https://reinforce.github.io/images/imageRL7-2-2.jpg) 

D先生 ; “確かに、見事に差がでていますですね・・・。”

QEU:FOUNDER ; “次に、いわゆる「モデルの中身」をみてみましょう。すなわち、Q値の事ですけどね・・・。”

**（平均値）**

![imageRL7-2-3](https://reinforce.github.io/images/imageRL7-2-3.jpg) 

**（最大値）**

![imageRL7-2-4](https://reinforce.github.io/images/imageRL7-2-4.jpg) 

D先生 ; “いやぁ・・・。これは面白い・・・。”

QEU:FOUNDER ; “引き続き、我々なりに「モデル化」できるように、がんばりましょう・・・。”
 

## ～　まとめ　～

C部長 ： “それではイケメンバトルにいきましょう。”
 
![imageRL7-2-5](https://reinforce.github.io/images/imageRL7-2-5.jpg) 

C部長 ： “今回はボクから・・・。”

<iframe width="560" height="315" src="https://www.youtube.com/embed/K1Dkw0Oelp4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

D先生 ： “例の、**「おきなわの人」の件**ね・・・。”
 
![imageRL7-2-6](https://reinforce.github.io/images/imageRL7-2-6.jpg) 

D先生 : “**「強制」**って初めて聞いたわ・・・。”


